[["index.html", "Sozialwissenschaftliche Datenanalyse mit R Einführung", " Sozialwissenschaftliche Datenanalyse mit R Kenneth Horvath &amp; Guy Schwegler HS 2021 Einführung Das Seminar Sozialwissenschaftliche Datenanalyse mit R bietet eine systematische Einführung in das Statistikpaket R sowie die Benutzeroberfläche RStudio. R ist eine Open Source Software, die sich unter anderem durch Flexibilität sowie durch vielfältige Möglichkeiten der numerischen und grafischen Datenanalyse auszeichnet. Das Seminar führt auf der einen Seite allgemein in den Aufbau des Programms und dessen Funktionsweisen ein. Auf der anderen Seite werden gewisse statistische Verfahren auch in inhaltliche Abstimmung mit der Vorlesung Grundlagen der multivariaten Statistik vermittelt. Anhand der Funktionsweisen und der Verfahren werden dann Techniken des effizienten Datenmanagements, Möglichkeiten zur eigenständigen Programmierung von kleinen Funktionen sowie Formen der grafischen Datenanalyse und Ergebnisdarstellung besprochen. Das vorliegende Dokument ist ein sogenanntes Bookdown (Xie 2020), siehe auch hier, und dient der Ergebnissicherung im Seminarverlauf. Das heisst dass die im Seminar besprochene Themen hiernochmals schriftlich festgehalten, diskutiert und allenfalls mit Literatur ergänzt werden (siehe für allgemeine Literatur etwa Diaz-Bone (2019), Kabacoff (2015) oder Manderscheid (2017)). Das Bookdown wird laufend aktualisiert. Ebenfalls werden in diesem Bookdown die Lösungen für die im Seminar beziehungsweise in den Wochenplänen gestellten Aufgaben präsentiert (Falllösungen). References "],["wocheplan-01.html", "1 Wocheplan 01 1.1 Sozialwissenschaftliche Datenanalyse 1.2 Ziel des Kurses 1.3 R als Programm &amp; RStudio 1.4 Lernziele der ersten Woche 1.5 Aufgaben der ersten Woche 1.6 Ergänzung: Standardabweichung zwischen Grundgesamtheit und Stichprobe", " 1 Wocheplan 01 Vorbereitung von der 01. auf die 02.Einheit. 1.1 Sozialwissenschaftliche Datenanalyse Das Seminar sozialwissenschaftliche Datenanalyse mit R versucht eine Realität des statistischen Arbeitens zu vermitteln und ergänzt so die Vorlesung Grundlagen der multivariaten Statistik gleich in zweierlei Hinsicht: Erstens wird eine Auswahl der gelernten statistischen Verfahren konkret angewendet (und so auch nochmals repetiert). Zweitens zeigt sich neben den eigentlichen Verfahren ein weiterer, impliziter Teil der Statistik: ein Umgang mit Daten, deren Aufbereitung und Verarbeitung sowie die damit einhergehenden Herausforderungen. Hinter dem Seminar steht eine bestimmte Vorstellung der sozialwissenschaftlichen Datenanalyse, die folgende Teile enthält (Wickham and Grolemund 2016): Figure 1.1: Modell Datenanalyse Als erster Schritt müssen die Daten eingelesen bzw. importiert werden. Die importierten Daten gilt es dann aufzubereiten und aufzuräumen. Das bedeutet, dass sie in einer konsistenten Form gespeichert werden sollen (z.Bsp. dass jede Zeile einer Person und jede Spalte einer Variable entspricht). Dieser zweite Schritt ist im Rahmen von Sekundärdaten (wie auch wir sie verwenden werden) oft bereits erfolgt. Ein weiterer Schritt ist es dann, die Daten zu transformieren. Das heisst die Fälle und ihre Ausprägungen werden auf ein bestimmes Interesse eingegrenzt (z.Bsp. auf alle Personen die über ein bestimmtes Einkommen verfügen), neue Variablen werden erstellt (die Funktionen bestehender Variablen sind, etwa Einkommensklassen), und eine Reihe von zusammenfassenden Statistiken werden berechnet (verschiedene univariate Kennwerte). Das Aufbereiten und Transformieren ist ein grosser Teil der statistischen Analyse (es ist ein Kampf mit den Daten, Wickham and Grolemund 2016, Kap.1.1). Ziel dieser Arbeit ist es, die Daten in eine passende Form zu bringen, um optimal mit ihnen arbeiten zu können. Wenn die Daten (voerst) in einer optimalen Form vorliegen gibt es zwei Hauptmotoren der Wissensgenerierung (Wickham and Grolemund 2016, Kap.1.1): Visualisierung und Modellierung. Mit Visualisierungen lässt sich schnell eine Übersicht gewinnen (z.Bsp. könnte es überhaupt einen Zusammenhang zwischen zwei Variablen geben?). Modellierungen wiederum ergänzen diese ersten Einsichten, indem präzise Antworten auf Fragen möglich sind (wie gross ist der Zusammenhang genau?). Das Transformieren, Visualisieren und Modellieren der Daten ist dabei keineswegs ein linearer Prozess, sondern es ergeben sich in ihm immer wieder Wechselwirkungen, Rückbezüge und dadurch neue Wege, um an die Daten heranzutreten. Der letzte Schritt der Datenanalyse ist die Kommunikation. Es gilt also sowohl das Vorgehen (zumindest teilweise) als inbesondere auch die Ergebnisse der Analyse anderen mitzuteilen. Diese Prozesse der Datenanalyse finden alle in einem bestimmen Rahmen statt (vgl. auch Sauer 2019, 3). Dies ist auf der einen Seite die Idees des Programmierens im Vorgehen selber (vgl. Wickham and Grolemund 2016, Kap.1.1). Auf der anderen Seite bilden aber die Sozialwissenschaften selber auch einen Rahmen, anhand dessen etwa Datenstrukturen (z.Bsp. dass eine Person ein Fall und damit eine Zeile ist) oder angemessene Ziele der Analyse (ab wann ist ein Zusammenhang etwa gross?) vorgegeben werden. 1.2 Ziel des Kurses Das Seminar verfolgt zwei miteinander verzahnte, übergeordnete Lernziele. Einerseits sollen die Studierenden sich Grundkenntnisse der statistischen Datenanalyse mit R aneignen. Andererseits werden ausgewählte Inhalte der Vorlesung praktisch angewandt und damit auch veranschaulicht.1 Konkret sollen die Studierenden am Ende des Semesters einen ersten Einblick in Abläufe und Anforderungen softwaregestützter Datenanalyse haben, typische Herausforderungen statistischen Arbeitens eigenständig bewältigen können, die allgemeine Funktionsweise und die Struktur von R verstehen, die Umsetzung ausgewählter multivariater Verfahren in R beherrschen, dabei auch grafische Verfahren als zentrale Bausteine aktueller Datenanalyse einsetzen können sowie die Grundlage dafür erworben haben, flexibel eigene Analysestrategien in R umzusetzen. 1.3 R als Programm &amp; RStudio R als Programmiersprache wurde von Beginn an für die Statistik beziehungsweise für die Statistiklehre entwickelt. Die Anfänge des Programms fanden in den 1990er Jahre an der Universität Auckland in Neuseeland statt, wo R von Ross Ihaka und Robert Gentleman entwickelt wurde (Manderscheid 2017, 1). Der Buchstabe R als Name geht sowohl auf eine ältere Grundlage zurück  die Programmiersprache S  als auch auf die Vornamen der beiden Entwickler (ebd., vgl. auch Sauer 2019, 13f). Das R-Projekt wurde in der Zusammenareit mit weiteren Wissenschaftler_Innen voran getrieben und bald auch unter der General Public Licence (GNU) veröffentlicht (Manderscheid 2017, 1). R ist daher frei zugänglich, kostenlos und darf von allen verändert werden. Es ist insbesondere auch diese Open Source Idee, die R zu seiner Verbreitung half  und die sicherstellt, dass die neusten Entwicklungen in und mit der Software stattfinden. R als Programm ist in Paketen organisiert und präsentiert sich als Statistikumgebung (Manderscheid 2017, 1). Ausgehend von der Basisversion bzw. des Basispaketes kann R beliebig erweitert werden. Unter https://cran.r-project.org/ findet sich eine beständig wachsende und umfangreiche Sammlung von Paketen, die sowohl Lösungen für allgemeine Verfahren anbieten (etwa Pakete für die multiple Korrespondenzanalyse, siehe soc.ca) als auch für spezifische Probleme (etwa für Atomic Force Microscope Image Analysis beim Paket AFM). Diese Pakete können installiert werden und es gilt sie dann jeweils noch zu laden, bevor sie verwendet werden können. Nach dem Beenden des Programms werden die verwendeten Pakete wieder versorgt und es gilt sie beim nächsten Mal erneut zu laden (die Pakete beleiben aber installiert). Letzterer Vorgang stellt sicher, dass R schlank bleibt, d.h. nur immer die benötigen Dinge auch ausgeführt werden. install.packages(&quot;soc.ca&quot;) #...installiert das Paket library(soc.ca) #...lädt das Paket Neben dieser Open Source Idee und der daraus folgenden, beständigen Aktualisierung und Erweiterungen des Programms zeichnet R sich weiter durch dessen Stärke im Bereich der Visualisierung aus. Es bieten sich unbegrenzte Möglichkeiten für Grafiken und Diagramme, sowohl bereits in der Basisversion als insbesondere auch mit spezifischen Paketen (siehe Chang et al. 2020). Neben der Basisversion von R und R als eigentlicher Programmiersprache gibt es grafische Benutzeroberflächen (GUIs), um mit der Programmiersprache umzugehen. Im Zentrum unseres Seminars steht RStudio, die am weitesten verbreitete grafische Benutzeroberfläche von R. Diese Oberfläche bietet einige praktische Zusatzfunktionen und erleichtert so das Arbeiten mit R durch Autovervollständigkeitsfunktionen, automatische Einrückungen, Syntaxhervorhebung, integrierte Hilfsfunktion, Informationen zu Objekten im Workspace, menügestützten Oberflächen und Daten-Viewer (Manderscheid 2017, 18). Die eigentliche Arbeit verrichtet aber weiterhin R selber, und R wird automatisch gestartet wird, wenn Sie RStudio starten (Sauer 2019, 21). Man kann diese Arbeitsteilung mit einem Auto vergleichen: R ist der Motor des Autos, während RStudio das Amaturenbrett ist, vor dem Sie sitzen und das Auto lenken. 1.4 Lernziele der ersten Woche Die erste Seminarwoche dient dazu, die technischen Voraussetzungen für die gemeinsame Arbeit im Seminar zu prüfen und mit der geplanten Arbeitsweise vertraut zu werden. Das Seminar zielt nicht nur auf einen Frontalunterricht ab, sondern ist als eine Art flipped classroom konzipiert. Sie bekommen also von Woche zu Woche konkrete Arbeitsaufträge (die Falllösungen). Diese sollen Sie eigenständig bewältigen und alle Probleme und Unklarheiten notieren, die sich im Arbeitsprozess ergeben. Die gemeinsamen Sitzungen dienen dann dazu, Lösungswege zu den Aufgaben zu präsentieren, offene Fragen zu klären, Konzepte vertiefend zu erläutern und die nächsten Schritte vorzubereiten. Für jede Woche werden Lernziele und Arbeitsaufträge definiert. Für die erste Seminarwoche lassen sich als Lernziele festhalten: Sie wissen, wie Sie die aktuellen Versionen von R und RStudio auf Ihrem Computer installieren Sie wissen, wie man R-Pakete installiert und in R lädt Sie können eine Funktion aufrufen Sie haben einen soliden ersten Eindruck, wie man mit R kommuniziert und einfache Operationen durchführt Sie haben eine erste Orientierung zu Unterstützungsangeboten, die man online findet (auch wenn diese teilweise noch überfordernd wirken) 1.5 Aufgaben der ersten Woche Installieren Sie die aktuellen Versionen von R und RStudio auf Ihrem Endgerät! Sie sollten sich Notizen machen, wenn es Probleme gibt  und für das nächste Mal gleich festhalten, wie Sie diese gelöst haben. Da die Details der Installation vom Betriebssystem und den Spezifikationen des Endgeräts abhängen, ist es normal, dass dieser Prozess manchmal erst auf den zweiten Versuch funktioniert. Mittels der Funktion version (ACHTUNG: ohne Klammern) lässt sich die Version von R abrufen. Ob dies der aktuellsten Version von R entspricht lässt sich auf der R Projektseite überprüfen. RStudio lässt sich über die Menüsteuerung updaten: Help &gt; Check for Updates. Wichtig: RStudio als grafische Benutzeroberfläche ist nicht dasselbe wie R. Ein Update von RStudio ist also nicht nicht gleich ein Update von R, sondern letzteres muss manuell erfolgen. Das Paket installr und dessen Funktion updateR() ermöglicht auf Windows dass sowohl R als auch die installierten Pakete geupdated werden. Ebenfalls bietet RStudio über die Menüsteuerung eine Möglichkeit, die Pakete zu installieren (Tools&gt;Check for Packages Updates). Zwei Unklarheiten, die aufgetaucht sind: Es sind mehrere Versionen von R auf meinen Computer installiert Ist das ein Problem? Nein, denn RStudio arbeitet automatisch mit der neusten Version. Aber es können auch ältere Versionen verwendet beziehungsweise zwischen den Versionen gewählt werden (siehe Tools &gt; Global Options &gt; General &gt; R Version). Was passiert mit meinen installierten Paketen? Die installierten Pakete bleiben grundsätzlich erhalten. Mittels der Funktion .libPaths() sehen Sie auch, wo diese installiert sind.2 Weiter ist der Umgang mit Paketen kein wirkliches Problem. Da Sie jeweils in Ihrem Code auch spezifizieren, welche Pakete Sie installieren und laden, würde Ihnen ein Fehler sofort auffallen. Dies kann zum Beispiel wiefolgt gemacht werden als Kode in einem Markdown: #install.packages(&quot;swirl&quot;) library(swirl) swirl() #install.packages(&quot;soc.ca&quot;) library(soc.ca) soc.mca(Datenset) Der install.packages()-Befehl ergibt eine Fehlermeldung beim sogenannten knitten, wenn dieser nicht als Kommentar formatiert ist. Der Fehler verweist darauf, dass R kein Repository via Markdown automatisch aufrufen kann. Zwei Möglichkeiten von Josias Bruderer, dies trotzdem zu umgehen (und den install.packages()-Befehl nicht einfach als Kommentar zu setzen): Eine Möglichkeit den Fehlerbefehl auszuschalten ist es, ein Repository anzugeben: install.packages(&quot;swirl&quot;, repos = &quot;https://cran.rstudio.com/&quot;) Eine andere und erweiterte Möglichkeit ist einen Befehl zu ergänzen, der jeweils nur dann ein Paket installiert, wenn dieses benötigt und noch nicht installiert ist: if (!require(&quot;swirl&quot;)) install.packages(&quot;swirl&quot;)   Installieren Sie das Paket swirl und laden Sie es. swirl ist eine in R implementierte interaktive Einführung in die Grundlagen von R! Hier ein Tipp von Julien Lattmann: Sollte dies nach dem Update von R und RStudio womöglich nicht funktioniert haben, dann lohnt es sich nochmals alles zu schliessen, ein paar Moment zu warten und dann nochmals neu zu probieren.   Rufen Sie die Funktion swirl() auf und spielen Sie ein wenig damit. Rufen Sie sich in Erinnerung, was Sie aus dem letzten Semester noch über die Arbeit mit R wissen! Notieren Sie sich, was Ihnen Sie noch kennen, was Ihnen neu vorkommt, und so weiter. Grundsätzlich ging es in dieser Teilaufgabe darum, einige Aspekte von R (erneut) kennenzulernen. Zwei zusammenhängende Dinge sollen hervorgehoben werden: Erwähnt wurde, dass das Programmieren verlangt, sehr genau zu schreiben und kleinste Ungenauigkeiten zu Fehlern führen (etwa ein fehlendes \"). Hier bietet RStudio eine Hilfe an, in dem der Kode farbig gekennzeichnet wird. Diese Hilfe war jedoch um Umgang mit swirl noch nicht ersichtlich, da im Paket direkt in der R Konsole gearbeitet wird. Dies führt zum zweiten Aspekt: Dieser Fokus auf die Konsole beim Paket swirl() ist eher die Ausnahm - und der Hauptteil des Kondierens erfolgt eigentlich immer im einem Skript beziehungsweise in einem Markdown-Dokument (siehe WP02).   Verwenden Sie ein wenig Zeit darauf, online nach R Tutorials, Foren, etc. zu suchen. Halten Sie die URLs von Seiten und Ressourcen fest, die Ihnen hilfreich und/oder wichtig vorkommen (aber unter Umständen noch etwas schwer zu durchschauen) ! Einige Hilfseiten aus Ihren Falllösungen: r-statistics.co: Einführung zu spezifischen Verfahren R Tutorial: viele kurze einführende Beiträge zu Aspekten von R. R Statistik Tutorial: Deutsche Beiträg zu einzelnen Verfahren. RStudio Community: Forum für RStudio R lernen: einige einführende Beiträge auf Deutsch R Forum: deutschsprachiges Forum zu R Einführung in R: ein weiteres, deutsches Bookdown als Einführung in R The R Graph Gallery: für fortgeschrittene Grafiken, insbesondere mit ggplot2 stack overflow: jede Menge Hilfe für alles um R (und Programmieren allgemein) github: eine andere Plattform  aber wiederum: jede Menge Hilfe für alles um R (und Programmieren allgemein) datacamp: kostenpflichte Tutorials für R 1.6 Ergänzung: Standardabweichung zwischen Grundgesamtheit und Stichprobe In der Einheit haben wir gesehen, dass sich eine von R berechnet Standardabweichung leicht von einer selbst berechneten Variante unterscheidet: y &lt;- c(2, 3, 4, 7, 8, 9) sd(y) ## [1] 2.880972 sqrt( sum((y - mean(y))^2) / (length(y)) ) ## [1] 2.629956 In der Hilfeseite der Funktion finden wir den Hinweis darauf, dass mit sd() ähnlich wie mit var() der sogennante unbiased estimator berechnet wird, der von n - 1 ausgeht. help(sd) Like var this uses denominator n - 1. [] The denominator n - 1 is used which gives an unbiased estimator of the (co)variance for i.i.d. observations. Die Standardabweichung wird berechnet, als würde man den Schätzwert für einer Grundgesamtheit aus einer Stichprobe berechnet (i.i.d = Independent and identically distributed random variables). Dies entspricht einer sogenannten Punkteschätzung (Diaz-Bone 2019, 155f): Bei der Punktschätzung errechnet man aus der Stichprobe einen Stichprobenkennwert und schätzt damit die entsprechende Maßzahl in der Grundgesamtheit. Der Stichprobenkennwert, mit dem die Schätzung erfolgt, heißt auch Schätzer. [] Die Standardabweichung eines metrischen Merkmals in der Stichprobe s ist dagegen keine erwartungstreue Schätzung für die Standardabweichung des metrischen Merkmals in der Grundgesamtheit. Aus einer Stichprobe kannman aber mit folgender Formel erwartungstreu schätzen: \\({\\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}}\\) Dabei wird n - 1 verwendet, da bei kleineren Stichprobenumfängen die Standardabweichung überschätzt wird. Bei grösseren Stichprobenumfängen wird dies dann immer weniger wichtig: z &lt;- rnorm(1000) #eine standardnormalverteilte Varialbe mit n = 50 sd(z) ## [1] 0.9829825 sqrt( sum((z - mean(z))^2) / (length(z)) ) ## [1] 0.9824909 References "],["wochenplan-02.html", "2 Wochenplan 02 2.1 Lernziele 2.2 Aufgaben", " 2 Wochenplan 02 Vorbereitung von der 02. auf die 03.Einheit. 2.1 Lernziele In der zweiten Seminarwoche geht es darum, die Grundlagen von R und RStudio zu repetieren und zu erweitern.3 Für den weiteren Verlauf wollen wir R als Sprache auffassen  sowohl als Programmiersprache als auch als Sprache in einem metaphorischen Sinn. Wir wollen also ein komplexes System zur Kommunikation kennenlernen. Wie bei einer anderen Sprache gibt es auch hier Zeichen mit Bedeutungen (ähnliche wie Nomen, Verben, . . . ) und Regeln zur Verknüpfung dieser Zeichen (ähnlich wie eine Grammatik). Diese Grundlagen gilt es alle erstmal kennenzulernen und zu verstehen. Am Anfang wird vieles schwer fallen, mit der Zeit gewinnt man aber Sicherheit. Der zentrale Punkt in dieser Vorstellung von R als Sprache ist dabei folgender: Wir lernen eine Sprache dadurch, dass wir sie immer wieder anwenden, Probleme lösen und vor allem auch Fehler machen. R als Software und als Programmiersprache hat eine steile Lernkurve und zu Beginn werden viele Probleme auftauchen. Im Umgang mit den Problemen soll allerdings auch eine eigene Arbeitsweise mit dem Programm erlernt werden (Fehlermeldungen lesen, Lösungsstrategien im Codieren erlernen, selber Hilfe suchen, . . . ). Für die beständige Erweiterung der Grundlagen und das Erlernen der Sprache R besteht eine Herausforderung darin, den Weg zwischen scheinbarer Trivialität und überfordernder Komplexität zu finden: Es gilt die kleinen Schritte ernstzunehmen, sonst werden die grossen Schritte sehr schnell mühsam. Für die zweite Seminarwoche lassen sich folgende Seminarziele festhalten: Sie können die verschiedenen Funktionsweisen der vier Fenster in RStudio erläutern. Sie verstehen den Unterschied zwischen der Arbeit in der Konsole und im Skript. Sie verstehen, wie und wozu man im Skript kommentiert. Sie haben R-Markdown als erweitertes Skript und Arbeitsinstrument kennengelernt. Sie wissen, was ein Arbeitsverzeichnis in R ist und wozu es gut ist. Sie verstehen das erste Grundelement der Sprache R: Funktionen Sie wissen, wie Funktionen aufgebaut sind Sie wissen, wie Sie sich Hilfe zu Funktionen holen; Sie wissen, was Argumente in einer Funktion bewirken. Sie verstehen das zweite Grundelement der Sprache R: Objekte Sie verstehen, was es bedeutet, dass in R alles ein Objekt ist; Sie wissen, wie man sich die jeweils aktuell verfügbare Objekte anzeigen lässt; Sie haben das Zusammenspiel von Funktionen und Objekte kennengelernt; Sie kennen bereits drei verschiedenen Arten von Objekten. 2.2 Aufgaben Fassen Sie noch einmal für sich und in eigenen Worten die Funktionen der vier Fenster von RStudio zusammen. Oben links findet sich in R-Studio das Skript-Fenster, in dem Befehle eingegeben und kommentiert werden können. Ausgeführt werden diese Befehle erst, wenn Sie Ctrl und Enter drücken (bzw. Cmd &amp; Enter). Diese Eingabe von Kodezeilen wird ergänzt durch die direkte Eingabe in der Konsole. In diesem Fenster läuft das eigentliche Programm R (es ist also dieselbe Ansicht wie wenn Sie R ohne grafische Benutzeroberfläche starten würden).4 Im Gegensatz zum Skript können hier Befehle nur immer einzeln eingegeben und sie müssen dann direkt ausgeführt werden. Dies ermöglicht ein schneller ausprobieren, aber eben kein wirklich speichern, beständiges überarbeiten, kommentieren und eine klare Dokumentation des Ablaufs, wie dies im Skript erfolgen kann. Die beiden Fenster zur Eingabe von Kode werden vom Environment-Fenster ergänzt. Hier finden sich die abgespeicherten Objekte sowie in den weiteren Reitern die bisher ausgeführten Befehle (History), aber auch eine erweitere Netzwerk- bzw. Serverumgebung (Connections, Build, ), falls Sie z.Bsp. mit weiteren Personen an einem Projekt arbeiten. Im vierten Fenster werden Grafiken, Hilfeseiten, die Vorschau für geknittete Dokumente und auch die Ordnerstruktur angezeigt. Sie finden diverse Einstellungsoptionen zu den vier Fenster und deren Anordnung unter Tools &gt; Gobal Options. Dort können Sie etwa unter dem Reiter General die Option zum Save Workspace to RData on exit zu Never wechseln. Dies führt dazu, dass Ihre Environment beim Verlassen von R immer gelöscht wird. Dies ist nicht etwa ein Nachteil, sondern eine Technik die Sie dazu veranlasst, alle benötigen Schritte in Ihrem Kode unterzubringen. Weiter könne Sie unter dem Reiter Spelling auch noch die Rechtschreibefunktion deaktiveren, da das Feature noch nicht wirklich für die deutsche Rechtschreibung zu funktionieren scheint (bzw. die Ergänzung von neuen Wörterbüchern nicht fehlerfrei abläuft). Die Ordnerstruktur im Reiter Files des vierten Fensters hängt mit Ihrem aktuellen Arbeitsverzeichnis zusammen. Ein Arbeitsverzeichnis ist der Ort, auf den R immer als Erstes zugreift und wo Dinge automatisch abgelegt werden. Dieses können über die Menüsteuerung Session &gt; Set Working Directory &gt; Choose Directory oder über den Befehl setwd() definieren (erstere Variante ist etwas einfacher). Der getwd() Befehl wiederum gibt das aktuell festgelegte Verzeichnis aus. Speichern Sie jeweils Ihr aktuelles Arbeitsverzeichnis als Teil des Markdowns, z.Bsp. so: setwd(&quot;C:/Users/SchweglG/R_Daten/HS20/E3&quot;) #Dies dient in einem Skript oder einem Markdown als Erinnerung, ... #...wo Ihr Arbeitsverzeichnis liegt (und damit wo Sie Ihre Daten wiederfinden)   Was ist der Vorteil der Arbeit im Skript gegenüber dem Schreiben von Code direkt in der Konsole? Und was könnten dann die Vorteile davon sein, mit R Markdown zu arbeiten? Und wann könnten Sie trotzdem besser mit einem klassischen Skript arbeiten? Schauen Sie sich auch die Formatierungsmöglichkeiten für Fliesstext in den Cheatsheets zu R Markdown an (siehe hier)! Vorteile der Arbeit im Skript: Da Kodezeilen nicht sofort ausgeführt werden müssen ermöglicht das Skript die Strukturierung und Abspeicherung von Befehlen (Delia Bazzigher). Ein weiterer Vorteil kann es sein, komplexe Funktion, die man z.B. in einem RMarkdown braucht, in einem Skript zu hinterlegen, um im RMarkdown nicht zu viel Code einzubauen, damit sich die Leser:innen nicht darum kümmern müssen (Fabio Keller). Vorteile der Arbeit in RMarkdown: RMarkdown bietet die Möglichkeit, Kodezeilen (die Chunk) auf vielfältige Weise mit Text zu ergänzen und diesen Text zu formatieren: bspw. Bold, Italics, Aufzählungszeichen, Übertitel in unterschiedlichen Grössen, etc. []. Auch zwingt es den User, genau zu arbeiten, da [ein RMarkdown] geschlossen funktioniert (bspw. ein Objekt kann sich im Environment befinden, aber nicht im Universum des Markdowns) (Valentina Meyer). Vorteile der Konsole: In der Konsole kann über die beiden Pfeiltasten runter und rauf durch bisher ausgeführte Befehle gescrollt werden. So können Sie die Rechnung erneut aufrufen und dem Objekt x zuweisen. Ebenfalls können in der Konsole Dinge ausprobiert oder Hilfefunktionen aufgerufen werden.   Öffnen Sie eine neue R Markdown Datei. Versuchen Sie Ihre bisherigen Notizen zu den Aufgaben und Ihren Code in dieser Datei unterzubringen (falls Sie dies nicht schon gemacht haben). Arbeiten Sie für die folgenden Aufgaben mit dieser Datei weiter.   Versuchen Sie allgemein zu beschreiben, was Funktionen und was Objekte sind. Fügen Sie eine kurze Erläuterung in Ihr Markdown-Dokument zur Frage ein, was im folgenden Code jeweils Funktionen und was Objekte (und wenn letzteres, welche Art von Objekt) sind: sqrt(x) Funktion(en): sqrt() Objekt(e): x - eine undefiniertes Objekt help(&quot;sqrt&quot;) Funktion(en): help() Objekt(e): sqrt() - ein Funktion als Objekt, hier im Charakterformat ausgeschrieben Warum erfolgt jetzt die Schreibweise von sqrt in der Art und Weise? Gehen wir dazu kurz in die Hilfefunktion selber: ?help Funktion(en): ? als andere Schreibweise von help() Objekt(e): help() - ein Funktion als Objekt, hier aber nicht im Charakterformat ausformuliert. Dort finden wir die Spezifizierung bei Topic: usually, a name or character string specifying the topic for which help is sought. A character string (enclosed in explicit single or double quotes) is always taken as naming a topic. Was heisst jetzt ein Charakter-String? Es gibt eben nicht nur numerische Daten x1 &lt;- 5 sondern auch textliche oder Charakterdaten x2 &lt;- &quot;fünf&quot; class(x1) ## [1] &quot;numeric&quot; typeof(x1) ## [1] &quot;double&quot; class(x2) ## [1] &quot;character&quot; typeof(x2) ## [1] &quot;character&quot; Warum zeigt R uns zwei verschiedene Merkmale bei x1 an? R speichert Zahlen auf eine bestimmte Art und Weise ab, nämlich normalerweise als sogennante doubles oder als Zahlen mit Nachkommastellen. Man könnte die 5 auch als Integer (also als Zahl ohne Kommata) abspeichern: x3 &lt;- 5L class(x3) ## [1] &quot;integer&quot; typeof(x3) ## [1] &quot;integer&quot; Das brauchen wir allerdings kaum im alltäglichen Umgang mit R. y &lt;- c(1, 3, 4, 5, 6, 7, NA) #und z &lt;- c(7, 8, 10, 11) Funktion(en): &lt;-, c() Objekt(e): y, z - ein Vektor; 1, 3, 4, 5, 6, 7, 7, 8, 10, 11 - jeweils numerische Objekte; NA - ein fehlender Wert Berechnen des arithmetischen Mittels: (7+ 8+ 10+ 11) / 4 ## [1] 9 sum(z) / length(z) ## [1] 9 mean(z) ## [1] 9 #Was passiert hier? mean(y) ## [1] NA mean(y, na.rm = T) ## [1] 4.333333 Hier haben wir wiederum ein Argument einer Funktion spezifiziert, um die fehlenden Werte auszuklammern. Wichtig: NA ist jetzt nicht dasselbe wie ein Charakter-Datum NA: y1 &lt;- c(1, 3, 4, 5, 6, 7, NA) y2 &lt;- c(1, 3, 4, 5, 6, 7, &quot;NA&quot;) mean(y1, na.rm = TRUE) ## [1] 4.333333 mean(y2, na.rm = TRUE) ## Warning in mean.default(y2, na.rm = TRUE): Argument ist weder numerisch noch ## boolesch: gebe NA zurück ## [1] NA und was ist jetzt mit dem Vektor y1 passiert? Wir sehen, dass Vektoren nur ein Datenformat enthalten können. Ansonsten müssten wir den Vektor als Liste speichern. Listen sind daher ein weiterer Objekttyp (mit dem wir aber nur selten arbeiten werden). is.logical(TRUE) Funktion(en): is.logical() Objekt(e): TRUE - ein logisches Datenformat Was ist TRUE? typeof(TRUE) ## [1] &quot;logical&quot; class(TRUE) ## [1] &quot;logical&quot; TRUE und FALSE Dies sind logische Daten  also Daten die angeben ob etwas wahr oder falsch ist. Wir können auch daraus einen Vektor machen: lv &lt;- c(TRUE, TRUE, FALSE, FALSE, TRUE) Und was passiert wohl hierbei? TRUE + FALSE ## [1] 1 Es sind also einfach auch Werte: 1 oder 0. mean(y1, na.rm = 1) ## [1] 4.333333 Zusammenfassung Wir haben jetzt bereits drei verschiedene Datenarten kennengelernt: numerische Daten Charakterdaten logische Daten. Weiter haben wir bereits drei verschiedene Objekttypen kennengelernt: einzelne Werte Vektoren (als eine Reihe von Werten derselben Datenart) Listen (also Reihe von Werten derselben Datenart) Diese drei Objekttypen werden in der 5. Aufgabe ergänzt von den sogenannten Matrizen.   Sehen Sie sich die Hilfeseite der Funktion matrix() an. Wozu dient diese Funktion? Welche Argumente akzeptiert / benötigt sie und wozu dienen diese? Illustrieren Sie die Funktionsweise anhand von einem Beispiel. Die Funktion matrix() erlaubt es uns ein zweidimensionales Objekt aus einem Set von Werten (einem Vektoren) zu erstellen: Ein Beispiel von Vanesse Leutener: Beispiel: Bei einer kleinen Umfrage wurden 6 Befragte nach Geschlecht und der Höhe des monatlichen Einkommens befragt. Geschlecht 1 = weiblich 2 = männlich 3 = divers Monatliches Einkommen 1 = weniger als 800 CHF 2 = ab 800-1500 CHF 3 = ab 1500-3000 CHF 4 = ab 3000-6000 CHF 5 = mehr als 6000 CHF Geschlecht &lt;- c(2, 3, 1, 1, 1, 2) Einkommen &lt;- c(3, 5, 4, 1, 5, 5) #so? matrix(Geschlecht, Einkommen) ## [,1] [,2] ## [1,] 2 1 ## [2,] 3 1 ## [3,] 1 2 #so? matrix(c(Geschlecht, Einkommen)) ## [,1] ## [1,] 2 ## [2,] 3 ## [3,] 1 ## [4,] 1 ## [5,] 1 ## [6,] 2 ## [7,] 3 ## [8,] 5 ## [9,] 4 ## [10,] 1 ## [11,] 5 ## [12,] 5 #so sollte das passen: matrix(c(Geschlecht, Einkommen), nrow = 6, ncol = 2) ## [,1] [,2] ## [1,] 2 3 ## [2,] 3 5 ## [3,] 1 4 ## [4,] 1 1 ## [5,] 1 5 ## [6,] 2 5 #Warum stimmt das nicht? matrix(c(Geschlecht, Einkommen), nrow = 6, ncol = 2, byrow = T) ## [,1] [,2] ## [1,] 2 3 ## [2,] 1 1 ## [3,] 1 2 ## [4,] 3 5 ## [5,] 4 1 ## [6,] 5 5 Bei der Anordnung von Zeilen und Spalten wollen wir jeweils Fälle/Personen als Zeilen und Variablen als Spalten darstellen. Und wie könnte wir diese Matrix beschriften? LV_G &lt;- matrix(c(Geschlecht, Einkommen), nrow = 6, ncol = 2) colnames(LV_G) &lt;- c(&quot;Geschlecht&quot;, &quot;Einkommen (Kat.)&quot;) rownames(LV_G) &lt;- c(&quot;F1&quot;, &quot;F2&quot;, &quot;F3&quot;, &quot;F4&quot;, &quot;F5&quot;, &quot;F6&quot;) LV_G ## Geschlecht Einkommen (Kat.) ## F1 2 3 ## F2 3 5 ## F3 1 4 ## F4 1 1 ## F5 1 5 ## F6 2 5 Ein Beispiel von Delia Bazzigher: Eine Studie untersucht die mediale Präsenz der aktuellen Geschehnisse in Afghanistan in der deutschsprachigen Schweiz im September. Dazu werden z.B. die Tageszeitungen untersucht. Es wird eine Häufigkeitszählung in den Zeitungen Tages-Anzeiger, NZZ, Luzerner Zeitung und Blick durchgeführt. Gezählt werden die Anzahl Artikel sowie deren jeweilige Position, d.h. neutral/sachlich, negati/verurteilend oder positiv/unterstützend (die Codierung dieser Kategorien wird hier nicht erläutert). mTageszeitungen &lt;- matrix(c(7,3,0, 12,7,0, 5,3,0, 8,2,3), nrow = 3, ncol = 4, byrow = FALSE, dimnames = list(c(&quot;negativ&quot;, &quot;neutral&quot;, &quot;positiv&quot;), c(&quot;Tages-Anzeiger&quot;, &quot;NZZ&quot;, &quot;Luzerner Zeitung&quot;, &quot;Blick&quot;))) mTageszeitungen ## Tages-Anzeiger NZZ Luzerner Zeitung Blick ## negativ 7 12 5 8 ## neutral 3 7 3 2 ## positiv 0 0 0 3 Was zeigt sich hier für ein Datenformat? Das ist eigentlich bereits abstrahiert! Wir könnten diese Kombination und das erstellen einer Matrix auch mit anderen Funktionen erreichen: LV_G1 &lt;- cbind(Geschlecht, Einkommen) LV_G2 &lt;- rbind(Geschlecht, Einkommen) LV_G1 ## Geschlecht Einkommen ## [1,] 2 3 ## [2,] 3 5 ## [3,] 1 4 ## [4,] 1 1 ## [5,] 1 5 ## [6,] 2 5 LV_G2 ## [,1] [,2] [,3] [,4] [,5] [,6] ## Geschlecht 2 3 1 1 1 2 ## Einkommen 3 5 4 1 5 5 typeof(LV_G1) ## [1] &quot;double&quot; class(LV_G1) ## [1] &quot;matrix&quot; &quot;array&quot; Was ist noch ein spezifische Eigenschaft einer Matrix? Geschlecht_c &lt;- c(&quot;m&quot;, &quot;d&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;) LV_G3 &lt;- cbind(Geschlecht_c, Einkommen) LV_G3 ## Geschlecht_c Einkommen ## [1,] &quot;m&quot; &quot;3&quot; ## [2,] &quot;d&quot; &quot;5&quot; ## [3,] &quot;f&quot; &quot;4&quot; ## [4,] &quot;f&quot; &quot;1&quot; ## [5,] &quot;f&quot; &quot;5&quot; ## [6,] &quot;m&quot; &quot;5&quot; typeof(LV_G3) ## [1] &quot;character&quot; class(LV_G3) ## [1] &quot;matrix&quot; &quot;array&quot; Matrizen können nur ein Datenformat speichern - und wandeln daher verschiedene Datenarten in das Niedrigste um. Deshalb arbeiten wir eher mit dem Datenformat bzw. Objekttyp des sogennanten data.frames (oder in ggplot dann tybbles). Dieser Objekttyp ermöglicht ein Speichern von verschiedenen Datenarten in einem zweidimensionalen Format: LV_G4 &lt;- data.frame(Geschlecht_c) LV_G4 &lt;- cbind(LV_G4, Einkommen) LV_G4 ## Geschlecht_c Einkommen ## 1 m 3 ## 2 d 5 ## 3 f 4 ## 4 f 1 ## 5 f 5 ## 6 m 5 typeof(LV_G4) ## [1] &quot;list&quot; class(LV_G4) ## [1] &quot;data.frame&quot; Falls Sie noch Probleme bei der Installation von R bzw. RStudio oder sonstige technische Schwierigkeiten haben, können Sie sich gerne weiterhin per E-Mail melden! Neuere Versionen von RStudio bieten im Fenster der Konsole auch die Möglichkeit, sogenannte Jobs auszuführen (ein Tpp von Josias Bruderer), siehe [hier]:(https://www.youtube.com/watch?v=EBlk1kRbKeU). "],["wochenplan-03.html", "3 Wochenplan 03 3.1 Lernziele 3.2 Aufgaben 3.3 Ergänzung: Seitenumbruch im RMarkdown", " 3 Wochenplan 03 Vorbereitung von der 03. auf die 04.Einheit. 3.1 Lernziele In der dritten Seminarwoche vertiefen wir Elemente der Programmiersprache R, die Sie bereits kennengelernt haben, und betten sie in neue Zusammenhänge ein. Vertieft wird nochmals der Umgang mit verschiedenen Datenarten und Objekttypen: Zahlen, Text und logische Werte sowie einzelne Werte, Vektoren und Matrizen. Neu wollen wir einige Möglichkeiten kennenlernen, über bestimmte Funktionen systematisch Vektoren zu definieren. Diese Grundlagen wollen wir dann nutzen, um erste statistische Inhalte bzw. Methoden einführen, nämlich die bivariaten Zusammenhänge von metrischen Variablen (Korrelation). Zusammenfassend lassen sich damit folgende Seminarziele festhalten: Sie können die drei bisher kennengelernten Objekttypen (einzelne Zahl, Vektor, und Matrix) kombinieren. Sie kennen die drei verschiedenen Datenarten von R: Sie verstehen die Rolle von numerischen Daten, Sie verstehen die Rolle von textförmigen Daten Sie verstehen die Rolle von logischen Daten. Sie können Vektoren mittels der Funktionen   seq()  sowie rep() definieren  und diese auf alle drei Arten von Daten anwenden. Sie verstehen, was eine Korrelation von zwei Variablen bedeutet und können bivariate Zusammenhänge in R berechnen und interpretieren. 3.2 Aufgaben Erstellen Sie vier verschiedene Vektoren mit je einer Länge von vier und verbinden Sie diese zu einer 4x4-Matrix. Die Funktion apply() erlaubt Ihnen, eine Funktionen wie z.B. mean() oder var() auf diese Matrix anzuwenden. Nur wie genau? Sehen Sie sich die Hilfe zu apply() an, probieren Sie die Funktion aus und versuchen Sie zu verstehen, wie sie genau funktioniert. Erläutern Sie apply() dann in eigenen Worten und mit Hilfe der von Ihnen erzeugten 4x4-Matrix! Zuerst können vier Vektoren erstellt werden: m1 &lt;- c(2,4,7,9) m2 &lt;- c(8,9,5,2) m3 &lt;- c(3,5,5,5) m4 &lt;- c(9,8,7,6) Nachdem die vier Vektoren erstellt wurden lassen sie sich über die rbind() und cbind() Funktionen auf zwei verschiedene Weisen zu einer Matrix verbinden, entweder zeilen- oder spaltenweise (siehe die Objekte ma und mb). ma &lt;- cbind(m1,m2,m3,m4) mb &lt;- rbind(m1,m2,m3,m4) ma ## m1 m2 m3 m4 ## [1,] 2 8 3 9 ## [2,] 4 9 5 8 ## [3,] 7 5 5 7 ## [4,] 9 2 5 6 mb ## [,1] [,2] [,3] [,4] ## m1 2 4 7 9 ## m2 8 9 5 2 ## m3 3 5 5 5 ## m4 9 8 7 6 Als nächster Schritt wendet apply() dann eine bestimmte Funktion auf ein Objekte an. Bei einem Objekt des Typs Matrix muss allerdings noch spezifiziert werden, ob die Funktion Zeilen- oder Spalteweise angewendet wird. Zeilen oder Spalten werden über die Zahlen 1 bzw. 2 definiert. apply(ma, 1, mean) ## [1] 5.5 6.5 6.0 5.5 apply(ma, 2, mean) ## m1 m2 m3 m4 ## 5.5 6.0 4.5 7.5 apply(mb, 1, mean) ## m1 m2 m3 m4 ## 5.5 6.0 4.5 7.5 apply(mb, 2, mean) ## [1] 5.5 6.5 6.0 5.5 So ergeben sich insgesamt vier Möglichkeiten, für die Berechnung der des Durchschnitts  allerdings finden sich nur zwei unterschiedliche Ergebnisse. Was passiert nun hier? apply(ma, c(1,2), mean) ## m1 m2 m3 m4 ## [1,] 2 8 3 9 ## [2,] 4 9 5 8 ## [3,] 7 5 5 7 ## [4,] 9 2 5 6 ma ## m1 m2 m3 m4 ## [1,] 2 8 3 9 ## [2,] 4 9 5 8 ## [3,] 7 5 5 7 ## [4,] 9 2 5 6 apply(ma, c(1,2), var) ## m1 m2 m3 m4 ## [1,] NA NA NA NA ## [2,] NA NA NA NA ## [3,] NA NA NA NA ## [4,] NA NA NA NA Hier wird eine Funktion sowohl auf Spalten als auch auf Zeilen angewendet - das heisst einfach auf die einzelnen Werte. Von einem einzelnen Wert kann man dann das arithmetische Mittel berechen, hingegen nicht die Varianz: var(2) ## [1] NA mean(2) ## [1] 2 Als Ergänzung finden Sie hier noch ein sozialwissenschaftliches Beispiel einer solchen Matrix von Katrin Oesch: Alter &lt;-c(32,61,45,29) Arbeitsjahre &lt;- c(14,33,20,2) Monatseinkommen &lt;- c(5500,8700,10200,3750) Ausbildungsjahre &lt;- c(3,12,9,7) DS_1 &lt;-c(Alter, Arbeitsjahre, Monatseinkommen, Ausbildungsjahre) Matrix_1 &lt;- matrix(DS_1, nrow = 4, ncol = 4, byrow = F) colnames(Matrix_1) &lt;- c(&quot;Alter&quot;,&quot;Ausbildungsjahre&quot;,&quot;Monatseinkommen&quot;,&quot;Ausbildungsjahre&quot;) apply(Matrix_1, MARGIN=2, FUN = mean) ## Alter Ausbildungsjahre Monatseinkommen Ausbildungsjahre ## 41.75 17.25 7037.50 7.75   Erstellen Sie je einen Vektor mit numerischen Daten, textförmigen Daten und logischen Daten. Die Funktionen as.numeric(), as.character() und as.logical() lassen Sie eine Datenarten in eine andere zwingen bzw. als eine andere Datenart interpretieren. Wann funktioniert dies? Und wo sind die Grenzen dieses Zwingens? a &lt;- c(0, 1, 2) b &lt;- c(TRUE, FALSE, T) #Die logischen Objekte können sowohl ausgeschreiben als auch lediglich als T und F aufgeführt werden c &lt;- c(&quot;null&quot;, &quot;eins&quot;, &quot;zwei&quot;) #die Anführungs- und Schlusszeichen beachten Nachdem wir die Vektoren definiert haben, können wir deren Typ bestimmen und mit den as.-Funktionen spielen: is.numeric(a) ## [1] TRUE is.logical(b) ## [1] TRUE is.character(c) ## [1] TRUE as.numeric(b) ## [1] 1 0 1 as.numeric(c) ## Warning: NAs durch Umwandlung erzeugt ## [1] NA NA NA as.logical(a) ## [1] FALSE TRUE TRUE as.logical(c) ## [1] NA NA NA as.character(a) ## [1] &quot;0&quot; &quot;1&quot; &quot;2&quot; as.character(b) ## [1] &quot;TRUE&quot; &quot;FALSE&quot; &quot;TRUE&quot; Die Grenzen dieses Zwingens der as.-Funktionen finden sich auf der einen Seite im Zusammenhang zu den logischen Daten: Dies funktioniert nur mit numerischen Daten  und alles über 1 wird als TRUE interpretiert. Auf der anderen Seite zeigt sich die Grenze bei ausgeschriebenen Zahlen in einem Charakter-Vektor. Diese können von R nicht in numerische oder auch logische Daten umformuliert werden. Allerdings gilt es eine wichtige Ausnahme zu beachten, nämlich wenn Zahlen als Charakter aufgeführt wurden: c2 &lt;- c(&quot;0&quot;, &quot;1&quot;, &quot;2&quot;) is.character(c2) ## [1] TRUE as.numeric(c2) ## [1] 0 1 2 as.logical(as.numeric(c2)) ## [1] FALSE TRUE TRUE Insbesondere beim Importieren von Datensätzen, die nicht in einem Rohformat gespeichert sind (etwa .sav Dateien), kann es passieren, dass metrische Variablen als Charakter-Vektor eingelesen werden.   Definieren Sie folgende Vektoren mittels der Funktionen seq() und rep(): Vektor 3a: 1 2 3 4 5 6 7 8 9 10 seq(from = 1, to = 10, by = 1) ## [1] 1 2 3 4 5 6 7 8 9 10 #oder einfacher noch: seq(1, 10, 1) ## [1] 1 2 3 4 5 6 7 8 9 10 Vektor 3b: 1 1 1 2 2 2 3 3 3 c(rep(1,3), rep(2,3), rep(3,3)) ## [1] 1 1 1 2 2 2 3 3 3 Vektor 3c: Die Zahlen des Vektors b als ausgeschriebene Wörter c(rep(&quot;Eins&quot;,3), rep(&quot;Zwei&quot;,3), rep(&quot;Drei&quot;,3)) ## [1] &quot;Eins&quot; &quot;Eins&quot; &quot;Eins&quot; &quot;Zwei&quot; &quot;Zwei&quot; &quot;Zwei&quot; &quot;Drei&quot; &quot;Drei&quot; &quot;Drei&quot; Vektor 3d: 1 4 7 10 13 seq(1, 13, 3) ## [1] 1 4 7 10 13 Vektor 3e: 1 4 9 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 1100 14899 678999 V3e &lt;- c(1, 4, 9, seq(50, 100, 1), 1100, 14899, 678999) V3e ## [1] 1 4 9 50 51 52 53 54 55 56 ## [11] 57 58 59 60 61 62 63 64 65 66 ## [21] 67 68 69 70 71 72 73 74 75 76 ## [31] 77 78 79 80 81 82 83 84 85 86 ## [41] 87 88 89 90 91 92 93 94 95 96 ## [51] 97 98 99 100 1100 14899 678999   Was macht das Argument trim für die Funktion mean()? Wozu könnten Sie dieses Argument in einem sozialwissenschaftlichen Kontext nutzen? Spezifizieren Sie das Argument auf sinnvolle Weise, um das arithmetische Mittel des in Aufgabe 3e erstellten Vektors zu berechnen! Mittels des Arguments trim können bestimmte Anteile der Elemente eines Vektors für die Berechnung entfernt werden. Dies ermöglicht es Extremwerte (sowohl sehr hohe als auch sehr tiefe Werte) aus der Berechnung zu entfernen. mean(V3e) ## [1] 12260.3 mean(V3e, trim = 0.1) ## [1] 75 Was erfolgt nun genau mit dem Wert 0.1? Eine Formel von Fabio Keller: V3e ## [1] 1 4 9 50 51 52 53 54 55 56 ## [11] 57 58 59 60 61 62 63 64 65 66 ## [21] 67 68 69 70 71 72 73 74 75 76 ## [31] 77 78 79 80 81 82 83 84 85 86 ## [41] 87 88 89 90 91 92 93 94 95 96 ## [51] 97 98 99 100 1100 14899 678999 #...die zwei kleinsten und grössten Werte 0.05 * length(V3e) ## [1] 2.85 mean(V3e, trim = 0.05) ## [1] 93.09434 #...die drei kleinsten und grössten Werte 0.06 * length(V3e) ## [1] 3.42 mean(V3e, trim = 0.06) ## [1] 75 Warum ändert sich allerdings nichts am Durchschnittswert, egal wie ich trim im Bereich zwischen 0.06 und 0.5 definiere (0.5 ist der Maximalwert)? mean(V3e, trim = 0.1) ## [1] 75 mean(V3e, trim = 0.49) ## [1] 75 Da der Vektor eine Zahlenreihe ist bleibt der Durchschnittswert immer gleich wenn jeweils von den Enden dieselbe Anzahlobjekte entfernt wird.   Sie sollen verschiedene Paare von Vektoren mittels der Funktionen c(), rep() und seq() definieren, die jeweils unterschiedlich korrelieren. Diese Vektoren  d.h. Variablen  und deren Korrelationen sollen sozialwissenschaftlichen Phänomenen entsprechen. Berechnen Sie jeweils den Korrelationskoeffizienten. Ein Paar von Vektoren (mit je einer Länge von rund 100) soll eine Korrelation von ca. 0.4 aufweisen, anhand eines Beispiels von Vanessa Leutner: Freundschaften &lt;- rep(c(2, 3, 10, 4, 1, 6, 3, 3, 3, 4, 4, 1, 9, 4, 2, 8, 5, 6, 3, 4), each = 5) Lebenszufriedenheit &lt;- rep(c(2, 2, 6, 3, 4, 5, 1, 4, 3, 5, 1, 5, 6, 1, 1, 4, 3, 4, 7, 1), each =5) cor(Freundschaften, Lebenszufriedenheit) ## [1] 0.403411 Ein Paar von Vektoren (mit je einer Länge von rund 100) soll eine sehr starke Korrelation aufweisen, anhand eines Beispiels von Dario Haab: VektorA &lt;- seq(1, 100, by = 1) VektorB &lt;- rep(1:50, each = 2) cor(VektorA, VektorB) ## [1] 0.99985 Wie könnte dieses Beispiel sozialwissenschaftlich interpretiert werden? Die Verteilung könnte beispielsweise von einer Firma stammen, die ihre Löhne alle zwei Jahre bei den Mitarbeiter*innen erhöht. Ein Paar von Vektoren (mit je einer Länge von rund 100) soll eine schwache negative Korrelation aufweisen, anhand eines Beispiels von Julien Lattmann: Alter &lt;- rep(seq(15, 65, 2), 4) Nutzungsdauer_Smartphone &lt;- c(rep(seq(120, 30, -10), 3), rep(70, 20), rep(50, 20), rep(30, 20), rep(10, 14)) cor(Alter, Nutzungsdauer_Smartphone) ## [1] -0.1453604 3.3 Ergänzung: Seitenumbruch im RMarkdown In der Einheit wurde noch kurz besprochen, wie es in RMarkdown möglich ist, einen Seitenumbruch für das geknittete PDF einzufügen. Josias Bruderer schlug vor \\newpage im Text einzufügen. Eine weitere Möglichkeiten könnten auch \\pagebreaksein (siehe hier). "],["wochenplan-04.html", "4 Wochenplan 04 4.1 Lernziele 4.2 Aufgaben", " 4 Wochenplan 04 Vorbereitung von der 04. auf die 05.Einheit. 4.1 Lernziele Nachdem wir uns bereits verschiedenste Grundlagen für die Arbeit mit R Studio erarbeitet und erste statistische Inhalte kennengelernt haben stehen nun einige ergänzende Aspekte an. Mittels diesen Aspekten wollen wir uns immer näher an die tatsächliche Arbeit der sozialwissenschaftlichen Datenanalyse bewegen: grafische Techniken sollen ausprobiert, Zufallsvariablen kennengelernt und weitere Datensätze erstellt werden. Der vierte Wochenplan soll uns so nicht zuletzt vorbereiten, das Prinzip der Inferenzstatistik mittels R zu verstehen. Konkret lassen sich folgende Seminarziele festhalten: Sie können zwei Variablen in einem Streudiagramm darstellen und die Darstellungen interpretieren. Sie kennen den Unterschied von Gleichverteilungen und Normalverteilungen und können in R entsprechend verteilte Zufallsvariablen erstellen. Sie können metrische Verteilungen in Histogrammen darstellen. Sie haben erste Techniken kennenglernt, wie Grafiken erweitert und kombiniert werden können. Sie verstehen, was ein Dataframe in R ist und können die Unterschiede zu einer Matrix benennen. Sie haben sich in R die Grundlagen für ein Verständnis von Inferenzstatistik allgemein und des Stichprobenfehlers im Besonderen erarbeitet. 4.2 Aufgaben Nutzten Sie plot() um die Verteilung eines Variablen-Paars darszustellen, das Sie im Rahmen des letzten Wochenplans und der Aufgabe zu den Korrelationen erstellt haben. Verwenden Sie weiter auch eine Farbe für den Plot und verweisen Sie im Titel sowie in den Achsenbeschriftungen auf das sozialwissenschaftliche Phänomen, das Sie darstellen. Als Beispiel dient hier ein Variablenpaar von Delia Bazzigher, das eine schwach-positive Korrelation aufweist: age &lt;- c(11, 16, rep(seq(12, 25, 3), 7), 17, 27, rep(seq(12, 70, 3), 3), 22) screenhours &lt;- c(rep(c(20, 25, 24, seq(23, 70, 3), 19, 20), 4), rep(40:47, 2)) cor(age, screenhours) ## [1] 0.2039158 Zuerst ein simpler Plot plot(age, screenhours) dann wir dann über diverse Elemente erweitern können: help(plot) Implizit haben wir immer schon einen Typ bestimmt, nämlich type= \"p\" davon können wir die Punktform und Grösse bestimmten: pch = &amp; lwd = Titel (auch mit Zeilenumbruch): main = Achsenbeschriftung: xlab = &amp; ylab = Grössen des Textes: cex.lab = Farbe (sowohl über eine schriftliche Bezeichnung als auch über Zahlen): col = () plot(age, screenhours, type = &quot;p&quot;, pch = 15, lwd = 2, main = &quot;Zusammenhang zwischen Alter und \\nZeit, die vor einem Bildschirm verbracht wurde&quot;, xlab = &quot;Alter (j)&quot;, ylab = &quot;Bildschirmzeit (h)&quot;, cex.lab = 1.2, col = c(1:100) )   Erstellen Sie Vektoren für eine Gleichverteilung mittels runif() und für eine Normalverteilung mittels rnorm(). Diese Vektoren sollen als Variablen Körpergrössen repräsentieren. Erstellen Sie die jeweils zwei Vektoren in unterschiedlichen Längen, und zwar m Folgenden werden die in der Aufgabe verlangten Verteilungen als Histogramme dargestellt. Insbesondere bei grösseren Fallzahlen werden so die Eigenschaften der Verteilungen deutlicher: Bei der Gleichverteilung hat jede Ausprägung dieselbe Auftrittswahrscheinlichkeit. Das heisst, dass jede Körpergrösse zwischen Minimal- und Maximalwert (die beiden Parameter der Funktion runif()) mit derselben Häufigkeit vorkommt. Dies entspricht aber nicht der empirischen Realität von Körpergrössen. Bei der Normalverteilung gruppieren sich die meisten Werte um den Mittelwert von 170cm, während kleine und grosse Werte mit zunehmender Abweichung immer weniger häufig auftreten. Dies wiederum entspricht stärker der tatsächlichen, empirischen Verteilungen von Körpergrössen. Unterschieden werden die Normalverteilungen (auch in der Funktion rnorm()) über die beiden expliziten Parameter des arithmetischen Mittels und der Standardabweichung (Diaz-Bone 2019, 140f). mit je 10 Fällen: summary(runif(10,150,210)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 157.2 167.6 182.2 179.8 189.6 203.2 summary(rnorm(10,180,10)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 163.0 168.1 176.7 174.1 180.3 182.9 hist(runif(10,150,210)) hist(rnorm(10,180,10)) mit je 30 Fällen: summary(runif(30,150,210)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 151.6 169.0 184.8 182.2 199.1 209.5 summary(rnorm(30,180,10)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 160.9 177.0 181.2 181.3 186.8 201.0 hist(runif(30,150,210)) hist(rnorm(30,180,10)) und mit je 1000 Fällen: summary(runif(1000,150,210)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 150.0 165.4 180.4 180.6 196.1 210.0 summary(rnorm(1000,180,10)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 150.0 173.0 179.2 179.4 185.7 208.5 hist(runif(1000,150,210)) hist(rnorm(1000,180,10)) Als Ergänzung: runif() und rnorm() werden noch von der Funktion rbinom() ergänzt, die zufällige binomiale Verteilungen erstellt. Dies sind Verteilungen die angeben, ob ein Ereignis bei einer bestimmten Wahrscheinlichkeit eingetreten ist oder nicht. Damit kann zum Beispiel aufgezeigt werden, wie oft Sie bei zehn Münzwürfen Kopf bekommen (Beispiel 1), oder auch wie oft Sie bei 100 Mal würfeln mit zwei Würfeln zwei Sechsen erzielen (Beispiel 2). rbinom(10, size = 1, prob = 0.5) #Beispiel 1 ## [1] 1 1 0 1 0 0 0 0 1 1 rbinom(100, 2, (1/6 * 1/6)) #Beispiel 2 ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 ## [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ## [75] 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0   Als nächstes sollen Sie die beiden Verteilungen aus der Aufgabe 2, die 1.000 Fälle aufweisen, grafisch darstellen. Wie können die 10 Klassen erreicht werden? Die Variante breaks = 10 scheint nicht immer zu funktionieren, sondern ist lediglich ein zu erreichender Vorschlag (vgl. die Hinweise in der Hilfe zu hist() unter dem Punkt breaks). Wir können aber die Punkte, wo die Klassenumbrüche erfolgen sollen, selber bestimmen. Insbesondere bei Gleichverteilungen mit deren klaren Grenzen funktionerit dies einfach über eine Sequenz. Bei der Normalverteilung müssten wir die Punkte, wo die Breaks erfolgen, teilweise noch händisch bestimmen. hist(runif(1000,150,210), main = &quot;Körpergrösse, \\n gleichverteilt&quot;, xlab = &quot;Körgpergrösse (cm)&quot;, col = &quot;darkgreen&quot;, xlim = c(150,210), breaks = seq(from=150, to=210, by= ((210-150)/10))) hist(rnorm(1000,180,10), main = &quot;Körpergrösse, \\n normalverteilt&quot;, xlab = &quot;Körgpergrösse (cm)&quot;, col = &quot;orange&quot;, xlim = c(150,210), breaks = c(100, 156, 162, 168, 174, 180, 186, 192, 198, 204, 300)) Über die Funktion abline() kann dem aktuellen Plot eine Linie hinzugefügt werden. Fügen Sie jeweils einem Histogramm den Mittelwert der anderen Verteilung als vertikale Linie hinzu (ebenfalls in der entsprechenden Farbe). hist(runif(1000,150,210), main = &quot;Körpergrösse, \\n gleichverteilt&quot;, xlab = &quot;Körgpergrösse (cm)&quot;, col = &quot;darkgreen&quot;, xlim = c(150,210), breaks = seq(from=150, to=210, by= ((210-150)/10))) abline(v = mean(rnorm(1000,180,10)), col = &quot;orange&quot;, lwd = 3) hist(rnorm(1000,180,10), main = &quot;Körpergrösse, \\n normalverteilt&quot;, xlab = &quot;Körgpergrösse (cm)&quot;, col = &quot;orange&quot;, xlim = c(150,210), breaks = c(100, 156, 162, 168, 174, 180, 186, 192, 198, 204, 300)) abline(v = mean(runif(1000,150,210)), col = &quot;darkgreen&quot;, lwd = 3) Hinweis: Es konnte zu Problemen beim knitten führen, wenn die abline() in einem anderen Codechunk aufgerufen wurde. Das Argument add = TRUE lässt Sie eine neue Grafik über die aktuelle Grafik legen. Versuchen Sie, auf diese Art Ihre beiden Histogramme in einer Grafik darzustellen. Hierzu können noch drei weitere Anpassungen vorgenommen werden: Die Dimensionen der Y-Achse können angepasst werden über ylim =. Die Farben der Histogramme können transparent gemacht werden in dem wir auf RGB Werte zurückgreifen (ein Vorschlag von Felix Sigrist). Wir können der Grafik noch eine Legende hinzufügen (ein Vorschlag von Josias Bruderer). hist(runif(1000,150,210), main = &quot;Körpergrössen&quot;, xlab = &quot;Körgpergrösse (cm)&quot;, col = &quot;darkgreen&quot;, xlim = c(145,215), ylim = c(0, 250)) #Funktion um die RGB-Daten der Farbe &quot;Orange&quot; zu erhalten: col2rgb(&quot;orange&quot;) ## [,1] ## red 255 ## green 165 ## blue 0 hist(rnorm(1000,180,10), col = rgb(255, 165, 0, 255/4, maxColorValue = 255), add = T) abline(v = mean(rnorm(1000,180,10)), col = &quot;orange&quot;, lwd = 3) abline(v = mean(runif(1000,150,210)), col = &quot;darkgreen&quot;, lwd = 3) legend(195, 200, legend = c(&quot;gleichverteil&quot;, &quot;normalverteilt&quot;), fill = c(&quot;darkgreen&quot;,&quot;orange&quot;), cex = 0.75 )   Erzeugen Sie ein Dataframe, das aus fünf Variablen besteht und 100 Fälle umfasst. Nutzen Sie dazu die verschiedenen Funktionen, die wir bisher kennengelernt haben (Zufallsvariablen, rep(), seq(), ). Probieren Sie ebenfalls, dass die fünfte Variablen dem Character Datenformat entspricht. Überlegen Sie sich einen sozialwissenschaftlichen Kontext für dieses Dataframe und benennen Sie die Variablen dementsprechend. Mit diesem Datensatz werden wir im nächsten Wochenplan weiterarbeiten. Für diese Aufgabe gab es viele tolle Beispiele. Hier zwei Varianten, einmal von Vanessa Leutner und einmal von Josias Bruderer: #Beispiel von Vanessa Leutner Einkommen &lt;- c(seq(from = 2000, to = 12000, length.out = 100)) Krankheitstage &lt;- rnorm(100, mean = 6.5, sd = 3) Ausbildungsjahre &lt;- rep(c(3,6,9,3,13,4,6,7,10,5), each = 10) Anzahl_Kinder &lt;- rep(c(1,0,1,2,3,2,3,5,6,7,1,2,3,4,5,3,2,1,4,5), each = 5) Geschlecht &lt;- rep(c(&quot;weiblich&quot;, &quot;männlich&quot;, &quot;weiblich&quot;, &quot;mänlich&quot;, &quot;divers&quot;), each = 20) df_vl &lt;- data.frame(Einkommen, Krankheitstage, Ausbildungsjahre, Anzahl_Kinder, Geschlecht) #Beispiel von Josias Bruderer df_jb &lt;- data.frame(alter = round(runif(100, 18, 64)), geschlecht = sample(rep(c(seq(1,3),2,3), 20)), tvdauer = sample(round(rnorm(100, 3, 1), 0)), happiness = sample(c(sample(1:10, 80, replace = T), rep(NA, 20))), prog = sample(rep(c(&quot;Kinder&quot;,&quot;Unterhaltung&quot;,&quot;Unterhaltung&quot;,&quot;News&quot;,&quot;Sport&quot;), 20))) Die Funktion round()wurde von vielen anderen auch genutzt. Eine andere Variante zum Runden hat noch Fabio Keller vorgeschlagen, nämlich mittels as.integer(): alter = as.integer(runif(100, 18, 64)) alter ## [1] 29 44 35 54 53 61 52 28 63 51 42 52 47 35 31 44 39 18 18 40 42 21 57 57 61 ## [26] 20 43 44 35 18 35 23 55 50 24 37 24 28 21 23 49 24 57 33 36 62 59 40 19 51 ## [51] 50 34 49 44 26 23 21 58 59 60 49 44 58 47 24 59 26 35 60 25 34 60 27 30 27 ## [76] 18 56 27 38 48 22 21 47 24 24 34 42 50 56 55 36 49 56 50 43 51 62 31 53 51 Zur Wiederholung: Was ist nochmals ein Objekt des Typs data.frames (Manderscheid 2017, 39)? Während () Vektoren weitgehend dem entsprechen, was in der sozialwissenschaftlichen Statistik und in anderen Auswertungsprogrammen als Variable bezeichnet wird, enthält das Objekt Dataframe mehrere Variablen: Die Zeilen eines Dataframes enthalten die Beobachtungen als Fälle, die Spalten die Faktoren und Vektoren als Variablen. Damit entspricht ein Dataframe einem zweidimensionalen, tabellarisch darstellbaren Datensatz. Die Faktoren und Vektoren in den Spalten müssen dabei die selbe Länge, d. h. die selbe Anzahl von Elementen haben, können dabei aber sowohl Zahlen als auch Buchstaben enthalten.   Bonusaufgabe: Das Paket ggplot2 (als Teil der grösseren Paketsammlung tidyverse) ermöglicht gegenüber der Basisversion von R besonders komplexe grafische Lösungen. Versuchen Sie Ihre Variablenverteilung aus Aufgabe 1 auch mit diesem Paket darzustellen. Sie können den Beispielcode unten nutzen (die von Ihnen zu ergänzende Aspekte sind mittels den beiden Zeichen &gt;&lt; im Code versehen) und gleichzeitig etwas recherchieren, wie ggplot2 funktioniert. p1 &lt;- ggplot(data = data.frame(cbind(&gt;Variable1&lt;, &gt;Variable2&lt;)), mapping = aes(x = &gt;Variable1&lt;, y = &gt;Variable2&lt;)) p1 + geom_point(size = &gt;Zahl&lt;, color = &quot;&gt;Farbe&lt;&quot;) + ggtitle(&quot;&gt;Ihr Titel&lt;&quot;) Das Paket ggplot2 hat eine sehr eigene Logik davon, wie Grafiken erstellt werden. Im Seminar selber werden wir in diese Logik nicht vertieft einsteigen. Ein gute Einführung in das Paket und dessen Möglichkeiten bietet das Buch Data Visualization (Healy 2019). References "],["wochenplan-05.html", "5 Wochenplan 05 5.1 Lernziele WP05 5.2 Hinweise Indizieren &amp; Subsetting 5.3 Aufgaben WP05", " 5 Wochenplan 05  zur Einheit der 05. und 06.Woche. 5.1 Lernziele WP05 In der kommenden Arbeitswoche geht es nun darum, zwei neue Aspekte der Arbeit mit R kennenzulernen und einzuüben: das Zugreifen auf einzelne Elemente und Teile von Datenobjekten (Indizieren und Subsetting) und die Kontrolle von längeren Befehlsabläufen mittels Schleifen (als erste einfache, eigene Funktionen). Für das Kennenlernen und Einüben der beiden Aspekte dient uns das data.frame-Objekt, das im Rahmen des letzten Wochenplans erstellt wurde. Folgende Lernziele lassen sich festhalten: Sie können gezielt auf einzelne Teile eines Datenobjekts zugreifen und kennen verschiedene Wege, das zu tun. Sie beginnen in der Arbeit mit R gezielt Funktionen zur Abfrage von Attributen von Objekten zu nutzen. Sie verstehen, wie eine for-Schleife funktioniert und können einfache Varianten davon selbst erstellen. 5.2 Hinweise Indizieren &amp; Subsetting Wie können wir nun mittels Indizieren (Auswählen) und Subsetting (Aufteilen) auf einzelne Elemente eines Objektes zugreifen? Teilweise haben wir diesen Aspekte im Umgang mit R bereits kennengelernt  aber nur implizit! Jetzt geht es darum, dies explizit zu machen. Dabei können wir drei Varianten, mit denen wir auf Daten zugreifen: Variante: [] Die erste Variante wählt nach den Dimensionen von Objekte die Elemente aus: vektor1 &lt;- c(1,2,10,5,13,20) vektor2[4] datensatz &lt;- data.frame(vektor1, c(1,1,1,1,2,1)) datensatz datensatz[1,1] datensatz[6,1] datensatz[,0] Variante: $ Die zweite Variante funktioniert in einer Logik von benannten Unterelementen eines Objektes, wie wir dies etwa als Variablen bei Datensätzen kennen: datensatz$vektor1 Variante: which() Mittels der dritten Variante fragen wir in einem Objekt die Eigenschaften von dessen einzelnen Elementen ab. Dies entspricht einer Vorstellung auf das Zugreifen von Fällen. Und wir tun dies über logische Bedingungen: which(datensatz$vektor1==20) Sie sehen bereits im letzten Beispiel, dass die Varianten kombinert werden können. Damit können wir uns Beispielweise einen Teildatensatz erstellen: datensatz[which(datensatz$vektor1==20),] datensatz[datensatz$vektor1==20,] #als einfachere Schreibweise 5.3 Aufgaben WP05 Wenden Sie die Funktionen dim(), names(), str(), class() und typeof() auf Ihren Datensatz an (d.h. das data.frame-Objekt an, welches Sie im Rahmen des letzten Wochenplans erstellt haben). Was sagen Ihnen diese Funktionen jeweils? Welcher Output dieser Funktionen leuchtet Ihnen ein, welcher weniger? Ein erster Datensatz von Valentina Meyer: #data.frame-Objekt von letzter Woche: #Vorbereitung Age &lt;- c(runif(100, min = 13, max = 18)) Alter &lt;- as.character(Age) #Dataframe erstellen df_VM &lt;- data.frame(ID=c(1:100), Alter, Gewicht= c(rep(15,4), rep(16,4), rep(17,8),rep(18,12), rep(19,14), rep(20,16), rep(21,18), rep(22, 12), rep(23, 8), rep(24,4)), Sport= c(seq(10,50,2), seq(50,100,3), seq(100,150,4), seq(150,200,3), seq(200,240,2), rep(120,5), rep(60,6)), Internetnutzung= c(seq(10,50,4), seq(50,100,3), seq(100,150,2), seq(150,200,3), seq(200,240,4), rep(100,6), rep(180,8), rep(60,4))) Über die Funktion view() oder indem wir im Environment-Fenster auf das Objekt des Datensatzes klicken können wir das erstelte Objekt auch betrachten. Was sagen uns nun die einzelnen Funktionen der Aufgabenstellung aus? dim(df_VM) ## [1] 100 5 die Dimensionen Ihres Datensatzes: die Zeilen- und Spaltenzahl, was der Anzahl Fällen und Variablen im Datensatz entspricht. Auf diese Dimensionen greifen Sie dann auch mittels den [] zu. names(df_VM) ## [1] &quot;ID&quot; &quot;Alter&quot; &quot;Gewicht&quot; &quot;Sport&quot; ## [5] &quot;Internetnutzung&quot; die Namen Ihrer Vektoren aus, also die Variablennamen im Datensatz. Diese können wir dann auch verändern: names(df_VM)[4] &lt;- &quot;Sportminuten&quot; Hingegen werden die Namen der Reihen (die Fallnummern) nicht ausgegeben. Diese können wir auch ändern falls wir wollten über rownames(). Die Änderung sehen wir dann im Datenviewer. rownames(df_VM)[2] &lt;- &quot;Fall-Zwei&quot; str(df_VM) ## &#39;data.frame&#39;: 100 obs. of 5 variables: ## $ ID : int 1 2 3 4 5 6 7 8 9 10 ... ## $ Alter : chr &quot;14.3590072724037&quot; &quot;13.6548999028746&quot; &quot;16.4260366531089&quot; &quot;13.5695021688007&quot; ... ## $ Gewicht : num 15 15 15 15 16 16 16 16 17 17 ... ## $ Sportminuten : num 10 12 14 16 18 20 22 24 26 28 ... ## $ Internetnutzung: num 10 14 18 22 26 30 34 38 42 46 ... die Struktur Ihres Datensatzes aus, das heisst die Unterobjekte bzw. Variablen im Datensatz, auf die Sie mittels dem Dollarzeichen zugreifen können. Diese Funktion bietet bieten eine schnelle Übersicht, etwa um Umkodierungen in einem Datensatz zu erkennen oder allgemein auszuweisen, was in einem Objekt enthalten ist. class(df_VM) ## [1] &quot;data.frame&quot; die Klasse Ihres Objektes, das heisst die von Ihnen zugewiesene Eigenschaft des Objektes. Für einen data.frame-Objekt bedeutet dies folgendes (aus der Hileseite der Funktion): A data frame is a structure in R that holds data and is similar to the datasets found in standard statistical packages (for example, SAS, SPSS, and Stata). The columns are variables and the rows are observations. You can have variables of different types (for example, numeric, character) in the same data frame. Data frames are the main structures youll use to store datasets. typeof(df_VM) ## [1] &quot;list&quot; und der Typ Ihres Objektes, das heisst die R-interne Art und Weise, die Daten Ihres Objektes abzuspeichern (hier zeigt sich also die Datenart, und nicht die Objektart/-typ). Listen ist die Speicherweise für verschiedenen Datenformen und kann verschiedene Objekte (bzw. verschiedene Klassen von Objekten) enthalten. Eine weitere nützliche Funktion (für data.frame Objekte und andere) ist summary(): summary(df_VM) ## ID Alter Gewicht Sportminuten ## Min. : 1.00 Length:100 Min. :15.00 Min. : 10.00 ## 1st Qu.: 25.75 Class :character 1st Qu.:18.00 1st Qu.: 59.75 ## Median : 50.50 Mode :character Median :20.00 Median :120.00 ## Mean : 50.50 Mean :19.84 Mean :120.38 ## 3rd Qu.: 75.25 3rd Qu.:21.00 3rd Qu.:186.75 ## Max. :100.00 Max. :24.00 Max. :240.00 ## Internetnutzung ## Min. : 10.00 ## 1st Qu.: 79.25 ## Median :123.00 ## Mean :124.96 ## 3rd Qu.:180.00 ## Max. :240.00 Die Funktion gibt uns für jede Variable eine Übersicht. Sie berechnet bei den metrischen Variablen die Mittelwerte sowie Streuungsmasse und bei Character-Daten die Klasse. Weiter würden auch noch NAs angezeigt, falls diese vorhanden wären. Die Funktion, so könnten wir zusammenfassen, berechnet also immer wieder dieselben Dinge für alle Variablen eines Datensatzes. Eine solche Repetition und Kontrolle von Befehlsabläufen können wir auch selber herstellen über sogenannte Schleifen. Grundsätzlich sind diese for-Schleifen so aufgebaut: for (Variation in Sequenz){ Funktion; } Im ersten Teil (vor den {}-Klammern) befindet sich der sogenannte Funktionskopf, in dem formale Elemente definiert sind, also mit welcher Variation einer Sequenz etwas ablaufen soll. Anschliessend folgt in den {}-Klammern der Funktionsrumpf, der bestimmt, was genau in der Schleife passieren soll. Hier ein einfaches Beispiel: for (i in 1:2) { print(i) } oder bezogen auf einen Datensatz: for (i in 1:2) { print(df_VM[,i]) } #bzw. for (i in 1:2) { print(df_VM[i,]) } Dies können wir auch erweitern, in dem wir nicht eine bestimmte Zahl definieren in den formalen Elementen, sondern die Anzahl Spalten oder Anzahl Zeilen abfragen  und uns so eigentlich den ganzen Datensatz ausgeben lassen: for (i in 1:ncol(df_VM)) { print(df_VM[,i]) } #bzw. for (i in 1:nrow(df_VM)) { print(df_VM[i,]) } Im folgenden Code-Chunk wird nun eine solche for-Schleife definiert, welche die vorherige Funktion für die metrischen Variable im Datensatz immitiert. Zuerst wird dies nur für mean() umgesetzt. Anschliessend werden noch min() und max() ergänzen im Funktionsrumpf. for (i in 3:ncol(df_VM)) { print(mean(df_VM[,i])) } summary(df_VM) #Ergänzung von min und max: for (i in 3:ncol(df_VM)) { print(names(df_VM[i])); print(c(&quot;Min&quot;, min(df_VM[,i]))); print(c(&quot;Mean&quot;, mean(df_VM[,i]))); print(c(&quot;Max&quot;, max(df_VM[,i]))); } #Verwendung von cat() anstelle von Print -- ein Vorschlag von Josias Bruderer for (i in 3:ncol(df_VM)) { cat(c(&quot;\\n&quot;, names(df_VM[i]))); cat(c(&quot; Min&quot;, min(df_VM[,i]))); cat(c(&quot; Mean&quot;, mean(df_VM[,i]))); cat(c(&quot; Max&quot;, max(df_VM[,i]))); }   2. Wählen Sie aus Ihrem Datensatz die Fälle 20 bis 30 sowie die zweite und dritte Variable aus (ohne einen Teildatensatz zu erstellen). Tun Sie dies auf zwei verschiedene Varianten! Variante 1 mittels [] und dem Zugreifen auf die Dimensionen des Datensatzes: df_VM[c(20:30), c(2,3)] #oder einfach: df_VM[20:30, 2:3] #oder df_VM[20:30, c(&quot;Alter&quot;, &quot;Gewicht&quot;)] Variante 2 mittels $ und dem Zugreifen auf die Dimensionen der Vektoren bzw. Variablen: cbind(df_VM$Alter[20:30], df_VM$Gewicht[20:30]) Wie könnten wir vorgehen, wenn wir genau diese Fälle und Variablen nicht wollten? Wir nutzen eine negative c()-Funktion: df_VM[-c(20:30), -c(2,3)]   3. Suchen Sie Fälle in Ihrem Datensatz unter der Verwendung von mindestens zwei Variablen. Diese Fälle sollen spezielle oder interessante Beispiele repräsentieren  begründen Sie Ihre Wahl! Falls Sie keine solche Fälle in Ihrem Datensatz finden können Sie vorhandene Ausprägungen auch anpassen. Ein Datensatz und Beispiel von Fabio Keller: gender &lt;- as.integer(runif(100, 0, 2)) income &lt;- rnorm(100, 10000, 4000) education &lt;- as.integer(rnorm(100, 2, 0.8)) age &lt;- runif(100, 18, 90) home &lt;- sample(c(rep(c(&quot;countryside&quot;, &quot;suburb&quot;, &quot;city&quot;), each = 33), &quot;city&quot;)) df_FK &lt;- data.frame(gender, income, education, age, home) #Ein spezieller Fall? which(df_FK$income &gt; 15000 &amp; df_FK$education == 0) ## [1] 7 #oder einfach: df_FK[df_FK$income &gt; 15000 &amp; df_FK$education == 0,] ## gender income education age home ## 7 0 20738.86 0 89.1498 suburb Ein Datensatz und Beispiel von Vanessa Leutener: Einkommen &lt;- c(seq(from = 2000, to = 12000, length.out = 100)) Krankheitstage &lt;- round(rnorm(runif(100, min = 3, max = 40), mean = 6.5, sd =3)) Ausbildungsjahre &lt;- (rep(c(3, 6, 9, 3, 13, 4, 6, 7, 10, 5), each = 10)) Anzahl_Kinder &lt;- rep(c(1, 0, 1, 2, 3, 2, 2, 0, 0 , 1, 7, 2, 3, 1, 0, 0, 2, 2,5, 0), each = 5) Geschlecht &lt;- sample(rep(c(&quot;weiblich&quot;, &quot;männlich&quot;, &quot;divers&quot;, &quot;männlich&quot;, &quot;weiblich&quot;), each = 20)) df_VL &lt;- data.frame(Einkommen, Krankheitstage, Ausbildungsjahre, Anzahl_Kinder, Geschlecht) # Eine erste Feststellung: plot(df_VL$Ausbildungsjahre, df_VL$Anzahl_Kinder) # Wie könnten wir jetzt spezielle Fälle finden? which(df_VL$Ausbildungsjahre&gt;mean(df_VL$Ausbildungsjahre) &amp; df_VL$Anzahl_Kinder&gt;mean(df_VL$Anzahl_Kinder)) ## [1] 21 22 23 24 25 26 27 28 29 30 81 82 83 84 85 86 87 88 89 90 # Hier weisen wir einfach noch dem 50 Fall (mit hoher Bildung) eine hohe Anzahl Kinder zu: df_VL$Anzahl_Kinder[50] &lt;- 8   4. Definieren Sie eine logische Bedingung die Ihnen erlaubt, Ihr Dataframe anhand der Character-Variable in zwei Gruppen zu teilen. Dies ist eine erste Variante um Teildatensätz zu erstellen (TD1 &amp; TD2). Teilen Sie dann als zweite Variante Ihren Datensatz anhand einer anderen logischen Bedingung in zwei andere Gruppen (TD3 &amp; TD4). Nutzen Sie hierfür wenn möglich zwei numerische Variablen. Ein Datensatz und Beispiel von Katrin Oesch: Wohnort &lt;-c(rep(1:6, times=5),3,4,2,2,6,5,1, rep(1,times=10),5,3,4,1,1,3, rep(seq(from=1, to=6, by=2),times=7), rep(3, times=8), rep(seq(from=1, to=6, by=3),times=9)) Lebenszufriedenheit &lt;-c(rep(seq(from=1,to=10, by=2),times=9), rep(4:6, times=12),10,9,4,4,8,7, rep(5,times=7),8,3,4, seq(from=2, to=9, by=3)) Alter_KO &lt;-c(rep(33:44),seq(from=28,to=66,by=4), 54,62,42,85,66,41,91,23, rep(43,times=7),87,44,43,65,31, rep(31:56),43,23,31,65,43,25,26,36, rep(seq(from=54,to=71,by=5),times=2), rep(28:35, each=2)) Arbeitsstatus &lt;-c(rep(1:8, times=5),4,2,2,6,5,7, rep(1,times=10),rep(2,times=4),5,3,4,1,3, rep(seq(from=1, to=8, by=2),times=), rep(3, times=8),5,6, rep(seq(from=1, to=8, by=3),times=7)) Abstimmungsberechtigung &lt;- c(rep(&quot;Ja&quot;,times=25), &quot;Nein&quot;,&quot;Nein&quot;,&quot;Ja&quot;,&quot;Nein&quot;,&quot;Nein&quot;,&quot;Ja&quot;,&quot;Nein&quot;, &quot;Nein&quot;,&quot;Ja&quot;,&quot;Nein&quot;,&quot;Nein&quot;, &quot;Ja&quot;, rep(&quot;Nein&quot;,times=15), &quot;Ja&quot;,&quot;Nein&quot;,&quot;Nein&quot;,&quot;Ja&quot;,&quot;Nein&quot;,&quot;Nein&quot;, &quot;Ja&quot;,&quot;Nein&quot;,&quot;Nein&quot;,&quot;Ja&quot;,&quot;Ja&quot;, rep(&quot;Ja&quot;,times=33),&quot;Nein&quot;,&quot;Ja&quot;,&quot;Nein&quot;,&quot;Ja&quot;) df_KO &lt;-data.frame(Wohnort, Lebenszufriedenheit, Alter_KO, Arbeitsstatus, Abstimmungsberechtigung) #und die vier Teildatensätz: berechtigt &lt;- which(df_KO$Abstimmungsberechtigung==&quot;Ja&quot;) TD1_KO &lt;- df_KO[berechtigt,] nichtberechtigt &lt;- which(df_KO$Abstimmungsberechtigung==&quot;Nein&quot;) TD2_KO &lt;- df_KO[nichtberechtigt,] staedtisch_alt &lt;- which(df_KO$Wohnort&lt;=3 &amp; df_KO$Alter&gt;=40) TD3_KO &lt;- df_KO[staedtisch_alt,] #Wie könnten wir den Vektor &#39;staedtisch_alt&#39; nutzen? TD4_KO &lt;- df_KO[-c(staedtisch_alt),] #Wir nehmen also einfach &quot;alle anderen&quot;.   5. Berechnen Sie jeweils die Standardabweichungen einer Variable bei den vier Teildatensätze, die Sie in Aufgabe 3 erstellt haben. Was wären (kurze) sozialwissenschaftliche Interpretationen Ihrer Ergebnisse? Ein Datensatz und Beispiel von Josias Bruderer: #Datensatz df_JB &lt;- data.frame(alter = round(runif(100, 18, 64)), geschlecht = sample(rep(c(seq(1,3),2,3), 20)), tvdauer = sample(round(rnorm(100, 3, 1), 0)), happiness = sample(c(sample(1:10, 80, replace = T), rep(NA, 20))), prog = sample(rep(c(&quot;Sandmännchen&quot;,&quot;Tagesschau&quot;, &quot;Akte-X&quot;,&quot;Simpsons&quot;,NA), 20))) TD1_JB &lt;- df_JB[which(!is.na(df_JB$prog)),] # Angabe zu Programm vorhanden TD2_JB &lt;- df_JB[which(is.na(df_JB$prog)),] # Angabe zu Programm nicht vorhanden #Berechnung der Standardabweichung für zwei Teildatensätze: for(t in list(TD1_JB, TD2_JB)){ print(mean(t$tvdauer, na.rm = T)); print(sd(t$tvdauer, na.rm = T)) } ## [1] 2.95 ## [1] 1.112643 ## [1] 3 ## [1] 1.169795 Hier sehen wir ein komplexeres Beispiel einer Schleife, dass die Möglichkeit von Listen nutzt. Schleifen können daher beliebig komplex werden  und sie sind ein erster Schritt hin zum Schreiben von eigenen Funktionen. Wir nehmen nochmals die Schleife von oben und wenden diese Auf den Datensatz von Katrin Oesch an: for (i in 1:(ncol(df_KO)-1)) { print(mean(df_KO[,i])) } Was wäre wenn wir diesen Prozess nun auf beliebige Datensätze anwenden möchten, wie etwa die vier Teildatensätz aus Aufgabe 4? Hierfür können wir eine eigene Funktion programmieren: NameEigeneFunktion &lt;- function(argumente){ anweisungen(mit argumenten); weitere anweisung(mit argumenten) } Im sogenannten Funktionskopf innerhalb der runden Klammern auf function folgend werden die formalen Argumente benannt und durch Kommas voneinander getrennt. Damit wird festgelegt, welche Eingabeinformationen die Funktion benötigt. [] Alle im Funktionskopf enthaltenen Argumente müssen im Funktionsrumpf, der in geschweiften Klammern {} folgt, als Objekte definiert werden. Dabei können Argumente selbst andernorts definierte Funktionen sein []. Der Funktionsrumpf besteht also aus einer Reihe von Befehlen sowie gegebenenfalls durch # gekennzeichnete Kommentare (Manderscheid 2017, 240f). Hier nun als ausformulierte Funktion: mean_df &lt;- function(df, nr) { df &lt;- df[,-c(nr)]; for (i in 1:ncol(df)) { print(mean(df[,i])) } } #für Teildatensatz 1 mean_df(TD1_KO, 5) #für Teildatensatz 4 mean_df(TD4_KO, 5)   6. Berechnen Sie für eine Variable und bei einer der vier Möglichkeiten aus Aufgabe 3 den Mittelwert, nun allerdings in einem Schritt (das heisst ohne zuerst einen Teildatensatz zu erstellen). Ein Datensatz und Beispiel von Julien Lattmann: #Datensatz Alter_LJ &lt;- round(runif(100, 18, 30), 0) Dauer_Ausbildung &lt;- round(runif(100, 9, 20), 0) Einstiegsgehalt &lt;- round(rnorm(100, 6000, 1800), 0) Zufriedenheit &lt;- round(runif(100, 1, 10), 0) Branche &lt;- rep(c(&quot;Finanz-/Versicherungswesen&quot;, &quot;Information/Kommunikation&quot;, &quot;Gesundheits-/Sozialwesen&quot;, &quot;Erziehung/Unterricht&quot;, &quot;Dienstleistungsbereich&quot;), 20) df_LJ &lt;- data.frame(Alter_LJ, Dauer_Ausbildung, Einstiegsgehalt, Zufriedenheit, Branche) #Variante 1 mean(df_LJ[df_LJ$Einstiegsgehalt &gt;= 6000 &amp; df_LJ$Dauer_Ausbildung &gt;= 14,&quot;Einstiegsgehalt&quot;]) ## [1] 7640.615 #Variante 2 mean(df_LJ$Einstiegsgehalt[df_LJ$Einstiegsgehalt &gt;= 6000 &amp; df_LJ$Dauer_Ausbildung &gt;= 14]) ## [1] 7640.615 References "],["wochenplan-06.html", "6 Wochenplan 06 6.1 Lernziele 6.2 Besprechung Skript standardfehler.R 6.3 ggplot2 Darstellung", " 6 Wochenplan 06  zur Einheit der 06. und 07.Woche. 6.1 Lernziele Über die vergangenen Wochen haben wir zahlreiche grundlegende Aspekte der Arbeit mit R kennengelernt. Als Vorbereitung im Rahmen des Wochenplans 06 sollen Sie diese Inhalte noch einmal Revue passieren lassen. Im Rahmen dieser Repetition werden weiter zwei neue Dinge vermittelt werden: Auf der einen Seite erfolgt die Schulung aktiver Rezeptionsfertigkeiten (sprich: R-Code lesen und verstehen lernen). Auch dies ist eine wichtige Arbeitstechnik in R. Auf der anderen Seite sollen Sie mit dem rezipierten Code das Prinzip der Inferenzstatistik (und der Stichprobenverteilung sowie des Standardfehlers) in R veranschaulicht bekommen. Konkret lassen sich folgende Seminarziele festhalten: Sie können von einer anderen Person geschriebenen R-Code entziffern und mit Kommentaren versehen. Sie entwickeln dabei ein Gefühl für unterschiedliche Arten, Code lesbar zu gestalten. Sie wissen, wie selbstgeschriebene Funktionen in R aussehen und können diese Schritt für Schritt interpretieren. Sie entwickeln Ihr Verständnis davon weiter, wie in R Grafiken genutzt und angepasst werden, um statistische Inhalte zu visualisieren. Sie nutzen R um über Konzepte der Inferenzstatistik (wie den Standardfehler) nachzudenken. 6.2 Besprechung Skript standardfehler.R Der folgende Code ist ein Skript zur Simulation des Standardfehlers und soll helfen, das Prinzip der Inferenzstatistik, den Standardfehler sowie die damit zusammenhängenden Ebenen von Grundgesamtheit, einzelner Stichprobe und Stichprobenverteilung zu verstehen und zu veranschaulichen (vgl. Diaz-Bone 2019, 145f). Ebene der Grundgesamtheit: Unsere Grundgesamtheit bilden 1000 Ausprägungen einer beliebigen Variable, die hier nun gleichverteilt ist und sich zwischen 0 und 20 bewegt. Die Verteilung dieser Variable können wir dann in einem Histogramm darstellen. variable_population &lt;- runif(10000, min = 0, max = 20) mean(variable_population) ## [1] 10.04105 sd(variable_population) ## [1] 5.744609 hist(variable_population, breaks = 30, col=&quot;orange&quot;, main=&quot;Unsere Grundgesamtheit&quot;, xlim = c(0,20) ) Ebene der einzelnen Stichprobe: Als nächster Schritt wird nun eine Stichprobe gezogen mit der Funktion sample(), in der zufällig 100 Fälle der Grundgesamtheit landen. Von dieser Stichprobe können wir dann den Mittelwert berechnen und diesen Wert in das Histogramm der Grundgesamtheit einfügen. Der Mittelwert der Stichprobe (schwarz) unterscheidet sich natürlich minimal vom Mittelwert der Grundgesamt (rot).5 Letzterer wird dann über die Funktion abline() eingezeichnet.6 sample1 &lt;- sample(variable_population, 100) mean(variable_population) ## [1] 10.04105 mean(sample1) ## [1] 11.17416 hist(variable_population, breaks = 30, col=&quot;orange&quot;, main=&quot;Unsere Grundgesamtheit&quot;, xlim = c(0,20) ) points(rep(mean(sample1), 2), c(0, 100), type = &quot;l&quot;, col = &quot;black&quot;, lwd = 8) #Hier wird nun auch noch der Mittelwert der Grundgesamtheit eingefügt #...aber mittels der Funktion &#39;abline()&#39; abline(v=mean(variable_population), col = &quot;red&quot;, lwd = 3) Da wir nun mit einer fiktiven Grundgesamtheit und in einem Modell arbeiten können wir immer wieder neue Stichproben ziehen. Dies könnten wir realisieren, indem wir den Codechunk von oben immer wieder repetieren. Oder als elegantere Variante: Wir schreiben uns den Code für eine for-Schleife, die uns eine beliebige Anzahl Stichproben zieht (i), den Mittelwert der Stichprobe berechnet und diesen Mittelwert in das Histogramm der Grundgesamtheit einzeichnet. Ebene der einzelnen Stichprobe als Loop: Der folgende Code ruft nun nochmals das Histogramm der Grundgesamtheit auf. Anschliessend zieht die Schleife 100 Stichproben, berechnet deren Mittelwerte und fügt diese als Linie mit verschiedenen Farben in die Grafik ein. hist(variable_population, breaks = 30, col = &quot;orange&quot;, main=&quot;Unsere Grundgesamtheit&quot;, xlim = c(0,20) ) for (i in 1:100) { sample1 &lt;- sample(variable_population, 100) points(rep(mean(sample1), 2), c(0, 200), type = &quot;l&quot;, col = i, lwd = 1) } Wir sehen, dass mit jeder gezogenen Stichprobe der Mittelwert etwas abweicht von demjenigen Wert in der Grundgesamtheit. Die Abweichung ist nun auch deutlicher zu sehen als vorher. Stichprobenziehung als Funktion: Als nächster Schritt wird nun eine Funktion geschrieben, mit der wir eine beliebige Anzahl Stichproben mit einer beliebigen Grösse ziehen können. Die Grundstruktur von Funktionen entspricht folgender Form (Manderscheid 2017, 240f): eigene.funktion &lt;- function(argumente) { anweisung } Im Funktionskopf innerhalb der runden Klammern, die auf function() folgen, werden die formalen Argumente benannt und durch Kommas voneinander getrennt. Damit wird festgelegt, welche Eingabeinformationen die Funktion benötigt. In unserem Beispiel sind dies die selbstgewählten Begriffe x (Objekt, von dem die Stichprobe gezogen werden soll), n (Grösse der Stichprobe) und trials(Anzahl der zu ziehenden Stichproben). Alle im Funktionskopf enthaltenen Argumente müssen im Funktionsrumpf, der in geschweiften Klammern {} folgt, als Objekte definiert werden. Der Funktionsrumpf besteht aus einer Reihe von Befehlen. Die in unserem Code definierte Funktion definiert zuerst ein Sample von der Grösse n aus dem Objekt x. Anschliessend hängt eine Schleife die Anzahl trials-1 weitere Sample an das bereits gezogene Sample dran. Die Spezifikation von -1erfolgt da ja bereits eine erste Stichprobe gezogen wird, bevor dann die weiteren darangehängt werden (wenn also 200 Stichproben gemacht werden sollen, dann wird zuerst 1 gezogen und dann 199 angehängt). Die einzelnen Schritte, die im Funktionsrumpf festgelegt werden, erscheinen nicht in der Konsole. Nur das Ergebnis der letzten Funktion im Rumpf erscheint abschliessend als Rückgabewert. Da in unserem Beispiel kein Ergebnis erzeugt wurde stellt die Funktion return() sicher, dass das Resultat der Funktion (das Objekt variable sample) ausgegeben wird. meine_samples &lt;- function(x, n, trials) { variable_sample &lt;- sample(x, n) for (i in 1:(trials - 1)){ variable_sample &lt;- cbind(variable_sample, sample(x, n)) } return(variable_sample) } Im letzten Codestück wird dann vom unsere Grundgesamtheit (x) eine 30er Stichprobe gezogen (n), und zwar 200mal (trials). Das Objekt stichproben_200 ist also eine Matrix mit 30 Zeilen und 200 Spalten.7 stichproben_200 &lt;- meine_samples(variable_population, 30, 200) Ebene der Stichprobenverteilung: Bereits in den vorhergehenden beiden Schritten (Stichprobe als Schleife und Stichprobe als Funktion) ging es nicht mehr nur um eine einzelne Stichprobe, sondern um verschiedene Stichproben und deren jeweilige Mittelwerte. Wir gehen also über zu einer Stichprobenverteilung. Im folgenden Code werden zuerst die Stichprobenkennwerte berechnet (d.h. die Mittelwerte der 200 gezogenen Stichproben) und dann als Histogramm dargestellt: Das Histogramm entspricht der Stichprobenverteilung der Stichprobenkennwerte.8 mittelwerte &lt;- apply(stichproben_200, 2, mean) hist(mittelwerte, breaks = 10, col = &quot;blue&quot;, main = &quot;Unsere Stichprobenmittelwerte&quot;, xlim=c(3,17) ) Hierbei wird nun ersichtlich, dass diese Stichprobenverteilung der Normalverteilung folgt (ab einem Stichprobenumfang von 30). Das heisst eben auch, dass je stärker ein Stichprobenmittelwert vom Mittelwert der Grundgesamtheit abweicht, desto unwahrscheinlicher ist dieser Wert. Oder umgekehrt: Tritt ein sehr stark abweichender Wert auf ist dies mit hoher Wahrscheinlichkeit nicht zufällig. Gemeinsame Darstellung der Stichprobenverteilung in der Grundgesamtheit anhand der Histogramme: In den folgenden Codezeilen wird nun die Stichprobenverteilung in das Histogramm der Grundgesamtheit eingefügt. Die Idee hierbei ist, dass die Stichprobengrösse dank der eigenen Funktion variiert werden kann. Daraus wird ersichtlich, dass je grösser der Stichprobenumfang ist, desto schmaler wird die Stichprobenverteilung (das blaue Histogramm), das heisst: desto kleiner wird der Standardfehler. hist(variable_population, breaks = 30, col = &quot;orange&quot;, main = &quot;Grundgesamtheit mit Stichprobenverteilung&quot;, xlim = c(0, 20), ) # Hiweise hier stimmt der Titel zuerst natürlich noch nicht. # Erst wenn das zweite Histogram darüber gelegt wird stimmt auch der Titel. mittelwerte_samples &lt;- apply(meine_samples(variable_population, 30, 1000), 2, mean) hist(mittelwerte_samples, breaks = 10, col = &quot;blue&quot;, add = T) Mit der Darstellung können wir uns nochmals die expliziten Parameter der Stichprobenverteilung vergegenwärtigen. Auf der einen Seite haben wir das arithmetische Mittel der Stichprobenmittelwerte, das heisst: $ $ Dieser Wert entspricht (annährend) dem arithmetischen Mittelwert des metrischen Merkmals in der Grundgesamtheit: $ = $ mean(mittelwerte_samples) ## [1] 10.00486 mean(variable_population) ## [1] 10.04105 Auf der anderen Seite finden wir die Standardabweichung der Stichprobenverteilung, der Standardfehler (oder Stichprobenfehler): $ = $ Dieser Wert gibt das Ausmass der Streuung der einzelnen Stichprobenmittelwerte um den Mittelwert der Stichprobenverteilung beziehungweise der Grundgesamtheit an. sd(variable_population)/sqrt(30) und entspricht ungefähr der Standardabweichung unserer Stichprobenverteilung: sd(mittelwerte_samples) Die Stichprobenmittelwerte streuen umso geringer um den Mittelwert der Stichprobenverteilung (das heisst um den Wert in der Grundgesamtheit), je grösser der Umfang der Stichprobe ist. Das heisst die Stichprobenwerte werden immer genauer, da deren zufällige Abweichung verkleinert wird. hist(variable_population, breaks = 30, col = &quot;orange&quot;, main = &quot;Grundgesamtheit mit Stichprobenverteilung&quot;, xlim = c(0, 20), ) mittelwerte_samples &lt;- apply(meine_samples(variable_population, 120, 1000), 2, mean) hist(mittelwerte_samples, breaks = 10, col = &quot;blue&quot;, add = T) sd(mittelwerte_samples) ## [1] 0.5253198 #Der Standardfehler wurde halbiert, da die Stichprobengrösse vervierfacht wurde. Was hat uns nun dieses Skript und insbesondere die letzten Darstellungen gebracht (Diaz-Bone 2019, 240f)? Das blaue Histogram visualisiert das, was wir jeweils bei der Intervallschätzung sowie beim Testen in der Inferenzstatistik konstruieren, nämlich eine Stichprobenverteilung. Bei der Intervallschätzung konstruieren wir ein Konfidenzintervall mittels des Stichprobenkennwerts, also eine Stichprobenverteilung anhand des Wertes beziehungsweis um den Wert aus der Stichprobe. Beim Testen von Hypothesen konstruieren wir die Stichprobenverteilungen unter Annahme der 0-Hypothese und prüfen dann, ob unser Stichprobenkennwert in den Annahme- oder in den Rückweisungsbereich fällt. Weiter zeigt uns die Darstellung den Stichprobenfehler/Standardfehler. Diese Kennzahl wird uns jeweils bei inferenzstatistischen Berechnungen ebenfalls ausgegeben. Sie hilft uns gemeinsam mit dem Stichprobenkennwert und der Stichprobengrösse ein jeweiliges Ergebnis zu beurteilen und ergänzt so die Aussagen, dass ein Ergebnis einfach nur signifikant ist oder nicht. 6.3 ggplot2 Darstellung Wir könnten natürlich die letzte Darstellung, in der gemeinsam die Stichprobenverteilung in der Grundgesamtheit anhand der Histogramme abgebildet wird, ebenfalls mittels des Paketes ggplot2 darstellen. Dieses umfasst komplexere Möglichkeiten, Grafiken mittels R zu kreieren und ist eigentlich der Standard für Grafiken. Als erstes müssen wir hierfür einen Datensatz generieren, in dem beide Variablen enthalten sind (also die Verteilungen der Grundgesamtheit und die Stichprobenverteilung). Diese Daten sind allerdings in einer speziellen Form abgespeichert, nämlich in einem sogenannten long-Format (Healy 2019, 56): Social scientists will likely be familiar with the distinction between wide-format and longjormat data. In a long-format table, every variable is a column, and every observation is a row. In a wide-format table, some variables are spread out across columns. #Zur Veranschaulichung werden nochmals die beiden Variablen generiert... variable_population &lt;- runif(10000, min = 0, max = 20) mittelwerte_samples &lt;- apply(meine_samples(variable_population, 30, 1000), 2, mean) #...und dann in einem dataframe Objekt zusammengefasst. # Diese Daten sind nun in einem &#39;long&#39;-Format. daten_spv &lt;- data.frame(Verteilung = c(rep(&quot;Grundgesamtheit&quot;, 10000), rep(&quot;Stichprobenverteilung&quot;, 1000)), Werte = c(variable_population, mittelwerte_samples)) Anschliessend können wir die Grafik mittels der Funktion ggplot() erstellen. Zuerst wird das leere Grafikobjekt generiert. Dieses greift auf die Werte zu (x) und unterscheidet diese über die verschiedenen Verteilungen (fill). Erst anschliessend werden über die + die weiteren Aspekte hinzugefügt als Schichten oder Layer der Grafik (vgl. Healy 2019, 59f). library(ggplot2) p &lt;- ggplot(daten_spv, aes(x=Werte, fill=Verteilung)) p + geom_histogram(col = &quot;black&quot;, position = &#39;identity&#39;, bins = 40) + theme_minimal() + ylab(&quot;Anzahl&quot;) + scale_fill_manual(values=c(&quot;orange&quot;, &quot;blue&quot;)) References "],["wochenplan-07.html", "7 Wochenplan 07 7.1 Lernziele 7.2 Aufgaben", " 7 Wochenplan 07 zur Einheit der 07. und 08. Woche. 7.1 Lernziele In der zweiten Hälfte des Semester möchten wir das statistische Arbeiten mit R anhand des Umgangs mit echten Daten kennenlernen. Dazu nutzen wir Daten aus dem European Social Survey (ESS). Am Beginn der Arbeit mit einem Datensatz steht das Kennenlernen der Daten: Es gilt die Variablen und ihre Ausprägungen zu verstehen, erste Plausibilitätsprüfungen durchzuführen und Daten für die spätere Analyse aufzubereiten. Diese Techniken sollen Sie im Rahmen des siebten Wochenplans kennenlernen und so die Grundlage schaffen, in den folgenden Einheiten statistische Analysen mit dem Datensatz vorzunehmen. Konkret lassen sich folgende Lernziele festhalten: Sie können einen CSV-Datensatz in R laden und kennen die Vor- und Nachteile dieses Dateiformats. Sie können einen geladen Datensatz anhand verschiedener Funktionen und in unterschiedlichem Detailliertheitsgrad beschreiben. Sie kennen erste Techniken zur Plausibilitätsprüfung von Datensätzen. Sie kennen erste Techniken zur Datenaufbereitung. Sie verstehen, wie und wozu man zufällige Teildatensätze erstellt. 7.2 Aufgaben Laden Sie sich den Datensatz ESS1-8e01.csv und ESS-Fragebogen (das Codebook) herunter (via OLAT) und speichern Sie die Dateien in Ihrem Arbeitsverzeichnis. Laden Sie anschliessend den Datensatz in RStudio über die Funktion read.csv(). Betrachten Sie den Datensatz über die Funktion View() oder indem Sie auf Ihr erstelltes Objekt im Fenster Environment klicken. Vergleichen Sie für einzelne Variablen die Informationen im Datensatz mit den entsprechenden Fragebogenfragen! setwd(&quot;C:/Users/SchweglG/R_Daten/06_HS21/R_Seminar-HS21&quot;) Als erster Schritt gilt es Ihr Arbeitsverzeichnis zu definieren, das heisst der Ort, an dem auch Ihre Daten abgelegt sind (ggfs. sind diese noch in einem weiteren Unterordner). In diesem Arbeitsverzeichnis würden auch allfällige weitere Daten liegen oder später automatisch von R abgespeichert (vgl. Aufgabe 5). Sobald Sie die Kodezeilen bis daten_ess &lt;- read.csv(file =\"\" eingegeben haben sollten Sie auf die Tabulatortaste klicken können und so Ihr Datenfile auswählen (und eben auch auf weitere Unterordner Ihres Arbeitsverzeichnis zurückgreifen). Hier gilt es allerdings folgendes zu beachten (vgl. auch diese Erläuterungen): Standardmässig ist das Arbeitsverzeichnis für R-Codechunks das Verzeichnis, in welchem Ihr RMarkdown-Dokument abgelegt ist. In RStudio können Sie diesen Standard auch über das Menü Tools -&gt; Global Options -&gt; R Markdown ändern. Hier gibt es nun zwei weitere Möglichkeiten: Sie können das aktuelle Arbeitsverzeichnis Ihrer R-Konsole (die Option Current) oder das Stammverzeichnis eines Projektes als Arbeitsverzeichnis verwenden (die Option Project).9 Mit Current sollte dann dasjene Verzeichnis ausgewählt werden, das wir auch über die Menüsteuerung zur Definition des Arbeitsverzeichnis auswählen. Jedoch wird damit noch nicht behoben, dass mit der Tabulatortaste weiter auf denjenigen Ordner zugegriffen wird, in dem das jeweils aktive RMardown-Dokument abgelegt ist. Deswegen ist es angebracht, vor dem Laden der Dateien auch nochmals das Arbeitsverzeichnis einzufügen: setwd(&quot;C:/Users/SchweglG/R_Daten/06_HS21/R_Seminar-HS21&quot;) daten_ess &lt;- read.csv(file = &quot;Daten/ESS1-8e01.csv&quot;) Eine weitere Alternative wäre das Arbeitsverzeichnis über einen Setup Kode anzupassen: &#39;&#39;&#39;{r, setup, include=FALSE} knitr::opts_knit$set(root.dir = &quot;C:/Users/SchweglG/R_Daten/06_HS21/R_Seminar-HS21&quot;) &#39;&#39;&#39; Und als Hinweis: Probieren Sie auch mal das Definieren und Laden des Datensatzes in einem RSkript (anstatt RMarkdown). Dort sollte dann auch die Tabulatortasten-Funktion möglich sein (unabhängig vom Speicherort des Skripts). Anschliessend können Sie den im Skript geschriebenen Kode in Ihr Markdown kopieren.   Öffnen Sie nun die Datei ESS1-8e01.csv in einem Tabellenkalkulations- oder einem Textbearbeitungsprogramm (z.B. Microsoft Excel, dem Windows-Editor oder TextEdit auf Mac). Vergleichen Sie die beiden Varianten (Ihr Dataframe in RStudio und die im Textbearbeitungsprogramm geöffnete Datei), um das Dateiformat CSV besser zu verstehen. Ersetzen Sie anschliessend im Tabellenkalkulations- oder Textbearbeitungsprogramm die Kommas in der ESS1-8e01.csv Datei mit Strichpunkten und speichern Sie diese neue Version ab. Gehen Sie zurück zu R Studio und versuchen Sie, die neue Version des Datensatzes zu laden. Welchen Paramter müssen Sie anpassen, damit das funktioniert? Was denken Sie könnten Vorteile des CSV-Formats sein? Welche möglichen Nachteile im Vergleich zu anderen Dateiformaten (Excel, SPSS ) sind denkbar? Das CSV-Dateiformat bietet eine sehr grundlegende Art und Weise um Daten oder eben Datensätze zu speichern. Indem die Kommas mit Strichpunkten ersetzt werden ändert sich das Zeichen, mit dem die Spalten bzw. Variablen getrennt werden. Die Fälle werden im Format jeweils über Zeilenumbrüche getrennt werden. ?read.csv Die read.csv-Funktion ermöglicht über das Argument sep = zu bestimmen, über welches Zeichen die Spalten eingelesen werden sollen. Wir definieren also über das Argument eine bestimmte Spezifizierung (es sind nicht die Standardwerte, die ,) mit dem einem bestimmten Paramater (es sind neu ;). Das CSV-Dateiformat hat vor allem zwei Vorteile: Auf der einen Seite  und wie in der Teilaufgabe oben deutlich wurde  können über beliebige Texteditoren direkt in diese Daten eingegriffen werden, da im Format lediglich Rohdaten in Textform abgespeichert sind. Bei anderen Dateiformaten für Datensätze wie etwa .sav (von der Statistiksoftware SPSS) ist dies nicht so einfach möglich. Damit ist das CSV-Format ein besonders transparenter Standard, mit dem fast alle Programme und Betriebssysteme umgehen können. Auf der anderen Seite ist das CSV-Dateiformat nicht nur transparent, sondern beansprucht auch nur sehr wenig Speicherplatz. Grosse Datensätze können so schnell von RStudio gelesen und bereitgestellt werden. Die grosse Datensätze werden dadurch nicht komplexer, sondern womöglich lediglich unübersichtlich. Dem gegenüber steht der Nachteil von CSV, dass das Dateiformat keine Möglichkeiten bietet, Informationen auf komplexere Art und Weise abzuspeichern (z.Bsp. Levels für kategoriale Variablenausprägungen oder das Speichern von mehr als zwei Dimensionen). CSV bleibt so ein Format für Rohdaten. Weiter besteht die Möglichkeit, dass Sonderzeichen in den Daten selbst zu Problemen führen (also etwa ein , als Teil einer Ausprägung, dass dann fälschlicherweise als Spaltenumbruch eingelesen würde).   Versuchen Sie mit Funktionen, die Sie bisher kennengelernt haben, den geladenen Datensatz kurz zu beschreiben. Wir haben uns hier ein Objekt der Klasse class(daten_ess) ## [1] &quot;data.frame&quot; geladen, dass Daten als typeof(daten_ess) ## [1] &quot;list&quot; abspeichert. Unser Datensatz umfasst.. dim(daten_ess)[1] ## [1] 1525 Fälle mit jeweils dim(daten_ess)[2] ## [1] 21 Variablen. Die zweite und die zweitletzte Variablen heisst etwa: names(daten_ess)[c(1,21)] ## [1] &quot;x&quot; &quot;hinctnta&quot; Die ersten fünf Variablen  alphabetisch geordnet  heissen: sort(names(daten_ess))[1:5] ## [1] &quot;agea&quot; &quot;chldhm&quot; &quot;cntry&quot; &quot;edctn&quot; &quot;edulvla&quot; Aktuell umfasst der Datensatz nur metrische Daten und eine eine Character-Variable als Konstante.10 Auch eine Variable wie gndr ist daher eine Zahl in dem Datensatz und wird aktuell von RStudio als ein numerischer Vektor behandelt. Die Funktion str() liefert hierfür eine Übersicht, in dem sie die interne Struktur eines R-Objekts aufzeigt. Für unseren Datensatz bedeutet dies: die Variablen (auf welche wir mit $ zugreifen können), deren Dateiformat sowie deren ersten paar Ausprägungen. str(daten_ess) ## &#39;data.frame&#39;: 1525 obs. of 21 variables: ## $ x : int 1 2 3 4 5 6 7 8 9 10 ... ## $ cntry : chr &quot;CH&quot; &quot;CH&quot; &quot;CH&quot; &quot;CH&quot; ... ## $ essround: int 8 8 8 8 8 8 8 8 8 8 ... ## $ idno : int 1 3 5 6 9 14 21 22 24 27 ... ## $ pspwght : num 0.9 1.1 1.02 1.09 1.03 ... ## $ pweight : num 0.465 0.465 0.465 0.465 0.465 ... ## $ polintr : int 2 2 2 3 2 2 2 3 2 1 ... ## $ happy : int 8 6 8 8 8 5 9 3 8 10 ... ## $ gndr : int 1 2 1 2 1 1 1 2 2 2 ... ## $ yrbrn : int 1960 1953 1987 1949 1963 1948 1961 1955 1980 1941 ... ## $ agea : int 56 63 29 67 53 68 55 61 36 75 ... ## $ chldhm : int 2 2 2 2 2 2 2 2 2 2 ... ## $ edulvla : int 3 3 5 2 3 3 3 2 5 4 ... ## $ eisced : int 3 3 7 2 3 3 3 2 6 5 ... ## $ eduyrs : int 9 12 18 9 10 9 9 8 13 16 ... ## $ pdwrk : int 1 1 1 0 0 0 1 0 1 0 ... ## $ edctn : int 0 0 0 0 0 0 0 0 0 0 ... ## $ wkhtot : int 60 20 50 37 44 46 45 44 24 55 ... ## $ isco08 : int 1200 5419 2210 5223 7112 4220 8300 8100 5223 1120 ... ## $ uemp3m : int 1 1 2 2 1 1 2 2 2 1 ... ## $ hinctnta: int 10 1 10 1 77 2 5 77 4 5 ... Über die summary()-Funktion wird auch nochmals deutlich, dass hier tatsächlich Rohdaten eingelesen wurden: Auch fehlende Werte gelten aktuell noch als gültige Zahlen (jemand wäre also im Jahr 7777 geboren). summary(daten_ess$yrbrn) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1922 1954 1968 1991 1984 7777 Weiter sehen wir, dass sogenannte Dummy-Variablen oft 1-2 anstatt 0-1 kodiert sind. summary(daten_ess$chldhm) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 1.000 2.000 1.656 2.000 2.000 oder wir sehen, dass Geschlecht ziemlich gleichmässig verteilt zu sein scheint: summary(daten_ess$gndr) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 1.000 1.000 1.483 2.000 2.000   In den Variablen zum Alter und den Bildungsabschlüssen der befragten Personen haben sich einige Fehler eingeschlichen. Was sind die Fehler? Und welche Fälle betrifft das (Fallnummer)? Wir können drei Stufen von Plausibilitätstests unterscheiden: Bei der ersten Stufe betrachten wir lediglich die Zahlen selber, wobei uns vor allem Häufigkeitsverteilungen und Extremwerten interessiert. Wir beginnen mit der Variable zum Alter: min(daten_ess$agea) ## [1] 14 max(daten_ess$agea) ## [1] 999 summary(daten_ess$agea) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 14.00 32.00 48.00 51.75 63.00 999.00 which(daten_ess$agea==999) ## [1] 121 196 320 717 791 800 which(daten_ess$agea&gt;120) ## [1] 121 132 196 320 717 791 800 Hier scheint nun ein erster Fehler sichtbar zu sein, nämlich beim Fall 132. Diese Person soll angeblich 320 Jahre alt sein: daten_ess$agea[132] ## [1] 320 Die Werte zu den Bildungsabschlüssen sind kategoriale Daten. Deshalb lohnt es sich hier mit Tabellen zu arbeiten, um Häufigkeitsverteilungen und Extremwerte zu betrachten: table(daten_ess$edulvla) ## ## 1 2 3 4 5 55 77 88 ## 51 265 665 45 491 3 1 4 table(daten_ess$eisced) ## ## 0 1 2 3 4 5 6 7 55 77 88 ## 1 50 265 543 122 226 114 197 3 1 3 Hier scheint noch nicht wirklich ein Fehler sichtbar zu sein. Auf der zweiten Stufe überprüfen wir nun, ob die Daten auch wirklich Sinn machen, das heisst im Rahmen der Codierungen gemäss Fragebogen. So wird schnell deutlich, dass wir natürlich fehlende Werte auf eine bestimmte Art und Weise in unserem Datensatz vorfinden. Beim Alter ist dies 999 (Not available), während es bei den Bildungsabschlüssen die Ausprägungen 55 (Other), 77 (Refusal), 88 (Dont know) und 99 (No answer) sind. Diese Ausprägungen können wir dann als fehlende Werte, sogenannte NAs definieren: daten_ess$agea[daten_ess$agea==999] &lt;- NA daten_ess$edulvla[daten_ess$edulvla&gt;5] &lt;- NA daten_ess$eisced[daten_ess$eisced&gt;7] &lt;- NA Ein Vergleich mit dem Codebuch zeigt dabei aber auch, dass erst Personen ab dem 15. Lebensjahr befragt wurden. Folgender Fall 352 macht also keinen Sinn: which(daten_ess$agea&lt;15) ## [1] 352 Hingegen ist die bei der Variable eisced auftauchende Ausprägung 0 ist durchaus zulässig und entspricht Not possible to harmonise into ES-ISCED. Auf der dritten Stufe vergleichen wir dann mittels Kreuztabellierungen und logischen Überlegungen jeweils (mindestens) zwei Variablen miteinander, um so nochmals Fehler zu entdecken. Dieses könnten wir mittels eines Plots machen, wie dies etwa Vanessa Leutner vorgeschlagen hat: plot(daten_ess$agea, daten_ess$yrbrn, xlim = c(10,100), ylim = c(1900,2017)) Dann können wir eine zusätzliche Variable berechnen, die als Fehlerindikator für die Angaben zum Alter dient. Diese zusätzliche Variable nutzen wir dann als logische Bedingung, um fehlerhafte Fälle zu entdecken: daten_ess$agea_l &lt;- daten_ess$agea + daten_ess$yrbrn daten_ess$agea_l[daten_ess$agea_l!=2016] ## [1] 2017 2017 2017 2017 2017 2017 2017 2017 NA 2017 2304 2017 NA 2017 2017 ## [16] 2017 NA 2017 2017 2017 1990 2017 2017 2017 2017 2017 2017 2017 2017 2017 ## [31] 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 NA ## [46] 2017 2017 2017 NA NA 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 ## [61] 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 ## [76] 2017 2017 2017 2017 1962 2017 2017 2017 2017 2017 2017 2017 2017 2017 which(daten_ess$agea_l!=2016 &amp; daten_ess$agea_l!=2017) ## [1] 132 352 1414 #Identifikation der Fälle daten_ess[daten_ess$agea_l!=2016 &amp; daten_ess$agea_l!=2017 &amp; !is.na(daten_ess$agea_l),c(&quot;agea&quot;, &quot;yrbrn&quot;)] ## agea yrbrn ## 132 320 1984 ## 352 14 1976 ## 1414 23 1939 Neben den bereits bekannten beiden Fällen (132 &amp; 352) wir so auch noch der dritte falsche Fehler im Datensatz bei der Altersvariable deutlich, nämlich Fall 1414. Bei den Angaben zum Bildungsniveau wieder können wir stärker Kreuztabellen nutzen, um zwei Fehler zu entdecken: table(daten_ess$edulvla, daten_ess$eisced) ## ## 0 1 2 3 4 5 6 7 ## 1 0 50 0 0 0 1 0 0 ## 2 0 0 265 0 0 0 0 0 ## 3 0 0 0 543 122 0 0 0 ## 4 0 0 0 0 0 45 0 0 ## 5 0 0 0 0 0 180 114 197 table(daten_ess$eduyrs, daten_ess$edulvla) ## ## 1 2 3 4 5 ## 0 2 1 0 0 0 ## 1 1 0 1 0 0 ## 2 1 2 0 0 0 ## 3 1 0 0 0 0 ## 4 4 1 0 0 0 ## 5 2 4 0 0 0 ## 6 12 5 3 0 1 ## 7 5 1 3 0 0 ## 8 5 44 52 2 9 ## 9 13 125 302 24 91 ## 10 1 34 71 6 20 ## 11 1 22 30 1 12 ## 12 3 19 82 1 62 ## 13 0 4 43 2 38 ## 14 0 0 39 2 31 ## 15 0 3 12 3 32 ## 16 0 0 13 3 37 ## 17 0 0 5 1 48 ## 18 0 0 3 0 52 ## 19 0 0 4 0 29 ## 20 0 0 0 0 13 ## 21 0 0 0 0 6 ## 22 0 0 0 0 5 ## 23 0 0 1 0 3 ## 25 0 0 0 0 1 ## 26 0 0 0 0 1 ## 77 0 0 0 0 0 ## 88 0 0 1 0 0 table(daten_ess$eduyrs, daten_ess$eisced) ## ## 0 1 2 3 4 5 6 7 ## 0 0 2 1 0 0 0 0 0 ## 1 0 1 0 1 0 0 0 0 ## 2 0 1 2 0 0 0 0 0 ## 3 0 1 0 0 0 0 0 0 ## 4 0 4 1 0 0 0 0 0 ## 5 0 2 4 0 0 0 0 0 ## 6 0 12 5 3 0 1 0 0 ## 7 0 5 1 3 0 0 0 0 ## 8 0 5 44 49 3 9 0 2 ## 9 0 13 125 287 15 90 12 13 ## 10 0 1 34 67 4 22 3 1 ## 11 0 1 22 20 10 10 2 1 ## 12 1 2 19 55 27 23 15 26 ## 13 0 0 4 23 20 21 10 9 ## 14 0 0 0 18 21 14 8 11 ## 15 0 0 3 5 7 11 15 9 ## 16 0 0 0 6 7 11 15 14 ## 17 0 0 0 1 4 6 15 28 ## 18 0 0 0 1 2 6 13 33 ## 19 0 0 0 2 2 1 4 24 ## 20 0 0 0 0 0 1 1 11 ## 21 0 0 0 0 0 0 0 6 ## 22 0 0 0 0 0 0 1 4 ## 23 0 0 0 1 0 0 0 3 ## 25 0 0 0 0 0 0 0 1 ## 26 0 0 0 0 0 0 0 1 ## 77 0 0 0 0 0 0 0 0 ## 88 0 0 0 1 0 0 0 0 #Identifikation der Fälle which(daten_ess$edulvla==1 &amp; daten_ess$eisced==5) ## [1] 443 which(daten_ess$edulvla==3 &amp; daten_ess$eisced==3 &amp; daten_ess$eduyrs&lt;6) ## [1] 1482 daten_ess[c(which(daten_ess$edulvla==1 &amp; daten_ess$eisced==5), which(daten_ess$edulvla==3 &amp; daten_ess$eisced==3 &amp; daten_ess$eduyrs&lt;6)), c(&quot;edulvla&quot;, &quot;eisced&quot;)] ## edulvla eisced ## 443 1 5 ## 1482 3 3 Wir finden so zwei Fälle entdeckt, bei denen Fehler in den Bildungsabschlüssen drin stecken, nämlich Fall 443 und Fall 1482. Josias Bruderer hat für ein solches Fehlersuchen mittels logischer Bedingungen auch die Alters- und Bildungsvariablen kombiniert: # Menschen die vor 3 Jahren in die Schule kamen errors_a &lt;- which(daten_ess$agea &lt; (daten_ess$eduyrs) + 3 &amp; daten_ess$agea &lt; 999 &amp; daten_ess$eduyrs &lt; 77) # Menschen mit Fehler im Bildungsabschluss gem. Definition ES-ISCED errors_b &lt;- c() errors_b &lt;- c(errors_b, which(daten_ess$eisced == 1 &amp; daten_ess$edulvla &gt; 1 &amp; daten_ess$edulvla &lt; 55)) errors_b &lt;- c(errors_b, which(daten_ess$eisced == 2 &amp; daten_ess$edulvla != 2 &amp; daten_ess$edulvla &lt; 55)) errors_b &lt;- c(errors_b, which(daten_ess$eisced == 3 &amp; daten_ess$edulvla != 3 &amp; daten_ess$edulvla &lt; 55)) errors_b &lt;- c(errors_b, which(daten_ess$eisced == 4 &amp; daten_ess$edulvla != 3 &amp; daten_ess$edulvla &lt; 55)) errors_b &lt;- c(errors_b, which(daten_ess$eisced == 5 &amp; daten_ess$edulvla != 4 &amp; daten_ess$edulvla != 5 &amp; daten_ess$edulvla &lt; 55)) errors_b &lt;- c(errors_b, which(daten_ess$eisced == 6 &amp; daten_ess$edulvla != 5 &amp; daten_ess$edulvla &lt; 55)) errors_b &lt;- c(errors_b, which(daten_ess$eisced == 7 &amp; daten_ess$edulvla != 5 &amp; daten_ess$edulvla &lt; 55)) # Menschen mit &quot;unlogischem&quot; Alter für Bildungsabschluss gem. ES-ISCED errors_c &lt;- c() errors_c &lt;- c(errors_c, which(daten_ess$eisced == 1 &amp; daten_ess$agea &lt; 11)) errors_c &lt;- c(errors_c, which(daten_ess$eisced == 2 &amp; daten_ess$agea &lt; 14)) errors_c &lt;- c(errors_c, which(daten_ess$eisced == 3 &amp; daten_ess$agea &lt; 16)) errors_c &lt;- c(errors_c, which(daten_ess$eisced == 4 &amp; daten_ess$agea &lt; 16)) errors_c &lt;- c(errors_c, which(daten_ess$eisced == 5 &amp; daten_ess$agea &lt; 18)) errors_c &lt;- c(errors_c, which(daten_ess$eisced == 6 &amp; daten_ess$agea &lt; 20)) errors_c &lt;- c(errors_c, which(daten_ess$eisced == 7 &amp; daten_ess$agea &lt; 20)) sort(unique(c(errors_a, errors_b, errors_c))) ## [1] 352 443 Im Umgang mit diesen fehlenden Werten haben wir drei Möglichkeiten: Wir können die benötigen Angaben von anderen Werten ableiten (1), zum Beispiel das Alter vom Geburtsjahr. which(daten_ess$agea&gt;120) ## [1] 132 daten_ess$agea[132] ## [1] 320 daten_ess$agea[132] &lt;- 2016 - daten_ess$yrbrn[132] daten_ess$agea[132] ## [1] 32 Weiter können wir uns einen Wert berechnen bzw. diesen schätzen (2): which(daten_ess$edulvla==3 &amp; daten_ess$eisced==3 &amp; daten_ess$eduyrs&lt;6) ## [1] 1482 daten_ess$eduyrs[1482] &lt;- round(mean(daten_ess$eduyrs[daten_ess$edulvla==3], na.rm = T)) daten_ess$eduyrs[1482] ## [1] 11 Die letzte Option ist dann noch die falschen Angaben als fehlende Werte zu definieren (3): daten_ess$edulvla[443] &lt;- NA   Bonusaufgabe: In den folgenden Wochen möchten wir nicht mit dem ganzen Datensatz arbeiten, sondern lediglich mit einer zufälligen Auswahl von 200 Fällen. Versuchen Sie, einen solchen Teildatensatz randomisiert zu erstellen und speichern Sie ihn mittels der Funktion write.csv() unter dem Namen TD_ESS1-8e01.csv ab. Um den zufälligen Datensatz mit 200 Fällen zu erstellen bieten sich mindestens zwei Möglichkeiten an: Die erste Möglichkeit nutzt die sample()-Funktion und wählt zufällig 200 Zahlen einer Variable aus, die von 1 bis 1525 durchläuft. Eine solche Variable ist x im Datensatz  oder auch einfach die Zeilenzahl. Die zufällig ausgewählten Zahlen dieser Variable können als Angabe für die Zeilen in die eckige Klammerfunktion eingesetzt werden. daten_ess_HS20 &lt;- daten_ess[sample(row.names(daten_ess),200),] #oder auch: daten_ess_HS20 &lt;- daten_ess[sample(daten_ess$x,200),] Die zweite Möglichkeit berechnet eine Zufallsvariable und sortiert den Datensatz dann nach der Grösse dieser Zufallsvariable. Anschliessend können dann die  jetzt völlig zufällig  ersten 200 Fälle ausgewählt werden. daten_ess$zv &lt;- rnorm(1525) daten_ess &lt;- daten_ess[order(daten_ess$zv),] daten_ess_HS20 &lt;- daten_ess[1:200,1:22] Neben diesen beiden Möglichkeiten haben weitere Seminarteilnehmer*innen auch Ideen geliefert. Josias Bruder und Fabio Keller haben die erste Möglichkeit etwas ergänzt: daten_ess_HS20 &lt;- daten_ess[sample(1:nrow(daten_ess),200),] Ebenso wie Felix Sigrist: daten_ess_HS20 &lt;- daten_ess[sample(nrow(daten_ess),200),] Delia Bazzigher und Julien Lattmann schlagen weiter die Funktion sample_n aus dem dpylr Paket vor (dieses ist Teil der Kernpakete des tidyverse): #installed.packages(&quot;dplyr&quot;) library(dplyr) daten_ess_HS20 &lt;- sample_n(daten_ess, 200) Der zufällige erstellte Datensatz kann anschliessend wiederum als CSV-Datei abgespeichert werden (die Datei wird dann im Arbeitsordner abgelegt): write.csv(daten_ess_HS20, file = &quot;Daten/ESS1-8e01_HS21.csv&quot;) Die RStudio-Projekten erlauben es Ihnen, Ihre Arbeit in mehrere Kontexte aufteilen, die jeweils über ein eigenes Arbeitsverzeichnis, einen eigenen Arbeitsbereich, einen eigenen Verlauf und eigene Quelldokumente verfügen. Sie sind eben nochmals abgeschlossene Projekte innerhalb von RStudio und können über die Menüsteuerung File -&gt; New Project gestartet werden. Wiederum spezifiziert bei numerische Daten dass int ganze Zahlen und num komplexe Zahlen sind (mit Kommastellen, negative Werte, usw.) "],["wochenplan-08.html", "8 Wochenplan 08 8.1 Lernziele 8.2 Aufgaben 8.3 Einführung Faktoren", " 8 Wochenplan 08 zur Einheit der 08. auf die 09.Woche. 8.1 Lernziele Nachdem wir uns bereits etwas mit dem ESS-Datensatz und dessen Struktur vertraut gemacht haben, gehen wir nun zur Datenanalyse in R über. Dieser Übergang ist ein fliessender: Die Schritte zum Kennenlernen eines Datensatzes, die Plausibilitätsprüfung und die Suche nach Fehlern liefern häufig bereits erste Ergebnisse im Sinne einer explorativen Datenanalysen, die umgekehrt dann womöglich neue Schritte zur Aufbereitung des Datensatzes veranlassen. Der 08. Wochenplan fokussiert die Rolle von Kreuztabellen in diesem Prozess, inklusive der Berechnung von Chi-Quadrat-Tests. Von hier geht es dann in den Folgewochen schrittweise weiter zu Berechnung von Regressionsanalysen. Wir können daher folgende Lernziele festhalten: Sie verstehen den Nutzen (und auch die Gefahren) der Funktion attach(). Sie können Kreuztabellen auf verschiedene Weisen darstellen und inhaltlich zutreffend interpretieren. Sie lernen die Interpretation von inferenzstatstischen Test in R anhand von einem Chi-Quadrat Test kennen. Sie verstehen, wie man das Ergebnis von Funktionen als Objekt abspeichert, und wissen, wie man auf die Elemente solcher Ergebnisobjekte zugreift. Sie entwickeln ein vertieftes Verständnis davon, wie die Phasen der Datenaufbereitung, der Datenexploration und der Datenanalyse zusammenhängen. 8.2 Aufgaben Laden Sie den Datensatz in RStudio, der 200 zufällige Fälle aus dem ganzen ESS Datensatz enthält.11 Erfassen Sie den hierfür benötigten Code nicht nur im Markdown, sondern erstellen Sie zusätlich ein eigenes R-Skript (z.B. unter dem Namen ess_import.R). Dieses Skript sollte automatisch das richtige Arbeitsverzeichnis definieren, den Datensatz laden und alle notwendigen Datenaufbereitungen vornehmen. Sie können also auch bereits die ersten Definitionen von fehlenden Werten dort integrieren. Die Idee ist, dass dieses Skript über die kommenden Wochen umfangreicher wird, wenn wir Faktoren definieren, Variablenbezeichnungen ändern, neue Variablen berechnen, usw. Hier das Beispiel eines möglichen Importskripts: # Laden der Daten setwd(&quot;C:/Users/SchweglG/R_Daten/06_HS21/R_Seminar-HS21&quot;) #daten_ess &lt;- read.csv(file = &quot;Daten/ESS1-8e01.csv&quot;) #daten_ess &lt;- daten_ess[sample(row.names(daten_ess),200),] #write.csv(daten_ess, file = &quot;Daten/ESS1-8e01_HS21.csv&quot;) daten_ess &lt;- read.csv(file = &quot;Daten/ESS1-8e01_HS21.csv&quot;) # nicht benoetigte Variablen loeschen ## bisher keine nicht benoetigen Variablen vorhanden # Definieren von fehlenden Werten daten_ess$yrbrn[daten_ess$yrbrn==7777 | daten_ess$yrbrn==8888 | daten_ess$yrbrn==9999] &lt;- NA daten_ess$agea[daten_ess$agea==999] &lt;- NA daten_ess$edulvla[daten_ess$edulvla&gt;5] &lt;- NA daten_ess$eisced[daten_ess$eisced&gt;7] &lt;- NA daten_ess$eduyrs[daten_ess$eduyrs&gt;76] &lt;- NA # Fehler in den Faellen Korrigieren ## Fall 132 daten_ess$agea[daten_ess$x==132] &lt;- 2016 - daten_ess$yrbrn[daten_ess$x==132] daten_ess$agea[daten_ess$x==132] ## Fall 352 daten_ess$agea[daten_ess$x==352] &lt;- 2016 - daten_ess$yrbrn[daten_ess$x==352] ## Fall 443 daten_ess$edulvla[daten_ess$x==443] &lt;- NA ## Fall 1414 daten_ess$agea[daten_ess$x==1414] &lt;- NA daten_ess$yrbrn[daten_ess$x==1414] &lt;- NA ## Fall 1482 daten_ess$eduyrs[daten_ess$x==1482] &lt;- round(mean(daten_ess$eduyrs[daten_ess$edulvla==3], na.rm = T)) # Faktoren definieren ## folgt in der naechsten Einheit Das R-Skript kann nun laufend ergänzt werden, etwa mit der Definition von weiteren fehlenden Werte bei anderen Variablen oder mit der Erstellen von Faktoren (siehe nächster WP). Das hat den Vorteil, dass zu Beginn einer neuen Einheit (d.h. wenn Sie ein neues Markdown erstellen) jeweils nicht der ganze Code nochmals aufgeführt werden muss, sondern direkt dieses Skript ausgeführt werden kann. Das Ausführen eines Skriptes (oder auch einer sonstigen Textdatei) erfolgt über die Funktion source(). #Nochmals das Arbeitsverzeichnis definieren falls nötig: setwd(&quot;C:/Users/SchweglG/R_Daten/06_HS21/R_Seminar-HS21&quot;) #Aufrufen des R Skripts: source(file = &quot;ess_import.R&quot;) Zur Wiederholung: Wir laden uns also wiederum Rohdaten und müssen dann fehlende Werte definieren. Natürlich kennt die Funktion read.csv() ein Umgang mit fehlenden Werten über das Argument na.strings. Allerdings sind in unserem Rohdatensatz fehlende Werte eben auch als Zahlen definiert (z.Bsp. 7777) und werden  bevor dies nicht anders definiert haben  auch als numerische Werte eingelesen.   2. Erläutern Sie in eigenen Worten die Idee der Funktion attach(). Nutzen Sie diese Funktion für die Aufgaben 3 und 4. Wenden Sie den detach()-Befehl an, bevor Sie zu Aufgabe 5 übergehen. Die attach()-Funktion erlaubt es uns, einen bestimmten Datensatz (d.h. ein Dataframe-Objekt) zu aktivieren (Manderscheid 2017, 54). Dadurch können wir direkt auf diesen Datensatz zugreifen, ohne den Umweg des $-Zeichens: #Am Beispiel der Variable zu Geschlecht: ##...vor dem attachen: table(daten_ess$gndr) ## ## 1 2 ## 108 92 ##die Funktion attach(daten_ess) ##...nach dem attachen: table(gndr) ## gndr ## 1 2 ## 108 92 Ohne die attach()-Funktion würde die letzte Zeile des Code-Chunks oben zu einer Fehlermeldung führen. Fabio Keller hat dies nochmals etwas anders formuliert, um den Prozess der Funktion zu verdeutlichen: Standardmässig gibt es das Global Environment, also die Super-Umgebung, in der alle Objekte existieren, die wir bisher erstellt haben. Die Funktion attach() erstellt eine zusätzliche Umgebung, in der Objekte existieren können. Sie zerteilt ein DataFrame, das mit dem what Argument angegeben wird, in seine einzelnen Variablen und lässt sie als freie Objekte in der neuen Umgebung leben, dessen Name man mit dem name Argument spezifizieren kann. Wann sollten wir mit der Funktion arbeiten, wann nicht? Mit der Funktion zu arbeiten ist vor allem dann sinnvoll, wenn die Arbeit mit nur einem Datensatz im Zentrum steht und wenn keine Änderungen mehr am Datensatz selber vorgenommen werden. Bei der Arbeit mit mehreren Datensatzsätzen könnte die Funktion leicht für Verwirrung sorgen (gerade auch dann wenn Ihre Codezeilen von jemand anderem interpretiert werden sollen). Eine Hierarchie bei mehreren attachten Objekten könnte noch über das Argument postdefiniert werden. Dies macht allerdings kaum Sinn. Sollten Sie weiterhin Änderungen am Datensatz vornehmen kann die attach()-Funktion dazu führen, dass die von Ihnen gemachten Änderungen nicht sichtbar werden (beim Betrachten des Datensatzes) oder auch nicht in die weiteren Schritte/Berechnungen aufgenommen werden. Sobald also Änderungen vorgenommen werden gilt es auch den Datensatz wieder zu detachen. Nach folgender Befehlszeile kommt eine Warnung  was könnte das Problem sein? attach(daten_ess) Jetzt findet R verschiedene freie Objekte (Keller) mit dem selben Namen in der Environment. Dies aufgrund der Tatsache, dass eben ein Datensatz mehrmals attached wurde (ersichtlich über Global Enviroment). Wenn der Datensatz jetzt nur einmal detached wird kann immer noch direkt auf dieses Objekt zugreifen, weil es eben noch ein zweites Mal attached wurde: detach(daten_ess) table(gndr)   3. Erstellen Sie mittels der Funktion table() eine Kreuztabelle zwischen der Variable zu Geschlecht und derjenigen zum Interesse an Politik. Wie können Sie vorgehen, um eine Kreuztabelle mit Prozentwerten (relative statt absolute Häufigkeiten) zu erstellen, die ausserdem die Randsummen enthält? Suchen Sie eine Darstellung der Tabelle - d.h. der Anordnung der Variablen in Zeilen und Spalten sowie der Wahl der Randsummen -, die sinnvoll das Verhältnis von abhängiger und unabhängiger Variable wiedergibt. Was fällt Ihnen inhaltlich an der Tabelle auf? Wenn wir Tabellen (bzw. Kontingenztabellen) darstellen, so ist die Konvention dass bei gerichteten Beziehungen die Ausprägungen der unabhängigen Variablen (X) den Spalten zugeordnet werden und die Ausprägungen der abhängigen Variablen (Y) den Reihen zugeordnet werden (Diaz-Bone 2019, 70). Die Aufgabenstellung spricht zwar nicht von einer gerichteten Beziehung, aber gleichzeitig macht es Sinn, Geschlecht als unabhängige Variable anzusehen (und nicht dass das Interesse an Politik das Geschlecht einer Person bestimmt). Gemäss dieser Überlegung sollen dann auch die Randsummen ausgerichtet werden, da uns die Verteilungen des Politikinteresses je nach Geschlecht interessiert. Das heisst das pro Geschlecht jeweils auf 100% summiert werden soll. Zur Erinnerung: Dies sind Ausprägungen der beiden Variablen: gndr 1 Male 2 Female 9 No answer polintr 1 Very interested 2 Quite interested 3 Hardly interested 4 Not at all interested 7 Refusal 8 Dont know 9 No answer Die Verteilung von gndr haben wir oben bereits betrachtet, daher folgt hier nur noch die Variable polintr: table(polintr) ## polintr ## 1 2 3 4 8 ## 38 74 67 20 1 # Womöglich verfügt die Variable &quot;polintr&quot; über fehlende Werte. Diese können wir definieren: daten_ess$polintr[daten_ess$polintr&gt;4] &lt;- NA #Dieser Befehl könnten wir dann ebenfalls in unser Import-Skript übernehmen. Anschliessend erstellen wir die benötige Tabelle. Hierbei lohnt es sich von innen nach aussen vorzugehen (vgl. auch Manderscheid 2017, 95): #als logischer Aufbau der Tabelle addmargins(table(polintr, gndr)) prop.table(table(polintr, gndr)) # Welche Variante macht nun mehr Sinn? prop.table(table(polintr, gndr),2) prop.table(table(polintr, gndr),1) #Randsummen ergänzen: addmargins(prop.table(table(polintr, gndr),2)) #Warum könnten wir eine der beiden Randsummen weglassen? Wieso nicht? # V1 addmargins(prop.table(table(polintr, gndr),2),1) round(addmargins(prop.table(table(polintr, gndr),2),1),2) #V2 round(addmargins(prop.table(table(polintr, gndr),2)),2) Hier wäre nun unsere benötigte Tabelle: round(addmargins(prop.table(table(polintr, gndr),2),1),2) ## gndr ## polintr 1 2 ## 1 0.26 0.11 ## 2 0.38 0.36 ## 3 0.29 0.39 ## 4 0.06 0.14 ## 8 0.01 0.00 ## Sum 1.00 1.00 Diese verdeutlicht, dass zwischen den Variablen ein Zusammenhang bestehen könnte, da Männer anscheinend öfters angeben, sehr an Politik interessiert zu sein.   4. Die Funktion CrossTable() ist Teil des Paketes gmodels und ermöglicht die flexible und detailreiche Arbeit mit Kreuztabellen. Versuchen Sie mittels dieser Funktion die Kreuztabelle aus Aufgabe 2 nachzubauen. #install.packages(&quot;gmodels&quot;) library(gmodels) ?CrossTable ## starting httpd help server ... done Ein Blick in die Hilfeseite zeigt uns, dass wir vor allem diejenigen Argument anders definieren müssen, die wir nicht in unserer Tabelle haben wollen. Das heisst wir müssen angeben (FALSE), dass wir keine Spalten- und Tabellenprozente sowie keine Angaben zum Chi-Quadrat-Beiträgen haben möchten. Leider scheint aber kein Argument vorhanden zu sein, mit der wir die absoluten Zahlen entfernen und die Randverteilung auch noch gemäss der Tabelle oben darstellen könnten. CrossTable(polintr, gndr, digits = 2, prop.r = F, prop.t = F, prop.chisq = F, ) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Col Total | ## |-------------------------| ## ## ## Total Observations in Table: 200 ## ## ## | gndr ## polintr | 1 | 2 | Row Total | ## -------------|-----------|-----------|-----------| ## 1 | 28 | 10 | 38 | ## | 0.26 | 0.11 | | ## -------------|-----------|-----------|-----------| ## 2 | 41 | 33 | 74 | ## | 0.38 | 0.36 | | ## -------------|-----------|-----------|-----------| ## 3 | 31 | 36 | 67 | ## | 0.29 | 0.39 | | ## -------------|-----------|-----------|-----------| ## 4 | 7 | 13 | 20 | ## | 0.06 | 0.14 | | ## -------------|-----------|-----------|-----------| ## 8 | 1 | 0 | 1 | ## | 0.01 | 0.00 | | ## -------------|-----------|-----------|-----------| ## Column Total | 108 | 92 | 200 | ## | 0.54 | 0.46 | | ## -------------|-----------|-----------|-----------| ## ## Hingegen bietet die Funktion über das Argument format noch eine schöne Darstellungsweise, die Josias Bruder genutzt hat: CrossTable(polintr, gndr, digits = 2, prop.r = F, prop.t = F, prop.chisq = F, format = &quot;SPSS&quot;) ## ## Cell Contents ## |-------------------------| ## | Count | ## | Column Percent | ## |-------------------------| ## ## Total Observations in Table: 200 ## ## | gndr ## polintr | 1 | 2 | Row Total | ## -------------|-----------|-----------|-----------| ## 1 | 28 | 10 | 38 | ## | 25.93% | 10.87% | | ## -------------|-----------|-----------|-----------| ## 2 | 41 | 33 | 74 | ## | 37.96% | 35.87% | | ## -------------|-----------|-----------|-----------| ## 3 | 31 | 36 | 67 | ## | 28.70% | 39.13% | | ## -------------|-----------|-----------|-----------| ## 4 | 7 | 13 | 20 | ## | 6.48% | 14.13% | | ## -------------|-----------|-----------|-----------| ## 8 | 1 | 0 | 1 | ## | 0.93% | 0.00% | | ## -------------|-----------|-----------|-----------| ## Column Total | 108 | 92 | 200 | ## | 54.00% | 46.00% | | ## -------------|-----------|-----------|-----------| ## ## Hinweis: Die Funktion CrossTable() kennt keinen Weg mit fehlenden Wert umzugehen. Sollten Sie also einen fehlenden Wert in einer Variable haben müssen Sie diesen selber entfernen. Hier ein Beispiel, in dem ein fehlender Wert beim Fall 45 (z.Bsp. bei der Variable polintr) vorhanden wäre: CrossTable(polintr[-c(45)], gndr[-c(45)], digits = 2, prop.r = F, prop.t = F, prop.chisq = F, format = &quot;SPSS&quot;)   5. Mittels CrossTable() lassen sich nicht nur Tabellen erstellen, sondern auch Chi-Quadrat Tests durchführen. + Kopieren Sie Ihren Kode der Kreuztabelle aus Ausgabe 4, ergänzen Sie diesen um das nötige Argument für den Chi-Quadrat Test und speichern Sie dies als ein neues Objekt CT_Ergebnis. + Greifen Sie anschliessend auf die darin enthaltenen Angaben zum Chi-Quadrat Test zu und interpretieren Sie diese - sowohl deskriptiv als auch inferenzstatistisch. detach(daten_ess) CT_Ergebnis &lt;- CrossTable(daten_ess$polintr, daten_ess$gndr, digits = 2, prop.r = F, prop.t = F, prop.chisq = F, chisq = T) #Achtung: hier muss nun das Format &#39;SPSS&#39; entfernt werden ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Col Total | ## |-------------------------| ## ## ## Total Observations in Table: 199 ## ## ## | daten_ess$gndr ## daten_ess$polintr | 1 | 2 | Row Total | ## ------------------|-----------|-----------|-----------| ## 1 | 28 | 10 | 38 | ## | 0.26 | 0.11 | | ## ------------------|-----------|-----------|-----------| ## 2 | 41 | 33 | 74 | ## | 0.38 | 0.36 | | ## ------------------|-----------|-----------|-----------| ## 3 | 31 | 36 | 67 | ## | 0.29 | 0.39 | | ## ------------------|-----------|-----------|-----------| ## 4 | 7 | 13 | 20 | ## | 0.07 | 0.14 | | ## ------------------|-----------|-----------|-----------| ## Column Total | 107 | 92 | 199 | ## | 0.54 | 0.46 | | ## ------------------|-----------|-----------|-----------| ## ## ## Statistics for All Table Factors ## ## ## Pearson&#39;s Chi-squared test ## ------------------------------------------------------------ ## Chi^2 = 10.49328 d.f. = 3 p = 0.01480654 ## ## ## summary(CT_Ergebnis) ## Length Class Mode ## t 8 table numeric ## prop.row 8 table numeric ## prop.col 8 table numeric ## prop.tbl 8 table numeric ## chisq 9 htest list str(CT_Ergebnis) ## List of 5 ## $ t : &#39;table&#39; int [1:4, 1:2] 28 41 31 7 10 33 36 13 ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ x: chr [1:4] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ## .. ..$ y: chr [1:2] &quot;1&quot; &quot;2&quot; ## $ prop.row: &#39;table&#39; num [1:4, 1:2] 0.737 0.554 0.463 0.35 0.263 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ x: chr [1:4] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ## .. ..$ y: chr [1:2] &quot;1&quot; &quot;2&quot; ## $ prop.col: &#39;table&#39; num [1:4, 1:2] 0.2617 0.3832 0.2897 0.0654 0.1087 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ x: chr [1:4] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ## .. ..$ y: chr [1:2] &quot;1&quot; &quot;2&quot; ## $ prop.tbl: &#39;table&#39; num [1:4, 1:2] 0.1407 0.206 0.1558 0.0352 0.0503 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ x: chr [1:4] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ## .. ..$ y: chr [1:2] &quot;1&quot; &quot;2&quot; ## $ chisq :List of 9 ## ..$ statistic: Named num 10.5 ## .. ..- attr(*, &quot;names&quot;)= chr &quot;X-squared&quot; ## ..$ parameter: Named int 3 ## .. ..- attr(*, &quot;names&quot;)= chr &quot;df&quot; ## ..$ p.value : num 0.0148 ## ..$ method : chr &quot;Pearson&#39;s Chi-squared test&quot; ## ..$ data.name: chr &quot;t&quot; ## ..$ observed : &#39;table&#39; int [1:4, 1:2] 28 41 31 7 10 33 36 13 ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ x: chr [1:4] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ## .. .. ..$ y: chr [1:2] &quot;1&quot; &quot;2&quot; ## ..$ expected : num [1:4, 1:2] 20.4 39.8 36 10.8 17.6 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ x: chr [1:4] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ## .. .. ..$ y: chr [1:2] &quot;1&quot; &quot;2&quot; ## ..$ residuals: &#39;table&#39; num [1:4, 1:2] 1.674 0.192 -0.837 -1.145 -1.806 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ x: chr [1:4] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ## .. .. ..$ y: chr [1:2] &quot;1&quot; &quot;2&quot; ## ..$ stdres : &#39;table&#39; num [1:4, 1:2] 2.738 0.356 -1.512 -1.775 -2.738 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ x: chr [1:4] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ## .. .. ..$ y: chr [1:2] &quot;1&quot; &quot;2&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;htest&quot; Neben den verschiedenen Werten der Tabellen (Fallzahlen sowie den Spalten-, Reihen- und Tabellenprozenten) finden wir auch das Unterobjekt des Chi-Quadrattests: chisq. Auf dieses können wir sowohl insgesamt zugreifen CT_Ergebnis$chisq ## ## Pearson&#39;s Chi-squared test ## ## data: t ## X-squared = 10.493, df = 3, p-value = 0.01481 #oder etwas umständlicher, aber auch möglich: CT_Ergebnis[&quot;chisq&quot;] ## $chisq ## ## Pearson&#39;s Chi-squared test ## ## data: t ## X-squared = 10.493, df = 3, p-value = 0.01481 als eben auch auf die Unterelemente: CT_Ergebnis$chisq$statistic ## X-squared ## 10.49328 CT_Ergebnis$chisq$parameter ## df ## 3 CT_Ergebnis$chisq$p.value ## [1] 0.01480654 CT_Ergebnis$chisq$method ## [1] &quot;Pearson&#39;s Chi-squared test&quot; CT_Ergebnis$chisq$data.name ## [1] &quot;t&quot; CT_Ergebnis$chisq$observed ## y ## x 1 2 ## 1 28 10 ## 2 41 33 ## 3 31 36 ## 4 7 13 CT_Ergebnis$chisq$expected ## y ## x 1 2 ## 1 20.43216 17.567839 ## 2 39.78894 34.211055 ## 3 36.02513 30.974874 ## 4 10.75377 9.246231 CT_Ergebnis$chisq$residuals ## y ## x 1 2 ## 1 1.6742286 -1.8055633 ## 2 0.1919918 -0.2070526 ## 3 -0.8372288 0.9029052 ## 4 -1.1446881 1.2344831 Wie können wir die Ergebnisse nun interpretieren? Zuerst deskriptiv statistisch: Wir erhalten einen Chi-Quadrat Wert gemäss Pearson von: CT_Ergebnis$chisq$statistic ## X-squared ## 10.49328 Das heisst dass ein Zusammenhang zwischen den beiden Variablen Geschlecht und Politikinteresse besteht. Allerdings ist die Grössenordnung dieses Wertes nicht nur durch eine Stärke des Zusammenhangs bestimmt, sondern auch durch das Tabellenformat (Diaz-Bone 2019, 86). Dieses Tabellenformat wird über die Freiheitsgrade (df) deutlich: (i-1) x (j-1). Oder eben: CT_Ergebnis$chisq$parameter ## df ## 3 Möchte man hingegen verschiedene Zusammenhänge und deren Stärke miteinander vergleichen, so kann man auf Cramers V zurückgreifen (etwa über die Funktion cramersV() im Paket lsr). #install.packages(&quot;lsr&quot;) library(lsr) cramersV(daten_ess$polintr, daten_ess$gndr) ## [1] 0.2296303 #oder auch manuell: sqrt(CT_Ergebnis$chisq$statistic/(200*(2-1))) ## X-squared ## 0.2290555 Cramers V verweist so auf einen schwachen bis mittleren Zusammenhang. Anschliessend können wir zur inferenzstatistischen Betrachtung übergehen: Einige von den Teilnehmer*innen haben den Chi-Quadrat Test von Hand durchgeführt. Dies ist natürlich auch möglich (vgl. Diaz-Bone 2019, 86): Hypothesen H0: In der Grundgesamtheit besteht kein Zusammenhang zwischen Geschlecht und Politikinteresse. H1: Es besteht in der Grundgesamtheit ein Zusammenhang zwischen Geschlecht und Politikinteresse. Stichprobnerverteilung Wiederum können wir die Stichprobernvertilung mittels des parameter-Unterobjektes bestimmen, nämlich als Verteilung mittels 3 Freiheitsgraden: CT_Ergebnis$chisq$parameter ## df ## 3 Annahme- und Rückweisungsbereich Der Test soll mit einem Signifikanzniveau von 5% durchgeführt werden: qchisq(0.95,3, lower.tail = T) ## [1] 7.814728 Auswertung der STichprobe und Testentscheidung Der Stichprobenkennwert ist: CT_Ergebnis$chisq$statistic ## X-squared ## 10.49328 Dieser Wert fällt nun in den Rückweisungsbereich und der Stichprobenbefund ist signifikant. Die H0 muss verworfen werden und auch in der Grundgesamtheit besteht ein Zusammenhang zwischen Geschlecht und Politikinteresse. Dieses Vorgehen entspricht allerdings nicht der Realität des statistischen Arbeitens. Wir arbeiten nämlich direkt mit dem sogenannten p-Wert. Dieser Wert liegt bei: CT_Ergebnis$chisq$p.value ## [1] 0.01480654 Unser berechnete Prüfwert Chi-Quadrat fällt auf einem 5% Signifikanzniveau in den Ablehnungsbereich beziehungsweise können wir auf einem 95% Signifikanzniveau die Alternativhypothese annehmen, dass auch in der Grundgesamtheit ein Zusammenhang zwischen den beiden Variablen besteht (vgl. auch Manderscheid 2017, 165f). Der p-Wert dreht dabei die Logik etwas um: Konkret sagt dieser Wert nämlich aus, dass lediglich eine round(CT_Ergebnis$chisq$p.value,3)*100 ## [1] 1.5 % Wahrscheinlichkeit besteht, dass unser Zusammenhang in der Stichprobe zufällig aufgetreten ist. Ist jetzt das einfach ein dummes Beispiel? Wir könnten uns jetzt nochmals anschauen, wie dieser Chi-Quadrat Wert zustandekommt: CrossTable(daten_ess$polintr, daten_ess$gndr, prop.r = F, prop.t = F, prop.c = F) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | Chi-square contribution | ## |-------------------------| ## ## ## Total Observations in Table: 199 ## ## ## | daten_ess$gndr ## daten_ess$polintr | 1 | 2 | Row Total | ## ------------------|-----------|-----------|-----------| ## 1 | 28 | 10 | 38 | ## | 2.803 | 3.260 | | ## ------------------|-----------|-----------|-----------| ## 2 | 41 | 33 | 74 | ## | 0.037 | 0.043 | | ## ------------------|-----------|-----------|-----------| ## 3 | 31 | 36 | 67 | ## | 0.701 | 0.815 | | ## ------------------|-----------|-----------|-----------| ## 4 | 7 | 13 | 20 | ## | 1.310 | 1.524 | | ## ------------------|-----------|-----------|-----------| ## Column Total | 107 | 92 | 199 | ## ------------------|-----------|-----------|-----------| ## ## Und dort sehen wir, dass vor allem die Unterschiede in der ersten Ausprägung (polintr = 1) die Differenz ausmachen. 8.3 Einführung Faktoren Genau genommen haben wir bisher immer so getant, als wären Variablen wie gndr und polintr metrische Variablen. Dies ist natürlich nicht korrekt, sondern wir haben es hier mit einer nominalen und einer ordinalen Variable zu tun. Solche nicht-metrischen oder eben kategorialen Variablen können wir in R über Faktoren definieren. Neben Vektoren (die wir bisher vor allem mit metrischen Variablen gleichgesetzt haben) sind Faktoren ein weiterer wichtiger Objekttyp der für die Organisation von sozialwissenschaftlichen Beobachtungen in Form von Variablen relevant ist (Manderscheid 2017, 35). Dieser Objekttyp enthält nun nicht mehr die Zahlen in einer Reihe, sondern Levels. Dies sind benannte Elemente und wir können diese insbesondere für Variablen mit nominalen oder ordinalen Skalenniveau verwenden. Der Unterschied zwischen numerischen Variablen/Vektoren und kategorial Variablen/Faktoren wird in R wichtig, denn zwischen den Objekttypen wird differenziert. Das heisst das gewisse Funktionen nicht mehr mit Faktoren ausgeführt werden können. Am Beispiel von gndr: #daten_ess$gndr + 1 #würde eigentlich aktuell noch Sinn machen table(daten_ess$gndr) ## ## 1 2 ## 108 92 plot(daten_ess$gndr) daten_ess$gndr &lt;- factor(daten_ess$gndr) levels(daten_ess$gndr) ## [1] &quot;1&quot; &quot;2&quot; levels(daten_ess$gndr) &lt;- c(&quot;weiblich&quot;, &quot;männlich&quot;) str(daten_ess$gndr) ## Factor w/ 2 levels &quot;weiblich&quot;,&quot;männlich&quot;: 1 2 2 2 1 1 2 1 1 1 ... #daten_ess$gndr + 1 #funktioniert jetzt nicht mehr table(daten_ess$gndr) ## ## weiblich männlich ## 108 92 plot(daten_ess$gndr) Und am Beispiel von polintr, was nun einem geordneten (d.h. ordinalen) Faktorn entspricht: table(daten_ess$polintr) ## ## 1 2 3 4 ## 38 74 67 20 plot(daten_ess$polintr) #Die Tabelle macht deutlich, dass hier eben die numerischen Ausprägungen in umgekehrter Reihenfolge aufgeführt sind # 1=als die ranghöchste Ausprägung, deshalb muss zuerst die Reihenfogle der Levels bestimmt werden: daten_ess$polintr &lt;- factor(daten_ess$polintr, levels =c(4,3,2,1), ordered = T) str(daten_ess$polintr) ## Ord.factor w/ 4 levels &quot;4&quot;&lt;&quot;3&quot;&lt;&quot;2&quot;&lt;&quot;1&quot;: 3 3 1 2 2 4 4 3 3 3 ... levels(daten_ess$polintr) &lt;- c(&quot;Not at all interested&quot;, &quot;Hardly interested&quot;, &quot;Quite interested&quot;, &quot;Very interested&quot;) str(daten_ess$polintr) ## Ord.factor w/ 4 levels &quot;Not at all interested&quot;&lt;..: 3 3 1 2 2 4 4 3 3 3 ... table(daten_ess$polintr) ## ## Not at all interested Hardly interested Quite interested ## 20 67 74 ## Very interested ## 38 plot(daten_ess$polintr) References "],["wochenplan-09.html", "9 Wochenplan 09 9.1 Lernziele 9.2 Aufgaben", " 9 Wochenplan 09 zur 09. &amp; 10. Einheit, Umgang mit Faktoren 9.1 Lernziele Aufbauend auf unserer bisherigen Arbeit mit Kreuztabellen besteht das Lernziel für diese Woche darin, dass Sie sich mit einem für die tägliche Arbeit mit R wichtigen Datenformat vertraut machen: Faktoren. Faktoren sind eine geeignete Art, kategoriale  d.h. nominale und ordinale  Daten in R zu repräsentieren. Der Umgang mit dieser Art von Daten ist allerdings nicht ganz leicht und erfordert ein wenig Übung. Die folgenden konkreten Lernziele lassen sich für ein erstes Kennenlernen von Faktoren formulieren: Sie können dafür geeignete Variablen in Faktoren umwandeln. Sie verstehen den Unterschied zwischen geordneten und ungeordneten Faktoren und können beide in R erstellen. Sie verstehen die spezifische Zwei-Ebenen-Struktur von Faktoren und insbesondere die Rolle von Levels. Sie wissen, wie man die Levels eines Faktors definiert. Sie wissen, wie man Levels neu benennt, umsortiert, zusammenfasst und/oder nicht benötigte Levels entfernt. 9.2 Aufgaben Laden Sie Ihr Datenimport-Skript und fügen Sie einen Abschnitt zur Definition und Aufbereitung einzelner Variablen als Faktoren ein. Arbeiten Sie anschliessend in einem R Markdown weiter. Gehen Sie unseren Übungsdatensatz konzentriert durch und überlegen Sie, welche Variablen als geordnete oder ungeordnete Faktoren gespeichert werden sollten. Definieren Sie nun jede dieser Variablen als Faktor (die einzige Ausnahme soll die Variable isco08 sein). Überprüfen Sie mittels Kreuztabellen, ob Ihre Faktoren sinnvoll und fehlerfrei definiert sind. Inkludieren Sie die einwandfrei funktionierenden Lösungen zur Faktorendefinition in Ihr Datenimportskript. Testen Sie danach, ob das Skript funktioniert: Löschen Sie Ihre Environment (und alle Objekte), laden Sie mittels source() den Datensatz neu und rufen Sie dann  als Test  eine als Faktor definierte Variable auf.   Arbeiten Sie für die Falllösung wie immer mit R Markdown und generieren Sie sich ein PDF Ihrer Lösungen. Beschriften Sie Ihr Dokument mit FL09_NameVorname.pdf und geben das Dokument via OLAT vor der nächsten Einheit ab. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
