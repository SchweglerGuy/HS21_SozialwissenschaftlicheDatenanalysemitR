[
["index.html", "Sozialwissenschaftliche Datenanalyse mit R Einführung", " Sozialwissenschaftliche Datenanalyse mit R Kenneth Horvath &amp; Guy Schwegler HS 2020 Einführung Das Seminar “Sozialwissenschaftliche Datenanalyse mit R” bietet eine systematische Einführung in das Statistikpaket R. R ist eine Open Source Software, die sich unter anderem durch Flexibilität und vielfältige Möglichkeiten der grafischen und numerischen Datenanalyse auszeichnet. Das Seminar führt in inhaltlicher Abstimmung mit der Vorlesung „Grundlagen der multivariaten Statistik“ in Aufbau und Funktionsweise des Programms sowie in die Umsetzung wichtiger statistischer Verfahren (etwa lineare Regression und logistische Regression) ein. Anhand dieser Verfahren werden unter anderem Techniken des effizienten Datenmanagements, Möglichkeiten, eigenständig kleine Funktionen zu programmieren, sowie Formen der grafischen Datenanalyse und Ergebnisdarstellung besprochen. Das vorliegende Dokument ist ein sogennantes Bookdown (Xie 2020), siehe auch hier, und dient der Ergebnissicherung im Seminarverlauf. Das heisst dass die im Seminar besprochene Themen hiernochmals schriftlich festgehalten, diskutiert und allenfalls Literatur ergänzt werden (siehe für allgemeine Literatur etwa Diaz-Bone (2019), Kabacoff (2015) oder Manderscheid (2017)). Das Bookdown wird laufend aktualisiert. Ebenfalls werden in diesem Bookdown die Lösungen für die im Seminar gestellten Aufgaben (Falllösungen) präsentiert. References "],
["wocheplan-01.html", "1 Wocheplan 01 1.1 Sozialwissenschaftliche Datenanalyse 1.2 Ziel des Kurses 1.3 R als Programm &amp; RStudio 1.4 Lernziele der ersten Woche 1.5 Aufgaben der ersten Woche", " 1 Wocheplan 01 …zur Einheit vom 17.09.2020, Einführung 1.1 Sozialwissenschaftliche Datenanalyse Das Seminar “sozialwissenschaftliche Datenanalyse mit R” versucht eine Realität des statistischen Arbeitens zu vermitteln und ergänzt so Vorlesung “Grundlagen der multivariaten Statistik” gleich in zweierlei Hinsicht: Erstens wird eine Auswahl der gelernten statistischen Verfahren konkret angewendet (und so auch nochmals repetiert). Zweitens zeigt sich neben den eigentlichen Verfahren ein weiterer, impliziter Teil der Statistik: ein Umgang mit Daten, deren Aufbereitung und Verarbeitung sowie all die damit einhergehenden Herausforderungen. Hinter dem Seminar steht eine bestimmte Vorstellung der sozialwissenschaftlichen Datenanalyse, die folgende Teile enthält (Wickham and Grolemund 2016): Figure 1.1: Modell Datenanalyse Als erster Schritt müssen die Daten eingelesen bzw. importiert werden. Die importierten Daten gilt es dann aufzubereiten und aufzuräumen. Das bedeutet, dass sie in einer konsistenten Form gespeichert werden sollen (z.Bsp. dass jede Zeile einer Person und jede Spalte einer Variable entspricht). Dieser zweite Schritt ist im Rahmen von Sekundärdaten (wie auch wir sie verwenden werden) oft bereits erfolgt. Ein weiterer Schritt ist es dann, die Daten zu transformieren. Das heisst, die Fälle und ihre Ausprägungen auf ein bestimmes Interesse eingegrenzt (z.Bsp. auf alle Personen die über ein bestimmtes Einkommen verfügen), neue Variablen werden erstellt (die Funktionen bestehender Variablen sind, etwa Einkommensklassen), und eine Reihe von zusammenfassenden Statistiken werden berechnet (verschiedene univariate Kennwerte). Das Aufbereiten und Transformieren ist ein grosser Teil der statistischen Analyse (es ist ein Kampf mit den Daten, Wickham and Grolemund 2016, Kap.1.1). Ziel dieser Arbeit ist es, die Daten in eine passende Form zu bringen, um optimal mit ihnen arbeiten zu können. Wenn die Daten (voerst) in einer optimalen Form vorliegen gibt es zwei Hauptmotoren der Wissensgenerierung (Wickham and Grolemund 2016, Kap.1.1): Visualisierung und Modellierung. Mit Visualisierungen lässt sich schnell eine Übersicht gewinnen (z.Bsp. könnte es überhaupt einen Zusammenhang zwischen zwei Variablen geben?). Modellierungen wiederum ergänzen diese ersten Einsichten, in dem sie präzise Antworten auf Fragen geben (wie gross ist der Zusammenhang genau?). Das Transformieren, Visualisieren und Modellieren der Daten ist dabei keineswegs ein linearer Prozess, sondern es ergeben sich in ihm immer wieder Wechselwirkungen, Rückbezüge und dadurch neue Wege, um an die Daten heranzutreten. Der letzte Schritt der Datenanalyse ist die Kommunikation. Es gilt also sowohl das Vorgehen (zumindest teilweise) als inbesondere die Ergebnisse der Analyse anderen mitzuteilen. Diese Prozesse der Datenanalyse finden alle in einem bestimmen Rahmen statt (vgl. auch Sauer 2019, 3). Dies ist auf der einen Seite die Idees des Programmierens im Vorgehen selber (vgl. Wickham and Grolemund 2016, Kap.1.1). Auf der anderen Seite bilden aber die Sozialwissenschaften selber auch einen Rahmen um dieses Vorgehen, anhand dessen etwa Datenstrukturen (z.Bsp. dass eine Person ein Fall und damit eine Zeile ist) oder angemessene Ziele der Analyse (ab wann ist ein Zusammenhang etwa “gross”?) vorgegeben werden. 1.2 Ziel des Kurses Das Seminar verfolgt zwei miteinander verzahnte, übergeordnete Lernziele. Einerseits sollen die Studierenden sich Grundkenntnisse der statistischen Datenanalyse mit R aneignen. Andererseits werden ausgewählte Inhalte der Vorlesung praktisch angewandt und damit auch veranschaulicht.1 Konkret sollen die Studierenden am Ende des Semesters… …einen ersten Einblick in Abläufe und Anforderungen softwaregestützter Datenanalyse haben, …typische Herausforderungen statistischen Arbeitens eigenständig bewältigen können, …die allgemeine Funktionsweise und die Struktur von R verstehen, …die Umsetzung ausgewählter multivariater Verfahren in R beherrschen, …dabei auch grafische Verfahren als zentrale Bausteine aktueller Datenanalyse einsetzen können …sowie die Grundlage dafür erworben haben, flexibel eigene Analysestrategien in R um-zusetzen. 1.3 R als Programm &amp; RStudio R als Programmiersprache wurde von Beginn an für die Statistik bzw. für die Statistiklehre entwickelt. Die Anfänge des Programms fanden in den 1990er Jahre an der Universität Auckland in Neuseeland statt, wo R von Ross Ihaka und Robert Gentleman entwickelt wurde (Manderscheid 2017, 1). Der Buchstabe “R” als Name geht sowohl auf eine ältere Grundlage zurück – die Programmiersprache “S” – als auch auf die Vornamen der beiden Entwickler (ebd., vgl. auch Sauer 2019, 13f). Das R-Projekt wurde in der Zusammenareit mit weiteren Wissenschaftler_Innen voran getrieben und bald auch unter der General Public Licence (GNU) veröffentlicht (Manderscheid 2017, 1). R ist daher frei zugänglich, kostenlos und darf von allen verändert werden. Es ist insbesondere auch diese Open Source Idee, die R zu seiner Verbreitung half – und die sicherstellt, dass die neusten Entwicklungen in und mit der Software stattfinden. R als Programm ist in Paketen organisiert und präsentiert sich als “Statistikumgebung” (Manderscheid 2017, 1). Ausgehend von der Basisversion bzw. des Basispaketes kann R beliebig erweitert werden. Unter https://cran.r-project.org/ findet sich eine beständig wachsende und umfangreiche Sammlung von Paketen, die sowohl Lösungen für allgemeine Verfahren anbieten (etwa Pakete für die multiple Korrespondenzanalyse, siehe “soc.ca”) als auch für spezifische Probleme (etwa für “Atomic Force Microscope Image Analysis” beim Paket “AFM”). Diese Pakete können installiert werden und es gilt sie dann jeweils noch zu laden, bevor sie verwendet werden können. Nach dem Beenden des Programms werden die verwendeten Pakete wiederum “versort” und es gilt sie beim nächsten mal wieder zu laden (die Pakete beleiben aber installiert). Letzterer Vorgang stellt sicher, dass R “schlank” bleibt, d.h. nur immer die benötigen Dinge auch ausgeführt werden. install.packages(&quot;soc.ca&quot;) #...installiert das Paket library(soc.ca) #...lädt das Paket Neben dieser Open Source Idee und der daraus folgenden, beständigen Aktualisierung und Erweiterungen des Programms zeichnet R sich weiter durch dessen Stärke im Bereich der Visualisierung aus. Es bieten sich unbegrenzte Möglichkeiten für Grafiken und Diagramme, sowohl bereits in der Basisversion als insbesondere auch mit spezifischen Paketen (siehe Chang et al. 2020). Neben der Basisversion von R und R als eigentlicher Programmiersprache gibt es grafische Benutzeroberflächen (GUIs), um mit der Programmiersprache umzugehen. Im Zentrum unseres Seminars steht RStudio, die am weitesten verbreitete grafische Benutzeroberfläche von R. Diese Oberfläche biete einige praktische Zusatzfunktionen und erleichtert so das Arbeiten mit R durch Autovervollständigkeitsfunktionen, automatische Einrückungen, Syntaxhervorhebung, integrierte Hilfsfunktion, Informationen zu Objekten im Workspace, menügestützten Oberflächen und Daten-Viewer (Manderscheid 2017, 18). Die eigentliche Arbeit verrichtet aber weiterhin R selber, und R wird automatisch gestartet wird, wenn Sie RStudio starten (Sauer 2019, 21). Man kann diese Arbeitsteilung mit einem Auto vergleichen: R ist der Motor des Autos, während RStudio das Amaturenbrett ist, vor dem Sie sitzen und das Auto lenken. 1.4 Lernziele der ersten Woche Die erste Seminarwoche dient dazu, die technischen Voraussetzungen für die gemeinsame Arbeit im Seminar zu prüfen und mit der geplanten Arbeitsweise vertraut zu werden. Das Seminar ist insbesondere in den online durchgeführten Teilen als eine Art “flipped classroom” konzipiert. Sie bekommen also von Woche zu Woche konkrete Arbeitsaufträge. Diese sollen Sie eigenständig bewältigen und alle Probleme und Unklarheiten notieren, die sich im Arbeitsprozess ergeben. Die gemeinsamen Sitzungen dienen dann dazu, Lösungswege zu den Aufgaben zu präsentieren, offene Fragen zu klären, Konzepte vertiefend zu erläutern und die nächsten Schritte vorzubereiten. Für jede Woche werden Lernziele und Arbeitsaufträge definiert. Für die erste Seminarwoche lassen sich als Lernziele festhalten: Sie wissen, wie Sie die aktuellen Versionen von R und RStudio auf Ihrem Computer installieren Sie wissen, wie man R-Pakete installiert und in R lädt Sie können eine Funktion aufrufen Sie haben einen soliden ersten Eindruck, wie man mit R kommuniziert und einfache Operationen durchführt Sie haben eine erste Orientierung zu Unterstützungsangeboten, die man online findet (auch wenn diese teilweise noch überfordernd wirken) 1.5 Aufgaben der ersten Woche Installieren Sie die aktuellen Versionen von R und RStudio auf Ihrem Endgerät! Sie sollten sich Notizen, wenn es Probleme gibt – und für das nächste Mal gleich festhalten, wie Sie diese gelöst haben. Da die Details der Installation vom Betriebssystem und den Spezifikationen des Endgeräts abhängen, ist es normal, dass dieser Prozess manchmal erst auf den zweiten Versuch funktioniert. Installieren Sie das Paket “swirl” und laden Sie es. “swirl” ist eine in R implementierte interaktive Einführung in die Grundlagen von R! Rufen Sie die Funktion swirl() auf und spielen Sie ein wenig damit. Rufen Sie sich in Erinnerung, was Sie aus dem letzten Semester noch über die Arbeit mit R wissen! Notieren Sie sich, was Ihnen Sie noch kennen, was Ihnen neu vorkommt, usw. Verwenden Sie ein wenig Zeit darauf, online nach R Tutorials, Foren, etc. zu suchen. Halten Sie die URLs von Seiten und Ressourcen fest, die Ihnen hilfreich und/oder wichtig (aber u.U. noch etwas schwer zu durchschauen) vorkommen! Erstellen Sie aus Ihren Notizen ein PDF Dokument, beschriften Sie dieses mit Fallloesung01_NameVorname.pdf und geben das Dokument via OLAT bis Mittwochmittag ab (23.09.2020, 12:00). References "],
["wochenplan-02.html", "2 Wochenplan 02 2.1 Lernziele der zweiten Woche 2.2 Aufgaben", " 2 Wochenplan 02 …zur Einheit vom 24.09. &amp; 01.10.2020, Grundlagen (Teil 1) 2.1 Lernziele der zweiten Woche In der zweiten Seminarwoche geht es darum, die Grundlagen von R und RStudio zu repetieren und zu erweitern.2 Für den weiteren Verlauf wollen wir R als Sprache auffassen – sowohl als Programmiersprache als auch als Sprache in einem metaphorischen Sinn. Wir wollen also ein komplexes System zur Kommunikation kennenlernen. Wie bei einer anderen Sprache gibt es auch hier Zeichen mit Bedeutungen (ähnliche wie Nomen, Verben, …) und Regeln zur Verknüpfung dieser Zeichen (ähnlich wie eine Grammatik). Diese Grundlagen gilt es alle erstmal kennenzulernen und zu verstehen. Am Anfang wird vieles schwer fallen, mit der Zeit gewinnt man aber Sicherheit. Der zentrale Punkt in dieser Vorstellung von R als Sprache ist dabei folgender: Wir lernen eine Sprache dadurch, dass wir sie immer wieder anwenden, Probleme lösen und vor allem auch Fehler machen. R als Software und als Programmiersprache hat eine steile Lernkurve und zu Beginn werden viele Probleme auftauchen. Im Umgang mit den Problemen soll allerdings auch eine eigene Arbeitsweise mit dem Programm erlernt werden (Fehlermeldungen lesen, Lösungsstrategien im Codieren erlernen, selber Hilfe suchen, …). Für die beständige Erweiterung der Grundlagen und das Erlernen der “Sprache R” besteht eine Herausforderung darin, den Weg zwischen scheinbarer Trivialität und überfordernder Komplexität zu finden: Es gilt die kleinen Schritte ernstzunehmen, sonst werden die grossen Schritte sehr schnell mühsam. Für die zweite Seminarwoche lassen sich folgende Seminarziele festhalten: Sie können die verschiedenen Funktionsweisen der vier Fenster in RStudio erläutern. Sie verstehen den Unterschied zwischen der Arbeit in der Konsole und im Skript. Sie verstehen, wie und wozu man im Skript kommentiert. Sie haben R-Markdown als erweitertes Skript und Arbeitsinstrument kennengelernt. Sie wissen, was ein Arbeitsverzeichnis in R ist und wozu es gut ist. Sie verstehen das erste Grundelement der “Sprache R”: Funktionen Sie wissen, wie Funktionen aufgebaut sind; Sie wissen, wie Sie sich Hilfe zu Funktionen holen; Sie wissen, was Argumente in einer Funktion bewirken. Sie verstehen das zweite Grundelement der “Sprache R”: Objekte Sie verstehen, was es bedeutet, dass in R “alles ein Objekt ist”; Sie wissen, wie man sich die jeweils aktuell verfügbare Objekte anzeigen lässt; Sie haben das Zusammenspiel von Funktionen und Objekte kennengelernt; Sie kennen bereits drei verschiedenen Arten von Objekten. 2.2 Aufgaben Fassen Sie noch einmal für sich und in eigenen Worten die Funktionen der vier Fenster von R zusammen. Oben links findet sich in R-Studio das Skript-Fenster, in dem Befehle eingegeben und kommentiert werden können. Ausgeführt werden diese Befehle erst, wenn Sie Ctrl und Enter drücken (bzw. Cmd &amp; Enter). Diese Eingabe von Kodezeilen wird ergänzt durch die direkte Eingabe in der Konsole. In diesem Fenster läuft das eigentliche Programm R (es ist also dieselbe Ansicht wie wenn Sie R ohne grafische Benutzeroberfläche starten würden). Im Gegensatz zum Skript können hier Befehle nur immer einzeln eingegeben und sie müssen dann direkt ausgeführt werden. Dies ermöglicht ein schneller ausprobieren, aber eben kein wirklich speichern, beständiges überarbeiten, kommentieren und eine klare Dokumentation des Ablaufs, wie dies im Skript erfolgen kann. Die beiden Fenster zur Eingabe von Kode werden vom Environment-Fenster ergänzt. Hier finden sich die abgespeicherten Objekte sowie in den weiteren Reitern die bisher ausgeführten Befehle (History), aber auch eine erweitere Netzwerk- bzw. Serverumgebung (Connections, Build, …), falls Sie z.Bsp. mit weiteren Personen an einem Projekt arbeiten. Im vierten Fenster werden Grafiken, Hilfeseiten, die Vorschau für geknittete Dokumente und auch die Ordnerstruktur angezeigt. Sie finden diverse Einstellungsoptionen zu den vier Fenster und deren Anordnung unter „Tools &gt; Gobal Options“. Dort können Sie etwa unter dem Reiter”General\" die Option zum “Save Workspace to RData on exit” zu Never wechseln. Dies führt dazu, dass Ihre Environment beim Verlassen von R immer gelöscht wird. Dies ist nicht etwa ein Nachteil, sondern eine Technik die Sie dazu veranlasst, alle benötigen Schritte in Ihrem Kode unterzubringen. Weiter könne Sie unter dem Reiter “Spelling” auch noch die Rechtschreibefunktion deaktiveren, da das Feature noch nicht wirklich für die deutsche Rechtschreibung zu funktionieren scheint (bzw. die Ergänzung von neuen Wörterbüchern nicht fehlerfrei abläuft). Die Ordnerstruktur im Reiter Files des vierten Fensters hängt mit Ihrem aktuellen Arbeitsverzeichnis zusammen. Ein Arbeitsverzeichnis ist der Ort, auf den R immer als Erstes zugreift und wo Dinge automatisch abgelegt werden. Dieses können über die Menüsteuerung “Session &gt; Set Working Directory &gt; Choose Directory” oder über den Befehl setwd() definieren (erstere Variante ist etwas einfacher). Der getwd() Befehl wiederum gibt das aktuell festgelegte Verzeichnis aus. Speichern Sie jeweils Ihr aktuelles Arbeitsverzeichnis als Teil des Markdowns, z.Bsp. so: setwd(&quot;C:/Users/SchweglG/R_Daten/HS20/E3&quot;) #Dies dient in einem Skript oder einem Markdown als Erinnerung, ... #...wo Ihr Arbeitsverzeichnis liegt (und damit wo Sie Ihre Daten wiederfinden) Wir können uns Beispielsweise eine CSV-Datei (Comma Separated Value) aus der Liste unserer installierten Pakete erstellen, und zwar über folgenden Befehl: write.table(pakete_liste, file=&quot;Paketliste_neu.csv&quot;, sep=&quot;,&quot;) Diese CSV-Datei, welche die Liste enhält, sollte dann in Ihrem aktuellen Arbeitsverzeichnis abgelegt werden. Führen Sie sowohl in der Konsole als auch im Skript einige Rechnungen durch und speichern Sie im Skript die Resultate als Kommentare. Hier ein paar Beispielrechnung: 1 + 1 5 + 3 - 4 (5 + 3 - 4) / 5 … Fortsetzung: Was ist der Vorteil der Arbeit im Skript gegenüber dem Schreiben von Code direkt in der Konsole? Sie möchten nun das Ergebnis Ihrer letzten Rechnung als Objekt x abspeichern. Wie könnten Sie das in der Konsole tun, ohne dass Sie nochmals den Kode der Rechnung (in unserem Beispiel “(5 + 3 - 4) / 5”) selber schreiben müssten? Und wo ist x nun hin – wo oder wie können Sie x sehen? Siehe für die Vorteile in der jeweiligen Arbeit die Beschreibungen oben. In der Konsole kann über die beiden Pfeiltasten runter und rauf durch bisher ausgeführte Befehle gescrollt werden. So können Sie die Rechnung erneut aufrufen und dem Objekt x zuweisen. Das Objekt taucht dann im Environment-Fenster auf. Es kann dann über die Funktion rm() wieder entfernt werden. Rekapitulieren Sie noch einmal: Was könnten die Vorteile davon sein, mit R Markdown zu arbeiten (Allaire et al. 2020)? Wann arbeitet man besser mit einem klassischen Skript? Schauen Sie sich auch die Formatierungsmöglichkeiten für Fliesstext in den Cheatsheets zu R Markdown an (siehe hier)! Neben der Konsole und dem Skript ist R Markdown die dritte Möglichkeit in R-Studio zu arbeiten (Allaire et al. (2020)).3 R Markown ist ebenfalls ein Paket und ermöglicht im Sinne eines erweiterten Skripts das Erstellen eines dynamische Analysedokumente: Sie kombinieren Kode, gerenderte Ausgaben (z. B. Grafiken) und Text. Eine neue R Markdown Datei kann via «File &gt; New File &gt; R Markdown» oder mit dem Button direkt unter dem «File» Button geöffnet werden. Die Datei selber ist unterteilt in einfachen Fliesstext (weisser Hintergrund) oder sogenannten R Chunk (grauer Hintergrund), Kodestücke für R. Ein R Chunk kann entweder über den «Insert»-Button im R Markdown-Fenster eingefügt werden, oder aber über die Tastenkombination «Ctrl + Alt + i» (bzw. «Cmd + Option + i» bei Mac). Öffnen Sie eine neue R Markdown Datei. Versuchen Sie Ihre bisherigen Notizen zu den Aufgaben und Ihren Code in dieser Datei unterzubringen. Arbeiten Sie für die folgenden Aufgaben mit dieser Datei weiter. Siehe Anmerkungen bei Aufgabe 3. Versuchen Sie allgemein zu beschreiben, was Funktionen und was Objekte sind. Fügen Sie eine kurze Erläuterung in Ihr Markdown-Dokument zur Frage ein, was im folgenden Code jeweils Funktionen und was Objekte (und wenn letzteres, welche Art von Objekt) sind: sqrt(x) help(&quot;sqrt&quot;) ?help y &lt;- c(1, 3, 4, 5, 6, 7, NA) z &lt;- c(7, 8, 10, 11) Die Grundlagen der Sprache von R sind Funktionen und Objekte (vgl. Manderscheid 2017, 25ff), d.h. Befehle in R bestehen aus diesen beiden Elemente. Die Funktionen bestimmen, was mit einem Objekt passiert. Ein Objekt wiederum kann verschiedenste Formen annehmen, etwa eine Zahl, eine Menge von verschiedenen Werten (z.Bsp. ein Datensatz), das Ergebnis einer Funktion – und selbst eine Funktion selber kann wieder zu einem Objekt werden. Dies zeigt sich etwa bei den beiden Hilfe-Funktionen help() und ?. Hier wird die Hilfe aufgerufen für eine andere Funktion. Für die weiteren Kodezeilen in der Aufgabe lassen sich jeweils folgende Funktionen und Objekte auflisten: Funktionen Objekte sqrt() x &lt;-, c() y, 1, 3, 4, 5, 6, 7, NA … … Im Objekt y scheint ein fehlender Wert vorhanden zu sein (NA = Not Available). Möchten man nun etwa die Varianz für dieses Objekt berechnen folgt als Resultat ebenfalls nur “NA”: var(y) ## [1] NA Wie könnten wir trotzdem die Varianz für das Objekt y berechnen? Hier kommen nun Argumente ins Spiel. Argumente erweitern nochmals die Funktionen, in dem Sie deren Details bestimmen. Wir können so etwa definieren, dass die Funktion var() beim Objekt y die fehlende Werte ignoriert: var(y, na.rm = T) ## [1] 4.666667 Unser Objekt z scheint hingegen keine solchen fehlenden Wert zu beinhaltet. Trotzdem scheint etwas nicht zu stimmen, wenn Sie das Resultat der Funktion zur Berechnung Standardabweichung sd() mit dem Ergebnis der dazugehörigen Formel vergleichen (vgl. Diaz-Bone 2019, 50f).4 Was könnte das Problem sein und wo die Lösung liegen? Bonusaufgabe: Sehen Sie sich die Hilfeseite der Funktion matrix() an. Wozu dient diese Funktion? Welche Argumente akzeptiert / benötigt sie und wozu dienen diese? Illustrieren Sie die Funktionsweise anhand von ein paar Beispielen. R kennt verschiedenste Arten von Objekten (fürs Erste bleiben wir hier bei verschiedenen Objekten für Zahlen). Im bisherigen Verlauf haben Sie bereits einzelne Zahlen sowie Vektoren als eine Reihe von Zahlen kennengelernt. Über die Funktion matrix() lässt sich nun ein dritter Objekttyp generieren, nämlich eine Matrix. Matrizen enthalten Zeilen und Spalten des jeweils selben Datentyps.5 matrix(c(1,2,3,4,5,6,7,8,9,10), nrow = 2, ncol = 5) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 3 5 7 9 ## [2,] 2 4 6 8 10 Auch lässt sich so beispielsweise eine Art sozialwissenschaftlicher Datensatz erstellen: einstell &lt;- c(1,3,5,4,3) koerp_g &lt;- c(176,180,192,156,168) gndr &lt;- c(1,2,1,2,0) daten &lt;- matrix(c(einstell, koerp_g, gndr), nrow = 5, ncol = 3, byrow = F) daten ## [,1] [,2] [,3] ## [1,] 1 176 1 ## [2,] 3 180 2 ## [3,] 5 192 1 ## [4,] 4 156 2 ## [5,] 3 168 0 In der Kodezeilen, wo die Funktion matrix() enthalten ist, sind wiederum Argumente vorhanden, nämlich nrow, ncol und byrow. Die genaue Funktion der Argument ist ebenfalls auf der Hilfeseite der Funktion ersichtlich. References "],
["wochenplan-03.html", "3 Wochenplan 03 3.1 Lernziele WP03 3.2 Aufgaben WP03", " 3 Wochenplan 03 …zur Einheit vom 01. &amp; 08.10.2020, Grundlagen (Teil 2) 3.1 Lernziele WP03 In der dritten Seminarwoche vertiefen wir Elemente der Programmiersprache R, die Sie bereits kennengelernt haben, und betten sie in neue Zusammenhänge ein. Das Ziel ist es, das Verständnis von Funktionen und Objekten, von den verschiedenen Arten von Objekten in R sowie immer auch von der “Grammatik” dieser Sprache weiter zu schärfen. Zusätzlich werden in dieser Woche zwei neue Grundlagen eingeführt: Erstens die verschiedenen Arten von Daten, mit denen wir in R arbeiten: Zahlen, Text und logische Werte; zweitens einige Möglichkeiten, über bestimmte Funktionen systematisch Vektoren zu definieren. Zusammenfassend lassen sich damit folgende Seminarziele festhalten: Sie können mittels Argumenten eine Funktion spezifizieren und damit Einfluss auf die Details der Berechnung und der Ergebnisdarstellung nehmen. Sie können die drei bisher kennengelernten Objekttypen (einzelne Zahl, Vektor, und Matrix) kombinieren. Sie kennen die drei verschiedenen Datenarten von R: Sie verstehen die Rolle von numerischen Daten, Sie verstehen die Rolle von textförmigen Daten Sie verstehen die Rolle von logischen Daten. Sie können Vektoren mittels der Funktionen … … seq() … sowie rep() definieren … und diese auf alle drei Arten von Daten anwenden. 3.2 Aufgaben WP03 Der Vektor v besteht aus folgenden Zahlen: 1, 3, 7, 9, 22, 2, 8, 14, 20, 3, 7, 9, 11, und 13. Berechnen Sie auf drei verschiedenen Arten das arithmetische Mittel dieses Vektors. v &lt;- c(1, 3, 7, 9, 22, 2, 8, 14, 20, 3, 7, 9, 11, 13) Eine erste Möglichkeit, das arithmetische Mittel zu berechnen, ist das manuelle Aufsummieren und das anschliessende Teilen durch die Anzahl Elemente. (1+ 3+ 7+ 9+ 22+ 2+ 8+ 14+ 20+ 3+ 7+ 9+ 11+ 13)/14 ## [1] 9.214286 Diese manuelle Variante kann etwa über die “Find/Replace” Funktion von R Studio erleichtert werden, indem die Kommata durch Pluszeichen ersetzt werden. Eine zweite Möglichkeit bietet die Kombination der Funktionen sum() und lenght(), welche die oben manuell ausgeführten Schritte umsetzen. sum(v)/length(v) ## [1] 9.214286 Die dritte Möglichkeit ist natürlich die Funktion für das arithmetische Mittel selber, mean(): mean(v) ## [1] 9.214286 Was macht das Argument trim für die Funktion mean()? Wozu könnten Sie dieses Argument in einem sozialwissenschaftlichen Kontext nutzen? Spezifizieren Sie das Argument auf sinnvolle Weise, um das arithmetische Mittel des in Aufgabe 1 erstellten Vektors v zu berechnen! Mittels des Arguments trim können Sie Anteile der Elemente eines Vektors für die Berechnung entfernen, also etwa die kleinsten und grössten 10%: mean(v, trim = 0.1) ## [1] 8.833333 sort(v) ## [1] 1 2 3 3 7 7 8 9 9 11 13 14 20 22 mean(c(2,3,3,7,7,8,9,9,11,13,14,20)) ## [1] 8.833333 Damit können sowohl die kleinen wie auch die grossen Extremwerte bei der Berechnung eines Durchschnitts ignoriert werden. Erstellen Sie vier verschiedene Vektoren mit je einer Länge von vier und verbinden Sie diese zu einer 4x4-Matrix. Die Funktion apply() erlaubt Ihnen, eine Funktionen wie z.B. mean() oder var() auf diese Matrix anzuwenden. Nur wie genau? Sehen Sie sich die Hilfe zu apply() an, probieren Sie die Funktion aus und versuchen Sie zu verstehen, wie sie genau funktioniert. Erläutern Sie apply() dann in eigenen Worten und mit Hilfe der von Ihnen erzeugten 4x4-Matrix! m1 &lt;- c(2,4,7,9) m2 &lt;- c(8,9,5,2) m3 &lt;- c(3,5,5,5) m4 &lt;- c(9,8,7,6) ma &lt;- cbind(m1,m2,m3,m4) mb &lt;- rbind(m1,m2,m3,m4) ma ## m1 m2 m3 m4 ## [1,] 2 8 3 9 ## [2,] 4 9 5 8 ## [3,] 7 5 5 7 ## [4,] 9 2 5 6 mb ## [,1] [,2] [,3] [,4] ## m1 2 4 7 9 ## m2 8 9 5 2 ## m3 3 5 5 5 ## m4 9 8 7 6 apply(ma, 1, mean) ## [1] 5.5 6.5 6.0 5.5 apply(ma, 2, mean) ## m1 m2 m3 m4 ## 5.5 6.0 4.5 7.5 apply(mb, 1, mean) ## m1 m2 m3 m4 ## 5.5 6.0 4.5 7.5 apply(mb, 2, mean) ## [1] 5.5 6.5 6.0 5.5 Nachdem die vier Vektoren erstellt wurden lassen sie sich über die ‚rbind()‘ und ‚cbind()‘ Funktionen auf zwei verschiedene Weisen zu einer Matrix verbinden, entweder Zeilen- oder Spalte-weise (siehe die Objekte ma und mb). apply() als nächster Schritt wendet eine bestimmte Funktion auf ein Objekte an. Bei einem Objekt des Typs Matrix muss allerdings noch spezifiziert werden, ob die Funktion Zeilen- oder Spalte-weise angewendet wird. Zielen oder Spalten werden über die Zahlen 1 bzw. 2 definiert. So ergeben sich insgesamt vier Möglichkeiten, für die Berechnung der des Durchschnitts – allerdings generieren diese vier Möglichkeiten nur zwei unterschiedliche Ergebnisse. Erstellen Sie je einen Vektor mit numerischen Daten, textförmigen Daten und logischen Daten. Die Funktionen as.numeric(), as.character() und as.logical() lassen Sie eine Datenarten in eine andere “zwingen” bzw. als eine andere Datenart interpretieren. Wann funktioniert dies? Und wo sind die Grenzen dieses “Zwingens”? a &lt;- c(0, 1, 2) b &lt;- c(&quot;null&quot;, &quot;eins&quot;, &quot;zwei&quot;) #die Anführungs- und Schlusszeichen beachten b2 &lt;- c(&quot;0&quot;, &quot;1&quot;, &quot;2&quot;) c &lt;- c(TRUE, FALSE, T) #Die logischen Objekte können sowohl ausgeschreiben als auch lediglich als T und F aufgeführt werden Nachdem wir die Vektoren definiert haben, können wir deren Typ bestimmen und mit den as.-Funktionen spielen: is.numeric(a) ## [1] TRUE is.character(b) ## [1] TRUE is.character(b2) ## [1] TRUE is.logical(c) ## [1] TRUE as.numeric(b) ## Warning: NAs durch Umwandlung erzeugt ## [1] NA NA NA as.numeric(b2) ## [1] 0 1 2 as.numeric(c) ## [1] 1 0 1 as.character(a) ## [1] &quot;0&quot; &quot;1&quot; &quot;2&quot; as.character(c) ## [1] &quot;TRUE&quot; &quot;FALSE&quot; &quot;TRUE&quot; as.logical(a) ## [1] FALSE TRUE TRUE as.logical(b) ## [1] NA NA NA as.logical(b2) ## [1] NA NA NA Die Grenzen dieses “Zwingens” der as.-Funktionen finden sich auf der einen Seite bei ausgeschriebenen Zahlen in einem Charakter-Vektor. Diese ausgeschriebenen Zahlen können von R nicht in numerische oder auch logische Daten umformuliert werden. Anderseits zeigen sich die Grenzen im Zusammenhang zu den logischen Daten: Hier funktioniert das “Zwingen” nur mit numerische Daten – und alles über 1 wird als TRUE interpretiert. Definieren Sie folgende Vektoren mittels der Funktionen seq() und rep(): Vektor 5a: 1 2 3 4 5 6 7 8 9 10 seq(from = 1, to = 10, by = 1) ## [1] 1 2 3 4 5 6 7 8 9 10 seq(1:10) ## [1] 1 2 3 4 5 6 7 8 9 10 #oder 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 Vektor 5b: 1 1 1 2 2 2 3 3 3 c(rep(1,3), rep(2,3), rep(3,3)) ## [1] 1 1 1 2 2 2 3 3 3 Vektor 5c: Die Zahlen des Vektors b als ausgeschriebene Wörter c(rep(&quot;eins&quot;,3), rep(&quot;zwei&quot;,3), rep(&quot;drei&quot;,3)) ## [1] &quot;eins&quot; &quot;eins&quot; &quot;eins&quot; &quot;zwei&quot; &quot;zwei&quot; &quot;zwei&quot; &quot;drei&quot; &quot;drei&quot; &quot;drei&quot; Vektor 5d: 1 4 7 10 13 seq(1,13,3) ## [1] 1 4 7 10 13 Vektor 5e: 1 1 1 1 2 3 4 5 c(rep(1,3), 1:5) ## [1] 1 1 1 1 2 3 4 5 "],
["wochenplan-04.html", "4 Wochenplan 04 4.1 Lernziele WP04 4.2 Aufgaben WP04 4.3 Hinweise für den Wochenplan 05", " 4 Wochenplan 04 …zur Einheit vom 08. &amp; 15.10.2020, Korrelationen, Verteilungen und erstes grafisches Arbeiten (Vorbereitung Inferenzstatistik) 4.1 Lernziele WP04 Nachdem wir uns bereits Grundlagen für die Arbeit mit R und R Studio erarbeitet haben, wollen wir in der vierten Woche nun erste statistische Inhalte bzw. Methoden einführen und diese mit neuen Arbeitstechniken in R Studio ergänzen. Zu ersteren gehören in dieser Woche die bivariaten Zusammenhänge von metrischen Variablen (Korrelation) und verschiedene Zufallsverteilungen. Als Arbeitstechniken wollen wir neu grafische Techniken in R kennenlernen. Der vierte Wochenplan soll uns so nicht zuletzt vorbereiten, das Prinzip der Inferenzstatistik mittels R zu verstehen. Konkret lassen sich folgende Seminarziele festhalten: Sie verstehen, was eine Korrelation von zwei Variablen bedeutet und können bivariate Zusammenhänge in R berechnen und interpretieren. Sie können zwei Variablen in einem Streudiagramm darstellen und die Darstellungen interpretieren. Sie kennen den Unterschied von Gleichverteilungen und Normalverteilungen und können in R entsprechend verteilte Zufallsvariablen erstellen. Sie können metrische Verteilungen in Histogrammen darstellen. Sie haben erste Techniken kennenglernt, wie Grafiken erweitert und kombiniert werden können. Sie haben sich in R die Grundlagen für ein Verständnis von Inferenzstatistik allgemein und des Stichprobenfehlers im Besonderen erarbeitet. 4.2 Aufgaben WP04 Sie sollen verschiedene Paare von Vektoren mittels der Funktionen c(), rep() und seq() definieren, die jeweils unterschiedlich korrelieren. Diese Vektoren – d.h. Variablen – und deren Korrelationen sollen sozialwissenschaftlichen Phänomenen entsprechen. Berechnen Sie jeweils den Korrelationskoeffizienten. Ein Paar von Vektoren (mit je einer “Länge” von rund 100) soll eine Korrelation von ca. 0.4 aufweisen (…ein Beispiel von Frau Kurmann). x1 &lt;- c(seq(4100,14000,100)) y1 &lt;- c(rep(2, 5), rep(1, 7), rep(4, 18), rep (5, 35), rep(3, 25), rep(6, 10)) cor(x1, y1) ## [1] 0.3768417 Ein Paar von Vektoren (mit je einer “Länge” von rund 100) soll eine sehr starke Korrelation aufweisen (…ein Beispiel von Herr Ineichen). x2 &lt;- c(1,1,1,1,1, rep(seq(from=2, to=9), each=11),10,10,10,10,10,10) y2 &lt;- y2 &lt;- c(1,1,1,1, rep(seq(from=2, to=9), each=11),10,10,10,10,10,10,10) cor(x2,y2) ## [1] 0.9940175 Ein Paar von Vektoren (mit je einer “Länge” von rund 100) soll eine schwache, negative Korrelation aufweisen (…ein leicht angepasstes Beispiel von Frau Stöckli). #Originalbeispiel x3 &lt;- c(rep(0:1, 50)) y3 &lt;- c(rep(1, 30), rep(2, 35), rep(3, 21), rep(4, 10), rep(5, 4)) cor(x3, y3) #Dies ergibt leider eine schwache positive Korrelation. Deshalb hier einfach leicht angepasst: ## [1] 0.009064354 x3 &lt;- c(rep(c(1,0),50)) y3 &lt;- c(rep(1, 30), rep(2, 35), rep(3, 21), rep(4, 10), rep(5, 4)) cor(x3, y3) ## [1] -0.009064354 Nutzten Sie plot() um die Verteilungen Ihrer drei Variablen-Paare darzustellen. Verweisen Sie im Titel und in den Achsenbeschriftungen auf das sozialwissenschaftliche Phänomen, das Sie darstellen. Im Folgenden werden die drei Plots der Korrelationen dargestellt. Die zweite Grafik wird noch über eine Linie ergänzt, mit der die Korrelation etwas visualisiert wird. ACHTUNG: Der b0-Wert dieser Linie ist lediglich geschätzt (und ebenfalls müssen die Grössenordnungen der beiden Variablen ähnlich sein, damit dies hier funktioniert). Die dritte Grafik wurde über die Funktion jitter() ergänzt. Sie visuliert die einzelne Fälle in einem Plot über eine leichte, künstliche Streuung der Werte. #Plot der ersten beiden Vektoren plot(x1/1000, y1, main = &quot;Korrelation Einkommen und Anzahl Zimmer&quot;, xlab = &quot;Einkommen in CHF&quot;, ylab = &quot;Anzahl Zimmer im Haushalt&quot;) #Plot der zweiten beiden Vektoren plot(x2, y2, xlim = c(0,18), main = &quot;Korrelation Alters- und Schuljahre&quot;, xlab =&quot;Alter in Jahren&quot;, ylab = &quot;bisherige Schulzeit in Jahren (ohne Kindergarten)&quot; ) abline(a = 0.5, b = cor(x2,y2), col = &quot;red&quot;) #Plot der dritten beiden Vektoren plot(jitter(x3), jitter(y3), main = &quot;Korrelation Geschlecht und Einkommen&quot;, xlab = &quot;Geschlecht&quot;, ylab = &quot;Einkommensklasse&quot;) Erstellen Sie je einen Vektor für eine Gleichverteilung mittels runif() und für eine Normalverteilung mittels rnorm(). Diese Vektoren sollen als Variablen Körpergrössen repräsentieren. Erstellen Sie die beiden Vektoren in unterschiedlichen Längen, und zwar… …mit je 10, …mit je 30 …und mit je 1000 Fällen. Im Folgenden werden die in der Aufgabe verlangten Verteilungen als Histogramme dargestellt. Insbesondere bei grösseren Fallzahlen werden so die Eigenschaften der Verteilungen deutlicher: Bei der Gleichverteilung hat jede Ausprägung dieselbe Auftrittswahrscheinlichkeit. Das heisst, dass jede Körpergrösse zwischen Minimal- und Maximalwert mit derselben Häufigkeit vorkommt. Dies entspricht aber nicht der empirischen Realität von Körpergrössen. Bei der Normalverteilung gruppieren sich die meisten Werte um den Mittelwert von 170cm, während kleine und grosse Werte mit zunehmender Abweichung immer weniger häufig auftreten. Dies wiederum entspricht stärker der tatsächlichen, empirischen Verteilungen von Körpergrössen. Unterschieden werden die Normalverteilungen auch in der Funktion rnorm() über die beiden expliziten Parameter des arithmetischen Mittels und der Standardabweichung (Diaz-Bone 2019, 140f). hist(runif(10,150,210)) hist(runif(30,150,210)) hist(runif(1000,150,210)) hist(rnorm(10, 170,10)) hist(rnorm(30, 170,10)) hist(rnorm(1000, 170,10)) Als Ergänzung:runif und rnorm() werden noch von der Funktion rbinom() ergänzt, die zufällige binomiale Verteilungen erstellt. Dies sind Verteilungen die Ihnen angeben, ob ein Ereignis bei einer bestimmten Wahrscheinlichkeit eingetreten ist oder nicht. Damit kann zum Beispiel aufgezeigt werden, wie oft Sie bei zehn Münzwürfen Kopf bekommen (Beispiel I), oder auch wie oft Sie bei dreimal würfeln mit zwei Würfeln eine Sechs erzielen (Beispiel II). rbinom(10,1,0.5) #Beispiel I ## [1] 1 0 0 0 1 1 1 0 0 1 rbinom(3,2, 2/6) #Beispiel II ## [1] 0 1 1 Als nächstes sollen Sie die beiden Verteilungen aus der Aufgabe 4, die 1.000 Fälle aufweisen, grafisch darstellen. Nutzen Sie die Funktion für Histogramme, um die beiden Verteilungen darzustellen. Die beiden Histogramme sollen dieselbe Spannbreite in der x-Achsen haben und jeweils 10 Klassen aufweisen. Wählen Sie je eine Farbe für die Gleich- und Normalverteilung. Über die Funktion abline() kann dem aktuellen Plot eine Linie hinzugefügt werden. Fügen Sie jeweils einem Histogramm den Mittelwert der anderen Verteilung als vertikale Linie hinzu (ebenfalls in der entsprechenden Farbe). Bonusaufgabe: Das Argument add = TRUE lässt Sie eine neue Grafik über die aktuelle Grafik legen. Versuchen Sie, auf diese Art Ihre beiden Histogramme in einer Grafik darzustellen. Die folgenden Histogramme zeigen die beiden Verteilungen, inklusive den Mittelwerten der jeweils anderen Verteilung als horizontale Linie. Um diese Linie besser ersichtlich zu machen wurde sie mittels des Arguments lwd breiter gemacht. hist(runif(1000,150,210), breaks = 10 , xlim = c(140,220), col = &quot;blue&quot;, main = &quot;Körpergrösse, gleichverteilt&quot;) abline(v = mean(rnorm(1000, 170,10)), col = &quot;green&quot;, lwd = 5) hist(rnorm(1000, 170,10), breaks = 10 , xlim = c(140,220), col = &quot;green&quot;, main = &quot;Körpergrösse, normalverteilt&quot;) abline(v = mean(runif(1000,150,210)), col = &quot;blue&quot;, lwd = 5) Der nachfolgende Code generiert nun die sich überlagernden Histogramme über das Argument add = T in der zweiten Grafik. Zusätzlich muss noch bei der Darstellung der Gleichverteilung die y-Achse angepasst werden, da die Normalverteilung Klassen mit höheren Häufigkeiten aufweist. Weiter wurde über die rgb() Funktion die Farbei “grün” transparent gemacht (vgl. auch hier). hist(runif(1000,150,210), breaks = 10 , xlim = c(140,220), ylim = c(0,200), col = &quot;blue&quot;, main = &quot;Körpergrösse, gleichverteilt und normalverteil&quot;) hist(rnorm(1000, 170,10), breaks = 10 , col=rgb(0,255,0, max = 255, alpha = 125), add = T) 4.3 Hinweise für den Wochenplan 05 Im Rahmen des Wochenplans 05 und der dazugehörigen Falllösung wird das Zugreifen auf Daten (Indizieren und Subsetting) sowie die sogenannte if-Schlaufe relevant. Der folgende Abschnitt zeigt nochmals kurz die Elemente, die am Ende der Einheit vom 15.10.2020 eingeführt wurden. Beim Zugreifen auf Daten – das heisst dem Indizieren – und dem Erstellen von Teildatensätzen – dem Subsetting – werden drei verschiednen Weisen unterscheiden. [ ]: Die erste Art und Weise dies zu tun ist über die Verwendung von eckigen Klammer (vgl. Manderscheid 2017, 57ff). #Auf einzelne Elemente vektor1 &lt;- c(1,2,10,5,13,20) vektor1[4] ## [1] 5 datensatz &lt;- data.frame(vektor1, c(1,1,1,1,2,1)) datensatz[1,1] ## [1] 1 datensatz[6,1] ## [1] 20 datensatz[,0] ## data frame with 0 columns and 6 rows $: Die zweite Möglichkeit auf Datenzuzugreifen ist der Dollar-Operator $. Damit wird ein Dataframe als Liste angesprochen und die Funktion gibt einen Vektor (bzw. eine Variable) zurück (vgl. Sauer 2019, 59). datensatz$vektor1 ## [1] 1 2 10 5 13 20 which(): Die dritte Weise um Daten zu indizieren bildet die which()-Funktion die danach fragt, welche Fälle einer bestimmten Bedingung entsprechen. which(datensatz$vektor1==20) ## [1] 6 Wie bereits bei der which()-Funktion ersichtlich lassen sich diese drei Varianten natürlich kombinieren. Im folgenden Beispiel wird etwa ein Teildatensatz erstellt, in dem alle Fälle enthalten sind, die beim vektor1 einen Wert von 20 aufweisen (beachten Sie die doppelten Gleich-Zeichen bei der logischen Bedingung). t_datensatz &lt;- datensatz[which(datensatz$vektor1==20),] t_datensatz &lt;- datensatz[datensatz$vektor1==20,] #derselbe Befehl, einfach ohne which() auszuformulieren Neben dem Zugreifen auf Daten findet sich auch eine Teilaufgabe in den Falllösungen 05, in der Sie eine eigenen if-Schleife programmieren sollen. Schleifen ermöglichen es, dass bestimme Funktionen (Schleifeninhalt) mehrmals ausgeführt werden können. Die Häufigkeit dieser Ausführung wird in einer Schleife über eine Bedingung definiert (Schleifenkontrolle). Sie für weitere Ergänzugen zu Schleifen auch Manderscheid (2017) auf S.206f, hier, und…. for (variable in vector) { #Schleifenkontrolle funktion; #Schleifeninhalt } Hier drei Beispiele für eine solche if-Schleife: for(i in 0:5) { print(i); } ## [1] 0 ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 for(i in 1:2) { print(datensatz[,i]); } ## [1] 1 2 10 5 13 20 ## [1] 1 1 1 1 2 1 for(i in 1:ncol(datensatz)) { print(datensatz[,i]); } ## [1] 1 2 10 5 13 20 ## [1] 1 1 1 1 2 1 References "],
["wochenplan-05.html", "5 Wochenplan 05 5.1 Lernziele WP05 5.2 Aufgaben WP05", " 5 Wochenplan 05 …zur Einheit vom 15. &amp; 22.10.2020, Zugreifen auf Daten (Indizieren und Subsetting) sowie if-Schleifen (Vorbereitung Inferenzstatistik, Teil 2) 5.1 Lernziele WP05 Ihre zentrale Aufgabe für die vergangene Woche bestand darin, R zu nutzen, um selbstständig “statistisch sinnvolle” Daten zu erzeugen und mit diesen zu arbeiten. Dazu haben wir zusätzlich zu den Funktionen seq() und rep() erstmals mit Zufallsvariablen gearbeitet, mit cor() eine erste bivariate statistische Funktion eingesetzt sowie Streudiagramme und Histogramme erstellt. In der kommenden Arbeitswoche geht es nun darum, zwei neue Aspekte der Arbeit mit R kennenzulernen und einzuüben: das Zugreifen auf einzelne Elemente und Teile von Datenobjekten (Indizieren und Subsetting) und die Kontrolle von längeren Befehlsabläufen. Ausserdem werden wir mit Dataframes eine wichtige Art von Datenstrukturen in R einführen. Folgende Lernziele lassen sich festhalten: Sie verstehen, was ein Dataframe in R ist und können die Unterschiede zu einer Matrix benennen. Sie können gezielt auf einzelne Teile eines Datenobjekts zugreifen und kennen verschiedene Wege, das zu tun. Sie verstehen, wie eine for-Schleife funktioniert und können einfache Varianten davon selbst erstellen. Sie beginnen in der Arbeit mit R gezielt Funktionen zur Abfrage von Attributen von Objekten zu nutzen. 5.2 Aufgaben WP05 Erzeugen Sie ein Dataframe, das aus fünf Variablen besteht und 100 Fälle umfasst. Nutzen Sie dazu die verschiedenen Funktionen, die wir in den letzten beiden Wochen kennengelernt haben (Zufallsvariablen, rep(), seq() …). Probieren Sie ebenfalls, dass die fünfte Variablen dem Character Datenformat entspricht. Überlegen Sie sich einen sozialwissenschaftlichen Kontext für dieses Dataframe und benennen Sie die Variablen dementsprechend. Für den folgenden Datensatz wurden unter anderem Beispiel aus der letzten Falllösung genommen (WP04, Aufgabe 1) und mit Zufallsvariablen ergänzt: Geschlecht &lt;- round(runif(100),0) Einkommen_CHF &lt;- c(seq(4100,14000,100)) AnzahlZimmer_H &lt;- c(rep(2, 5), rep(1, 7), rep(4, 18), rep (5, 35), rep(3, 25), rep(6, 10)) Schulzeit_J &lt;- round(rnorm(100,14,2),0) Einkommen_Kl &lt;- c(rep(&quot;unterd&quot;, 19), rep(&quot;durchs&quot;, 41), rep(&quot;ueberd&quot;, 40)) #stammt nicht aus einer Lösung datensatz &lt;- data.frame(Geschlecht,Einkommen_CHF,AnzahlZimmer_H,Schulzeit_J,Einkommen_Kl) Wenden Sie die Funktionen dim(), names(), str(), class() und typeof() auf Ihr Dataframe an. Was sagen Ihnen diese Funktionen jeweils? Welcher Output dieser Funktionen leuchtet Ihnen ein, welcher weniger? Die folgenden Funktionen geben Ihnen… dim(datensatz) ## [1] 100 5 …die Dimensionen Ihres Datensatzes aus, das heisst die Zeilen- und Spaltenzahl, was der Anzahl Fällen und Variablen im Datensatz entspricht; names(datensatz) ## [1] &quot;Geschlecht&quot; &quot;Einkommen_CHF&quot; &quot;AnzahlZimmer_H&quot; &quot;Schulzeit_J&quot; ## [5] &quot;Einkommen_Kl&quot; …die Namen Ihrer Vektoren aus, also die Variablennamen im Datensatz; str(datensatz) ## &#39;data.frame&#39;: 100 obs. of 5 variables: ## $ Geschlecht : num 1 1 1 0 1 0 0 0 1 0 ... ## $ Einkommen_CHF : num 4100 4200 4300 4400 4500 4600 4700 4800 4900 5000 ... ## $ AnzahlZimmer_H: num 2 2 2 2 2 1 1 1 1 1 ... ## $ Schulzeit_J : num 13 15 15 16 14 13 14 14 14 17 ... ## $ Einkommen_Kl : chr &quot;unterd&quot; &quot;unterd&quot; &quot;unterd&quot; &quot;unterd&quot; ... …die Struktur Ihres Datensatzes aus, das heisst die Unterobjekte bzw. Variablen im Datensatz, auf die Sie mittels dem Dollarzeichen zugreifen können. class(datensatz) ## [1] &quot;data.frame&quot; …die Klasse Ihres Objektes datensatz aus, das heisst die von Ihnen zugewiesene Eigenschaft des Objektes; typeof(datensatz) ## [1] &quot;list&quot; …und den Typ Ihres Objektes, das heisst die R-interne Art und Weise, das Objekt abzuspeichern (vgl. Kabacoff 2015, 23ff). Schreiben Sie eine for-Schleife, die Ihnen für die vier numerischen Spalte Ihres Dataframes den Mittelwert berechnet. for (i in 1:(dim(datensatz)[2]-1)) { print(mean(datensatz[,i])) } ## [1] 0.58 ## [1] 9050 ## [1] 3.99 ## [1] 13.94 Einige Hinweise hierzu: Der Zusatz (dim(datensatz)[2]-1)) generiert lediglich die Zahl 4. Die for-Schleife soll ja den Mittelwerte für die vier numerischen Variablen des Datensatzes berechnen. Dies könnte man auch einfach hinschreiben (i in 1:4) oder mit verschiedenen weiteren Möglichkeiten erreichen (etwa über (ncol(datensatz)-1)). Der Vorteil einer so definierten (anstatt hingeschriebenen) Variante ist, dass wir eigentlich gar nicht wissen müssten wieviele Variablen das wären – und die Anzahl variieren könnte. for (i in 1:4) { print(mean(datensatz[,i])) } ## [1] 0.58 ## [1] 9050 ## [1] 3.99 ## [1] 13.94 #...funktioniert auch... Innerhalb der geschweiften Klammern wird definiert, was für jedes i wiederholt werden soll. In unserem Beispiel ist dies die Berechnung des Mittelwertes, während mit i unsere vier verschiedenen Variablen bezeichnet werden. Die Funktion print() stellt sicher, dass die berechneten Werte auch ausgegeben werden. Weitere tolle Lösungsvorschläge: #...von Frau Diethelm for(v in datensatz[,1:4]) { print(mean(v)) } #...von Herr Ineichen (allerdings mit einem kleinen Problem) for(i in seq_along(datensatz)){ print(mean(datensatz[,i])) } #..von Frau Nguyen mit cleverem Zusatz for (i in names(datensatz)) { if(class(datensatz[,i])==&#39;numeric&#39;){ print(mean(datensatz[,i])) } } Überprüfen Sie Ihre Ergebnisse, indem Sie die Berechnung des Mittelwerts “händisch” für jede Spalte einzeln durchführen (nutzen Sie dazu Indizierung/Subsetting mittels eckiger Klammern). mean(datensatz[,1]) #entspricht mean(datensatz$Geschlecht) ## [1] 0.58 mean(datensatz[,2]) #... mean(datensatz$Einkommen_CHF) ## [1] 9050 mean(datensatz[,3]) #... ## [1] 3.99 mean(datensatz[,4]) ## [1] 13.94 Definieren Sie als eine erste Variante eine logische Bedingung, die Ihnen erlaubt, Ihr Dataframe anhand der Character-Variable in ungefähr zwei Gruppen zu teilen. Teilen Sie dann als zweite Variante Ihren Datensatz anhand einer anderen logischen Bedingung mit einer Ihrer numerischen Variablen in zwei Gruppen. In diesen logischen Bedingung können wir verschiedene logische Operatoren verwenden (vgl. Manderscheid 2017, 26): gleich == ungleich != größer &gt; größer gleich &gt;~ kleiner &lt; kleiner gleich &lt;~ und &amp; oder | #Variante 1 datensatz_v11 &lt;- datensatz[datensatz$Einkommen_Kl==&quot;durchs&quot;,] datensatz_v12 &lt;- datensatz[datensatz$Einkommen_Kl!=&quot;durchs&quot;,] #Variante 1 datensatz_v21 &lt;- datensatz[datensatz$Einkommen_CHF&lt;9900,] datensatz_v22 &lt;- datensatz[datensatz$Einkommen_CHF&gt;10000,] Die erste Variante generiert zwei Teildatensätze, wobei im ersten alle mit durchschnittlichem Einkommen enthalten sind und im zweiten Teildatensatz diejenigen, die entweder über ein unter- oder überdurchschnittliches Einkommen verfügen. Die zweite Variante generiert zwei Teildatensätze indem eine Unterscheidung getroffen wird zwischen denjenigen, die entweder mehr oder weniger als 10’000CHF pro Monat verdienen. Wieso verwendet man hierfür nun nicht einfach eine Funktion, z.Bsp. subset() (oder auch split())? subset(datensatz, subset = datensatz$Einkommen_Kl==&quot;durchs&quot;) #das würde auch gehen Die Überlegungen, die wir machen, sollen immer einem Objekt gelten, d.h. wir wollen ein Bewusstsein für ein Objekt schaffen – im Beispiel des data.frames hat dieses Objekt eben Zeilen und Spalten und diesen können wir indizieren. Wir brauchen keine spezifischen Funktionen sondern können immer in einem allgemeinen Sinne (gemäss den drei Varianten [], $, which()) auf Objekte zugreifen. Dieses Zugreifen können wir immer verwenden – für Funktionen, für Argumente, für Teildatensätze (Subsetting), usw. Das Erstellen von Teildatensätzen kann nun auch komplexer werden durch verschiedenen Kombinationen. Im Folgenden werden zuerst die unterdurchschnittlichen und die durchschnittlichen Einkommen in einem Datensatz gepackt (Beispiel 1) und dann die Männer (Geschlecht==0), die unterdurchschnittlich viel Geld verdienen (Beispiel 2). # datensatz[datensatz$Einkommen_Kl==c(&quot;unterd&quot;, &quot;durchs&quot;),] !!! funktionert nicht!!! #...die funktionierenden Kombinationen: datensatz[datensatz$Einkommen_Kl==&quot;unterd&quot; | datensatz$Einkommen_Kl== &quot;durchs&quot;,] #Bsp1 datensatz[datensatz$Einkommen_Kl==&quot;unterd&quot; &amp; datensatz$Geschlecht==0,] #Bsp2 Berechnen Sie bei einer der beiden Varianten aus Aufgabe 6 dann noch einmal die Mittelwerte pro Spalte, aber getrennt für diese beiden Gruppen. Hierfür konnte nun einfach die for-Schleife nochmals genutzt werden: #...für den ersten Teildatensatz der ersten Variante for (i in 1:4) { print(mean(datensatz_v11[,i])) } ## [1] 0.6097561 ## [1] 8000 ## [1] 4.731707 ## [1] 14.02439 #...für den zweiten Teildatensatz der ersten Variante for (i in 1:4) { print(mean(datensatz_v12[,i])) } ## [1] 0.559322 ## [1] 9779.661 ## [1] 3.474576 ## [1] 13.88136 #...für den ersten Teildatensatz der zweiten Variante for (i in 1:4) { print(mean(datensatz_v21[,i])) } ## [1] 0.5862069 ## [1] 6950 ## [1] 3.948276 ## [1] 14.03448 #...für den zweiten Teildatensatz der zweiten Variante for (i in 1:4) { print(mean(datensatz_v22[,i])) } ## [1] 0.575 ## [1] 12050 ## [1] 4 ## [1] 13.8 References "],
["wochenplan-06.html", "6 Wochenplan 06 6.1 Lernziele WP06 6.2 Aufgaben WP06 6.3 Besprechung Skript “standardfehler.R”", " 6 Wochenplan 06 …zur Einheit vom 22. &amp; 29.10.2020, aktives Rezipieren von R-Code und Repetition (Beispiel Inferenzstatistik) 6.1 Lernziele WP06 Über die vergangenen Wochen haben wir zahlreiche grundlegende Aspekte der Arbeit mit R kennengelernt. Als Vorbereitung im Rahmen des Wochenplans 06 sollen Sie diese Inhalte noch einmal Revue passieren zu lassen. Im Rahmen dieser Repetition werden weiter zwei neue Dinge vermittelt werden: Auf der einen Seite erfolgt die Schulung aktiver Rezeptionsfertigkeiten (sprich: R-Code lesen und verstehen lernen). Auch dies ist eine wichtige Arbeitstechnik in R. Auf der anderen Seite sollen Sie mit dem rezipierten Code das Prinzip der Inferenzstatistik (und der Stichprobenverteilung sowie des Standardfehlers) in R veranschaulicht bekommen. Konkret lassen sich folgende Seminarziele festhalten: Sie können von einer anderen Person geschriebenen R-Code entziffern und mit Kommentaren versehen. Sie entwickeln dabei ein Gefühl für unterschiedliche Arten, Code lesbar zu gestalten. Sie wissen, wie selbstgeschriebene Funktionen in R aussehen und können diese Schritt für Schritt interpretieren. Sie entwickeln Ihr Verständnis davon weiter, wie in R Grafiken genutzt und angepasst werden, um statistische Inhalte zu visualisieren. Sie nutzen R um im gegebenen Beispiel über Konzepte der Inferenzstatistik (wie den Standardfehler) nachzudenken. 6.2 Aufgaben WP06 Öffnen Sie die Datei “standardfehler.R” in RStudio (auf OLAT verfügbar). Gehen Sie den Code Zeile für Zeile durch und versuchen Sie im Detail (!) zu verstehen, was hier vor sich geht. Halten Sie Ihre Einsichten als Kommentare fest. Auch allfällige Fragen und Unklarheiten können Sie einfach als Kommentare notieren. Fügen Sie anschliessend die von Ihnen gesammelten Fragen und die dazugehörigen Codezeilen in ein Markdown-Dokument ein. Die Codezeilen sollen als R Chunk im Markdown aufgeführt sein, aber mit der Ergänzung eval = FALSE versehen. Im Fliesstext des Markdowns können Sie Ihre Frage erläutern. Führen Sie den Code Schritt für Schritt aus. Konzentrieren Sie sich dann auf den letzten Punkt (“# Gemeinsame Darstellung …”). Was hat dieser Code mit dem Konzept des Standardfehlers / des Stichprobenfehlers zu tun? Spielen Sie ein wenig mit den verschiedenen Parametern - welche Zusammenhänge, die für den Standardfehler wesentlich sind, lassen sich erkennen? Sammeln Sie Punkte, die Ihnen aus den bisherigen Seminareinheiten unklar geblieben sind. Welche Aspekte würden Sie gerne noch einmal wiederholen/erläutert bekommen? Sammeln Sie diese Aspekte ebenfalls im Markdown auf dieselbe Art und Weise wie bei Aufgabe 2. 6.3 Besprechung Skript “standardfehler.R” Der Code “standardfehler.R” ist ein Skript zur Simulation des Standardfehlers und soll helfen, das Prinzip der Inferenzstatistik, den Standardfehler sowie die damit zusammenhängenden Ebenen von Grundgesamtheit, einzelner Stichprobe und Stichprobenverteilung zu verstehen und zu veranschaulichen (vgl. Diaz-Bone 2019, 145f). 6.3.1 Ebene der Grundgesamtheit Unsere Grundgesamtheit bilden 1000 Ausprägungen einer beliebigen Variable, die hier nun gleichverteilt ist und sich zwischen 0 und 20 bewegt. Die Verteilung dieser Variable können wir dann in einem Histogramm darstellen. variable_population &lt;- runif(10000, min = 0, max = 20) mean(variable_population) ## [1] 9.868251 sd(variable_population) ## [1] 5.779099 hist(variable_population, breaks = 30, col=&quot;orange&quot;, main=&quot;Unsere Grundgesamtheit&quot;, xlim = c(0,20) ) 6.3.2 Ebene der einzelnen Stichprobe Als nächster Schritt wird nun eine Stichprobe gezogen mit der Funktion sample(), in der zufällig 100 Fälle der Grundgesamtheit landen. Von dieser Stichprobe können wir dann den Mittelwert berechnen und diesen Wert in das Histogramm der Grundgesamtheit einfügen. Der Mittelwert der Stichprobe unterscheidet sich natürlich minimal vom Mittelwert der Grundgesamt. sample1 &lt;- sample(variable_population, 100) mean(sample1) ## [1] 11.20421 hist(variable_population, breaks = 30, col=&quot;orange&quot;, main=&quot;Unsere Grundgesamtheit&quot;, xlim = c(0,20) ) points(rep(mean(sample1), 2), c(0, 100), type = &quot;l&quot;, col = &quot;black&quot;, lwd = 8) #Hier wird nun auch noch der Mittelwert der Grundgesamtheit eingefügt #...aber mittels der Funktion &#39;abline()&#39; abline(v=mean(sample1), col = &quot;red&quot;, lwd = 3) Da wir nun mit einer fiktiven Grundgesamtheit und in einem Modell arbeiten können wir immer wieder neue Stichproben ziehen. Dies könnten wir realisieren, indem wir den Codechunk von oben immer wieder repetieren. Oder als “elegantere” Variante: Wir schreiben uns den Code für eine for-Schleife, die uns eine beliebige Anzahl Stichproben zieht (i), den Mittelwert der Stichprobe berechnet und diesen Mittelwert in das Histogramm der Grundgesamtheit einzeichnet. Der folgende Code ruft nun nochmals das Histogramm der Grundgesamtheit auf. Anschliessend zieht die Schleife 100 Stichproben, berechnet deren Mittelwerte und fügt diese als Linie mit verschiedenen Farben in die Grafik ein. hist(variable_population, breaks = 30, col = &quot;orange&quot;, main=&quot;Unsere Grundgesamtheit&quot;, xlim = c(0,20) ) for (i in 1:100) { sample1 &lt;- sample(variable_population, 100) points(rep(mean(sample1), 2), c(0, 200), type = &quot;l&quot;, col = i, lwd = 1) } Wir sehen dass mit jeder gezogener Stichprobe der Mittelwert etwas abweicht von demjenigen Wert in der Grundgesamtheit. Als nächster Schritt wird nun eine Funktion geschrieben, mit der wir eine beliebige Anzahl Stichproben mit einer beliebigen Grösse ziehen können. Die Grundstruktur von Funktionen entspricht folgender Form (Manderscheid 2017, 240f): eigene.funktion &lt;- function(argumente) { anweisung } Im Funktionskopf innerhalb der runden Klammern, die auf function folgen, werden die formalen Argumente benannt und durch Kommas voneinander getrennt. Damit wird festgelegt, welche Eingabeinformationen die Funktion benötigt. In unserem Beispiel sind dies die (selbstgewählten) Begriffe x (Objekt, von dem die Stichprobe gezogen werden soll), n (Grösse der Stichprobe) und trials (Anzahl der zu ziehenden Stichproben). Alle im Funktionskopf enthaltenen Argumente müssen im Funktionsrumpf, der in geschweiften Klammern {} folgt, als Objekte definiert werden. Der Funktionsrumpf besteht aus einer Reihe von Befehlen. Die in unserem Code definierte Funktion definiert zuerst ein Sample von der Grösse n aus dem Objekt x. Anschliessend hängt eine Schleife die Anzahl trials-1 weitere Sample an das bereits gezogene Sample dran. Die einzelnen Schritte, die im Funktionsrumpf festgelegt werden, erscheinen nicht in der Konsole. Nur das Ergebnis der letzten Funktion im Rumpf erscheint abschliessend als Rückgabewert. Da in unserem Beispiel kein Ergebnis erzeugt wurde stellt die Funktion return() sicher, dass das Resultat der Funktion (das Objekt variable sample) ausgegeben wird. Im letzten Codestück wird dann vom unsere Grundgesamtheit (x) eine 30er Stichprobe gezogen (n), und zwar 200mal (trials). Das Objekt stichproben_200 ist also eine Matrix mit 30 Zeilen und 200 Spalten. meine_samples &lt;- function(x, n, trials) { variable_sample &lt;- sample(x, n) for (i in 1:(trials - 1)){ variable_sample &lt;- cbind(variable_sample, sample(x, n)) } return(variable_sample) } stichproben_200 &lt;- meine_samples(variable_population, 30, 200) 6.3.3 Ebene der Stichprobenverteilung Bereits in den vorhergehenden beiden Schritten (Stichprobe als Schleife und Stichprobe als Funktion) ging es nicht mehr nur um eine einzelne Stichprobe, sondern um verschiedene Stichproben und deren jeweilige Mittelwerte. Wir gehen also über zu einer Stichprobenverteilung. Im folgenden Code werden zuerst die Stichprobenkennwerte berechnet (d.h. die Mittelwerte der 200 gezogenen Stichproben) und dann als Histogramm dargestellt: Das Histogramm entspricht der Stichprobenverteilung der Stichprobenkennwerte. stichproben_200 &lt;- meine_samples(variable_population, 30, 200) mittelwerte &lt;- apply(stichproben_200, 2, mean) hist(mittelwerte, breaks = 10, col = &quot;blue&quot;, main = &quot;Unsere Stichprobenmittelwerte&quot;, xlim=c(3,17) ) Hierbei wird nun ersichtlich, dass diese Stichprobenverteilung der Normalverteilung folgt (ab einem Stichprobenumfang von 30). Das heisst eben auch, dass je stärker ein Stichprobenmittelwert vom Mittelwert der Grundgesamtheit abweicht, desto unwahrscheinlicher ist dieser Wert. Oder umgekehrt: Tritt ein sehr stark abweichender Wert auf ist dies mit hoher Wahrscheinlichkeit nicht zufällig. In den folgenden Codezeilen wird nun die Stichprobenverteilung in das Histogramm der Grundgesamtheit eingefügt. Die Idee hierbei ist, dass die Stichprobengrösse dank der eigenen Funktion variiert werden kann. Daraus wird ersichtlich, dass je grösser der Stichprobenumfang ist, desto schmaler wird die Stichprobenverteilung (das blaue Histogramm), das heisst: desto kleiner wird der Standardfehler. hist(variable_population, breaks = 30, col = &quot;orange&quot;, main = &quot;Grundgesamtheit mit Stichprobenverteilung&quot;, xlim = c(0, 20), ) mittelwerte_samples &lt;- apply(meine_samples(variable_population, 30, 1000), 2, mean) hist(mittelwerte_samples, breaks = 10, col = &quot;blue&quot;, add = T) Damit können wir uns nochmals die expliziten Parameter der Stichprobenverteilung vergegenwärtigen: Auf der einen Seite haben wir das arithmetische Mittel der Stichprobenmittelwerte, das heisst: \\(\\mu\\frac{ }{ x }\\) Dieser Wert entspricht (annährend) dem arithmetischen Mittelwert des metrischen Merkmals in der Grundgesamtheit: \\(\\mu\\frac{ }{ x } = \\mu\\) mean(mittelwerte_samples) ## [1] 9.900318 mean(variable_population) ## [1] 9.868251 Auf der anderen Seite finden wir die Standardabweichung der Stichprobenverteilung, der Standardfehler (oder Stichprobenfehler): \\(\\sigma\\frac{ }{ x } = \\frac{ \\sigma }{ \\sqrt{ n } }\\) Dieser Wert gibt das Ausmass der Streuung der Stichprobenmittelwerte um den Mittelwert an. sd(variable_population)/sqrt(30) ## [1] 1.055114 …und entspricht natürlich einfach der Standardabweichung unserer Stichprobenverteilung: sd(mittelwerte_samples) ## [1] 1.070461 Die Stichprobenmittelwerte streuen umso geringer um den Mittelwert der Stichprobenverteilung (das heisst um den Wert in der Grundgesamtheit), je grösser der Umfang der Stichprobe ist. Das heisst die Stichprobenwerte werden immer “genauer”, da deren zufällige Abweichung verkleinert wird. hist(variable_population, breaks = 30, col = &quot;orange&quot;, main = &quot;Grundgesamtheit mit Stichprobenverteilung&quot;, xlim = c(0, 20), ) mittelwerte_samples &lt;- apply(meine_samples(variable_population, 120, 1000), 2, mean) mittelwerte_samples &lt;- mittelwerte_samples hist(mittelwerte_samples, breaks = 10, col = &quot;blue&quot;, add = T) sd(mittelwerte_samples) ## [1] 0.5074497 #Der Standardfehler wurde halbiert, da die Stichprobengrösse vervierfacht wurde. References "],
["wochenplan-07.html", "7 Wochenplan 07 7.1 Lernziele WP07 7.2 Aufgaben WP07", " 7 Wochenplan 07 …zu den Einheiten vom 29.10. und 12.11.2020, Einführung in die Arbeit mit Daten 7.1 Lernziele WP07 In der zweiten Hälfte des Semester möchten wir das statistische Arbeiten mit R anhand des Umgangs mit “echten” Daten kennenlernen. Dazu nutzen wir Daten aus dem European Social Survey (ESS).6 Am Beginn der Arbeit mit einem Datensatz steht das Kennenlernen der Daten: Es gilt die Variablen und ihre Ausprägungen zu verstehen, erste Plausibilitätsprüfungen durchzuführen und Daten für die spätere Analyse aufzubereiten. Diese Techniken sollen Sie im Rahmen des siebten Wochenplans kennenlernen und so die Grundlage schaffen, in den folgenden Einheiten statistische Analysen mit dem Datensatz vorzunehmen. Konkret lassen sich folgende Lernziele festhalten: Sie können einen CSV-Datensatz in R laden und kennen die Vor- und Nachteile dieses Dateiformats. Sie können einen geladen Datensatz anhand verschiedener Funktionen und in unterschiedlichem Detailliertheitsgrad beschreiben. Sie kennen erste Techniken zur Plausibilitätsprüfung von Datensätzen. Sie kennen erste Techniken zur Datenaufbereitung. Sie verstehen, wie und wozu man zufällige Teildatensätze erstellt. 7.2 Aufgaben WP07 Laden Sie sich den Datensatz “ESS1-8e01.csv” und ESS-Fragebogen (das Codebook) herunter (via OLAT) und speichern Sie die Dateien in Ihrem Arbeitsverzeichnis. Laden Sie anschliessend den Datensatz in RStudio über die Funktion read.csv(). Betrachten Sie den Datensatz über die Funktion View() oder indem Sie auf Ihr erstelltes Objekt im Tab “Environment” klicken. Vergleichen Sie für einzelne Variablen die Informationen im Datensatz mit den entsprechenden Fragebogenfragen! setwd(&quot;C:/IhrDateiPfad/R-Seminar_HS20/IhrArbeitsordner&quot;) daten_ess &lt;- read.csv(file = &quot;Data/ESS1-8e01.csv&quot;) View(daten_ess) Als erster Schritt gilt es Ihr Arbeitsverzeichnis zu definieren, das heisst der Ort, an dem auch Ihre Daten abgelegt sind (ggfs. sind diese noch in einem weiteren Unterordner). In diesem Arbeitsverzeichnis würden auch allfällige weitere Daten liegen bzw. später automatisch von R abgespeichert. Öffnen Sie nun die Datei “ESS1-8e01.csv” in einem Tabellenkalkulations- oder einem Textbearbeitungsprogramm (z.B. Microsoft Excel, dem Windows-Editor oder TextEdit auf Mac). Vergleichen Sie die beiden Varianten (Ihr Dataframe in RStudio und die im Textbearbeitungsprogramm geöffnete Datei), um das Dateiformat “CSV” besser zu verstehen. Ersetzen Sie anschliessend im Tabellenkalkulations- oder Textbearbeitungsprogramm die Kommas in der “ESS1-8e01.csv” Datei mit Strichpunkten und speichern Sie diese neue Version ab. Gehen Sie zurück zu RStudio und versuchen Sie, die neue Version des Datensatzes zu laden. Welchen Paramter müssen Sie anpassen, damit das funktioniert? Was denken Sie könnten Vorteile des CSV-Formats sein? Welche möglichen Nachteile im Vergleich zu anderen Dateiformaten (Excel, SPSS …) sind denkbar? daten_ess_nv &lt;- read.csv(file = &quot;Data/ESS1-8e01_NeueVersion.csv&quot;, sep = &quot;;&quot;) View(daten_ess_nv) Indem die Kommas mit Strichpunkten ersetzt werden ändert im CSV-Datensatz das Zeichen, mit dem die Spalten bzw. Variablen getrennt werden (die Fälle werden über den Zeilenumbruch definiert). Das geänderte Zeichen kann dann über das Argument sep in der read.csv() Funktion genauer definiert werden um die geänderte Datei ebenfalls korrekt einzulesen. Das CSV-Dateiformat hat vor allem zwei Vorteile: Auf der einen Seite – und wie in der Teilaufgabe oben deutlich wurde – können über beliebige Texteditoren direkt in diese Daten eingegriffen werden, da im Format lediglich Rohdaten in Textform abgespeichert sind. Bei anderen Dateiformaten für Datensätze wie etwa .sav (von der Statistiksoftware SPSS) ist dies nicht so einfach möglich. Auf der anderen Seite ist das CSV-Dateiformat nicht nur transparent, sondern beansprucht auch nur sehr wenig Speicherplatz. Grosse Datensätze können so schnell von RStudio gelesen und bereitgestellt werden. Dem gegenüber steht der Nachteil, dass das Dateiformat keine Möglichkeiten bietet, Informationen auf komplexere Art und Weise abzuspeichern (z.Bsp. Labels für Variablenausprägungen oder ähnliches). CSV bleibt so ein Format für “Rohdaten”. Versuchen Sie mit Funktionen, die Sie bisher kennengelernt haben, den geladenen Datensatz kurz zu beschreiben. Unser Datensatz umfasst.. dim(daten_ess)[1] ## [1] 1525 Fälle mit jeweils… dim(daten_ess)[2] ## [1] 21 Variablen. Die Variablen heissen etwa: sort(names(daten_ess))[1:5] ## [1] &quot;agea&quot; &quot;chldhm&quot; &quot;cntry&quot; &quot;edctn&quot; &quot;edulvla&quot; usw. Aktuell entsprechen die Werte der Variablen in fast allen Fällen numerische Ausprägungen (int bei ganzen Zahlen, num bei Zahlen mit Kommastellen). Lediglich die Variable cntry enthält Buchstaben, oder eben Character-Daten. Dies wird über die folgende Funktion ersichtlich: str(daten_ess) ## &#39;data.frame&#39;: 1525 obs. of 21 variables: ## $ x : int 1 2 3 4 5 6 7 8 9 10 ... ## $ cntry : chr &quot;CH&quot; &quot;CH&quot; &quot;CH&quot; &quot;CH&quot; ... ## $ essround: int 8 8 8 8 8 8 8 8 8 8 ... ## $ idno : int 1 3 5 6 9 14 21 22 24 27 ... ## $ pspwght : num 0.9 1.1 1.02 1.09 1.03 ... ## $ pweight : num 0.465 0.465 0.465 0.465 0.465 ... ## $ polintr : int 2 2 2 3 2 2 2 3 2 1 ... ## $ happy : int 8 6 8 8 8 5 9 3 8 10 ... ## $ gndr : int 1 2 1 2 1 1 1 2 2 2 ... ## $ yrbrn : int 1960 1953 1987 1949 1963 1948 1961 1955 1980 1941 ... ## $ agea : int 56 63 29 67 53 68 55 61 36 75 ... ## $ chldhm : int 2 2 2 2 2 2 2 2 2 2 ... ## $ edulvla : int 3 3 5 2 3 3 3 2 5 4 ... ## $ eisced : int 3 3 7 2 3 3 3 2 6 5 ... ## $ eduyrs : int 9 12 18 9 10 9 9 8 13 16 ... ## $ pdwrk : int 1 1 1 0 0 0 1 0 1 0 ... ## $ edctn : int 0 0 0 0 0 0 0 0 0 0 ... ## $ wkhtot : int 60 20 50 37 44 46 45 44 24 55 ... ## $ isco08 : int 1200 5419 2210 5223 7112 4220 8300 8100 5223 1120 ... ## $ uemp3m : int 1 1 2 2 1 1 2 2 2 1 ... ## $ hinctnta: int 10 1 10 1 77 2 5 77 4 5 ... Auch eine Variable wie gndr ist daher eine Zahl in dem Datensatz und wird aktuell von RStudio als eine numerische Variable behandelt. summary(daten_ess) Über die summary() wir dann nochmals deutlich, dass hier tatsächlich Rohdaten eingelesen wurden: Auch fehlende Werte gelten aktuell noch als gültige Zahlen (…jemand wäre also im Jahr 7777 geboren). summary(daten_ess$yrbrn) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1922 1954 1968 1991 1984 7777 In den Variablen zum Alter und den Bildungsabschlüssen der befragten Personen haben sich einige Fehler eingeschlichen. Was sind die Fehler? Und welche Fälle betrifft das (Fallnummer)? Der oben beschriebene Fall ist nicht ein Fehler im Datensatz, sondern vielmehr ein fehlender Wert, was das Codebuch ausweist. Fehlende Werte können wie folgt definiert werden: daten_ess$yrbrn[daten_ess$yrbrn==7777 | daten_ess$yrbrn==8888 | daten_ess$yrbrn==9999] &lt;- NA daten_ess$agea[daten_ess$agea==999] &lt;- NA #Dieses Definieren von fehlende Werten kann auch für die drei Variablen zu Bildungsabschluessen auf ähnliche Weise erfolgen: daten_ess$edulvla[daten_ess$edulvla&gt;5] &lt;- NA daten_ess$eisced[daten_ess$eisced&gt;7] &lt;- NA daten_ess$eduyrs[daten_ess$eduyrs&gt;76] &lt;- NA Anschliessend können erste Plausibilitätstest bei den verschiedenen Variablen durchgeführt werden. Hierbei unterscheiden wir drei verschieden Stufen: Bei der ersten Stufe betrachten wir lediglich die Zahlen selber, wobei uns vor allem Häufigkeitsverteilungen und Extremwerten interessiert: summary(daten_ess$yrbrn) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1922 1954 1968 1968 1984 2001 6 summary(daten_ess$agea) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 14.00 32.00 48.00 48.01 62.00 320.00 6 summary(daten_ess$eduyrs) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00 9.00 10.00 11.29 13.00 26.00 3 table(daten_ess$edulvla) ## ## 1 2 3 4 5 ## 51 265 665 45 491 table(daten_ess$eisced) ## ## 0 1 2 3 4 5 6 7 ## 1 50 265 543 122 226 114 197 Die lässt uns bereits einen ersten Fehler entdecken, nämlich das eine Person angeblich 320 Jahre alt sein soll, und zwar folgener Fall: which(daten_ess$agea==320) ## [1] 132 Auf der zweiten Stufe überprüfen wir nun, ob die Daten auch wirklich Sinn machen, das heisst im Rahmen der Codierungen gemäss Fragebogen. Ein Vergleich mit dem Codebuch zeigt dabei, dass erst Personen ab dem 15.Lebensjahr befragt wurden. Folgender Fall macht also keinen Sinn: which(daten_ess$agea&lt;15) ## [1] 352 Hingegen ist die bei der Variable eisced auftauchende Ausprägung 0 ist durchaus zulässig und entspricht “Not possible to harmonise into ES-ISCED”. Auf der dritten Stufe vergleichen wir dann mittels Kreuztabellierungen und logischen Überlegungen jeweils (mindestens) zwei Variablen miteinander, um so nochmals Fehler zu entdecken. Zuerst können wir dazu eien zusätzliche Variable als Fehlerindikator für die Angaben zum Alter berechnen. Diese zusätzliche Variable nutzen wir dann als logische Bedingung, um drei fehlerhafte Fälle zu entdecken: daten_ess$agea_l &lt;- daten_ess$agea + daten_ess$yrbrn daten_ess$agea_l[daten_ess$agea_l!=2016] ## [1] 2017 2017 2017 2017 2017 2017 2017 2017 NA 2017 2304 2017 NA 2017 2017 ## [16] 2017 NA 2017 2017 2017 1990 2017 2017 2017 2017 2017 2017 2017 2017 2017 ## [31] 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 NA ## [46] 2017 2017 2017 NA NA 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 ## [61] 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 ## [76] 2017 2017 2017 2017 1962 2017 2017 2017 2017 2017 2017 2017 2017 2017 which(daten_ess$agea_l!=2016 &amp; daten_ess$agea_l!=2017) ## [1] 132 352 1414 #Identifikation der Fälle daten_ess[daten_ess$agea_l!=2016 &amp; daten_ess$agea_l!=2017 &amp; !is.na(daten_ess$agea_l),c(&quot;agea&quot;, &quot;yrbrn&quot;)] ## agea yrbrn ## 132 320 1984 ## 352 14 1976 ## 1414 23 1939 Bei den Angaben zum Bildungsniveau wieder können wir stärker Kreuztabellen nutzen, um zwei Fehler zu entdecken: table(daten_ess$edulvla, daten_ess$eisced) ## ## 0 1 2 3 4 5 6 7 ## 1 0 50 0 0 0 1 0 0 ## 2 0 0 265 0 0 0 0 0 ## 3 0 0 0 543 122 0 0 0 ## 4 0 0 0 0 0 45 0 0 ## 5 0 0 0 0 0 180 114 197 table(daten_ess$eduyrs, daten_ess$edulvla) ## ## 1 2 3 4 5 ## 0 2 1 0 0 0 ## 1 1 0 1 0 0 ## 2 1 2 0 0 0 ## 3 1 0 0 0 0 ## 4 4 1 0 0 0 ## 5 2 4 0 0 0 ## 6 12 5 3 0 1 ## 7 5 1 3 0 0 ## 8 5 44 52 2 9 ## 9 13 125 302 24 91 ## 10 1 34 71 6 20 ## 11 1 22 30 1 12 ## 12 3 19 82 1 62 ## 13 0 4 43 2 38 ## 14 0 0 39 2 31 ## 15 0 3 12 3 32 ## 16 0 0 13 3 37 ## 17 0 0 5 1 48 ## 18 0 0 3 0 52 ## 19 0 0 4 0 29 ## 20 0 0 0 0 13 ## 21 0 0 0 0 6 ## 22 0 0 0 0 5 ## 23 0 0 1 0 3 ## 25 0 0 0 0 1 ## 26 0 0 0 0 1 table(daten_ess$eduyrs, daten_ess$eisced) ## ## 0 1 2 3 4 5 6 7 ## 0 0 2 1 0 0 0 0 0 ## 1 0 1 0 1 0 0 0 0 ## 2 0 1 2 0 0 0 0 0 ## 3 0 1 0 0 0 0 0 0 ## 4 0 4 1 0 0 0 0 0 ## 5 0 2 4 0 0 0 0 0 ## 6 0 12 5 3 0 1 0 0 ## 7 0 5 1 3 0 0 0 0 ## 8 0 5 44 49 3 9 0 2 ## 9 0 13 125 287 15 90 12 13 ## 10 0 1 34 67 4 22 3 1 ## 11 0 1 22 20 10 10 2 1 ## 12 1 2 19 55 27 23 15 26 ## 13 0 0 4 23 20 21 10 9 ## 14 0 0 0 18 21 14 8 11 ## 15 0 0 3 5 7 11 15 9 ## 16 0 0 0 6 7 11 15 14 ## 17 0 0 0 1 4 6 15 28 ## 18 0 0 0 1 2 6 13 33 ## 19 0 0 0 2 2 1 4 24 ## 20 0 0 0 0 0 1 1 11 ## 21 0 0 0 0 0 0 0 6 ## 22 0 0 0 0 0 0 1 4 ## 23 0 0 0 1 0 0 0 3 ## 25 0 0 0 0 0 0 0 1 ## 26 0 0 0 0 0 0 0 1 #Identifikation der Fälle which(daten_ess$edulvla==1 &amp; daten_ess$eisced==5) ## [1] 443 which(daten_ess$edulvla==3 &amp; daten_ess$eisced==3 &amp; daten_ess$eduyrs&lt;6) ## [1] 1482 Im Umgang mit diesen fehlenden Werten haben wir drei Möglichkeiten. Wir können die benötigen Angaben von anderen Werten ableiten (1), zum Beispiel das Alter vom Geburtsjahr. daten_ess$agea[132] &lt;- 2016 - daten_ess$yrbrn[132] Weiter können wir uns einen Werte berechnen bzw. diesen schätzen (2): daten_ess$eduyrs[1482] &lt;- round(mean(daten_ess$eduyrs[daten_ess$edulvla==3], na.rm = T)) Die letzte Option ist dann noch die falschen Angaben als fehlende Werte zu definieren (3): daten_ess$edulvla[443] &lt;- NA Bonusaufgabe: In den folgenden Wochen möchten wir nicht mit dem ganzen Datensatz arbeiten, sondern lediglich mit einer zufälligen Auswahl von 200 Fällen. Versuchen Sie einen solchen Teildatensatz randomisiert zu erstellen und speichern Sie ihn mittels der Funktion write.csv() unter dem Namen “TD_ESS1-8e01.csv” ab. Um den zufälligen Datensatz mit 200 Fällen zu erstellen bieten sich mindestens zwei Möglichkeiten an: Die erste Möglichkeit nutzt die sample() Funktion und wählt zufällig 200 Zahlen einer Variable aus, die von 1 bis 1525 durchläuft. Eine solche Variable ist x im Datensatz – oder auch einfach die Reihenzahl. Die zufällig ausgewählten Zahlen dieser Variable können als Angabe für die Zeilen in die eckige Klammerfunktion eingesetzt werden. daten_ess_HS20 &lt;- daten_ess[sample(row.names(daten_ess),200),] #oder auch: daten_ess_HS20 &lt;- daten_ess[sample(daten_ess,200),] Die zweite Möglichkeit berechnet eine Zufallsvariable und sortiert den Datensatz dann nach der Grösse dieser Zufallsvariable. Anschliessend können dann die – jetzt völlig zufällig – ersten 200 Fälle ausgewählt werden. daten_ess$zv &lt;- rnorm(1525) daten_ess &lt;- daten_ess[order(daten_ess$zv),] daten_ess_HS20 &lt;- daten_ess[1:200,] Diverse Seminarteilnehmer*innen haben auch mit der Funktion sample_n() des Paketes “dplyr” gearbeitet:7 #install.packages(&quot;dplyr&quot;) library(dplyr) daten_ess_HS20 &lt;- sampl_n(daten_ess, 200) Dieser zufällige Datensatz kann wiederum als CSV-Datei abgespeichert werden: write.csv(daten_ess_HS20, file = &quot;Data/ESS1-8e01_HS20.csv&quot;) Der Datensatz sowie das Codebook wurden mittels des ESS Cumulative Data Wizard erstellt: https://www.europeansocialsurvey.org/downloadwizard/?loggedin.↩︎ Die Funktion sample_n() bzw. das Paket “dplyr” ist Teil des sogenannten Tidyverse, einer spezifischen Sammlung von R-Paketen für die Datenwissenschaft (“an opinionated collection (…) for data science”). Allen Paketen liegen eine gemeinsame Designphilosophie, Grammatik und Datenstruktur zugrunde, siehe auch hier. Das Tiydverse ist eine leistungsstarke Erweiterung von R, mit der diverse Prozesse verbessert werden.↩︎ "],
["wochenplan-08.html", "8 Wochenplan 08 8.1 Lernziele WP08 8.2 Aufgaben WP08", " 8 Wochenplan 08 …zu den Einheiten vom 12 &amp; 19.11.2020, Beginn der Datenanalyse 8.1 Lernziele WP08 Nachdem wir uns bereits einen ersten Überblick über den ESS-Datensatz verschafft haben, gehen wir in dieser Woche zur Datenanalyse in R über. Dieser Übergang ist ein fliessender: Die ersten Schritte zum Kennenlernen eines Datensatzes, die Plausibilitätsprüfung und die Suche nach Fehlern liefern häufig bereits erste Ergebnisse im Sinne einer explorativen Datenanalysen, die umgekehrt dann womöglich neue Schritte zur Aufbereitung des Datensatzes veranlassen. Der Arbeitsplan für die kommende Woche fokussiert die Rolle von Kreuztabellen in diesem Prozess, inklusive der Berechnung von Chi-Quadrat-Tests. Von hier geht es dann in den Folgewochen schrittweise weiter zu Berechnung von Regressionsanalysen. Wir können daher folgende Lernziele festhalten: Sie verstehen den Nutzen (und auch die Gefahren) der Funktion attach(). Sie können Kreuztabellen auf verschiedene Weisen darstellen und inhaltlich zutreffend interpretieren. Sie verstehen, wie man das Ergebnis von Funktionen als Objekt “abspeichert”, und wissen, wie man auf die Elemente solcher “Ergebnisobjekte” zugreift. Sie entwickeln ein vertieftes Verständnis davon, wie die Phasen der Datenaufbereitung, der Datenexploration und der Datenanalyse zusammenhängen. 8.2 Aufgaben WP08 Laden Sie den Datensatz in RStudio, der 200 zufällige Fälle aus dem ganzen ESS Datensatz enthält („ESS1-8e01_HS20.csv“).8 Erfassen Sie den hierfür benötigten Code nicht nur im Markdown, sondern erstellen Sie dazu zusätlich ein eigenes R-Skript (z.B. unter dem Namen”ess_import.R\"). Dieses Skript sollte automatisch das richtige Arbeitsverzeichnis definieren, den Datensatz laden und alle notwendigen Datenaufbereitungen vornehmen. Die Idee ist, dass dieses Skript über die kommenden Wochen umfangreicher wird, wenn wir Faktoren definieren, Variablenbezeichnungen ändern, neue Variablen berechnen etc. Folgende Befehlszeilen wurden in einem Skript namens “ess_import.R” in einem Unterordner des Arbeitsverzeichnisses abgespeichert (vgl. für die Angaben zu den fehlenden Werten auch Aufgabe 4 aus dem WP07): #Laden der Daten setwd(&quot;C:/Users/SchweglG/R_Daten/IhrArbeitsverzeichnis&quot;) daten_ess &lt;- read.csv(file = &quot;Data/ESS1-8e01_HS20.csv&quot;) #Definieren von fehlenden Werten daten_ess$yrbrn[daten_ess$yrbrn==7777 | daten_ess$yrbrn==8888 | daten_ess$yrbrn==9999] &lt;- NA daten_ess$agea[daten_ess$agea==999] &lt;- NA daten_ess$edulvla[daten_ess$edulvla&gt;5] &lt;- NA daten_ess$eisced[daten_ess$eisced&gt;7] &lt;- NA daten_ess$eduyrs[daten_ess$eduyrs&gt;76] &lt;- NA Das R-Skript kann nun laufend ergänzt werden, etwa mit der Definition von weiteren fehlenden Werte bei anderen Variablen oder mit der Erstellen von Faktoren (siehe nächster WP). Das hat den Vorteil, dass zu Beginn einer neuen Einheit (d.h. wenn Sie ein neues Markdown erstellen) jeweils nicht der ganze Code nochmals aufgeführt werden muss, sondern direkt dieses Skript ausgeführt werden kann. Das Ausführen eines Skriptes (oder auch einer sonstigen Textdatei) erfolgt über die Funktion source(). #Nochmals das Arbeitsverzeichnis definieren falls nötig: setwd(&quot;C:/Users/SchweglG/R_Daten/IhrArbeitsverzeichnis&quot;) #Aufrufen des R Skripts: source(file = &quot;Data/ess_import.R&quot;) Erläutern Sie in eigenen Worten die Idee der Funktion attach(). Nutzen Sie diese Funktion für die Aufgaben 3 und 4. Wenden Sie den detach()-Befehl an, bevor Sie zu Aufgabe 5 übergehen. Die attach()-Funktion erlaubt es uns, einen bestimmten Datensatz (d.h. ein Dataframe-Objekt) zu „aktivieren“ (Manderscheid 2017, 54).9 Dadurch können wir direkt auf diesen Datensatz zugreifen, ohne den Umweg des ‚$‘-Zeichens: #Am Beispiel der Variable zu Geschlecht: ##...vor dem attachen: table(daten_ess$gndr) ##die Funktion attach(daten_ess) ##...nach dem attachen: table(gndr) ## gndr ## male female ## 98 102 Ohne die attach()-Funktion würde die zweite Zeile des Code-Chunks oben zu einer Fehlermeldung führen. Mit der Funktion zu arbeiten ist vor allem dann sinnvoll, wenn die Arbeit mit nur einem Datensatz im Zentrum steht (ebd.) und wenn keine Änderungen mehr am Datensatz selber vorgenommen werden. Bei der Arbeit mit mehreren Datensatzsätzen könnte die Funktion leicht für Verwirrung sorgen (gerade auch dann wenn Ihre Codezeilen von jemand anderem interpretiert werden sollen). Und sollten Sie weiterhin Änderungen am Datensatz vornehmen kann die attach()-Funktion dazu führen, dass die von Ihnen gemachten Änderungen nicht sichtbar werden (beim Betrachten des Datensatzes) oder auch nicht in die weiteren Schritte/Berechnungen aufgenommen werden. Sobald also Änderungen vorgenommen werden gilt es auch den Datensatz wieder zu „detachen“. Erstellen Sie mittels der Funktion table() eine Kreuztabelle zwischen der Variable zu Geschlecht und derjenigen zum Interesse an Politik. Wie können Sie vorgehen, um eine Kreuztabelle mit Prozentwerten (relative statt absolute Häufigkeiten) zu erstellen, die ausserdem die Randsummen enthält? Suchen Sie eine Darstellung der Tabelle – d.h. der Anordnung der Variablen in Zeilen und Spalten sowie der Wahl der Randsummen –, die sinnvoll das Verhältnis von abhängiger und unabhängiger Variable wiedergibt. Was fällt Ihnen inhaltlich an der Tabelle auf? Wenn wir Tabellen (bzw. Kontingenztabellen) darstellen, so ist die Konvention dass “bei gerichteten Beziehungen die Ausprägungen der unabhängigen Variablen (X) den Spalten zugeordnet werden und die Ausprägungen der abhängigen Variablen (Y) den Reihen zugeordnet werden” (Diaz-Bone 2019, 70). Die Aufgabenstellung spricht zwar nicht von einer gerichteten Beziehung, aber gleichzeitig macht es Sinn, Geschlecht als unabhängige Variable anzusehen (und nicht dass das Interesse an Politik das Geschlecht einer Person bestimmt). Gemäss dieser Überlegung sollen dann auch die Randsummen ausgerichtet werden, da uns die Verteilungen des Politikinteresses je nach Geschlecht interessiert. Das heisst das pro Geschlecht jeweils auf 100% summiert werden soll. Wir betrachten als ersten Schritt auch noch die Variable polintr: table(polintr) ## polintr ## Not at all interested Hardly interested Quite interested ## 20 59 76 ## Very interested ## 45 #Ählich wie die Geschlechtervariabel scheint auch polintr ebenfalls keine fehlenden Werte zu enthalten. Ansonsten können diese ebenfalls noch definiert werden: daten_ess$polintr[daten_ess$polintr&gt;4] &lt;- NA #Dieser Befehl könnten wir dann ebenfalls in unser Import-Skript übernehmen. Anschliessend erstellen wir die benötige Tabelle. Hierbei lohnt es sich von “innen” nach “aussen” vorzugehen (vgl. auch Manderscheid 2017, 95): #als logischer Aufbau der Tabelle table(polintr, gndr) prop.table(table(polintr, gndr)) prop.table(table(polintr, gndr),2) addmargins(prop.table(table(polintr, gndr),2)) addmargins(prop.table(table(polintr, gndr),2),1) round(addmargins(prop.table(table(polintr, gndr),2),1),2) #die benötige Tabelle round(addmargins(prop.table(table(polintr, gndr),2),1),2) ## gndr ## polintr male female ## Not at all interested 0.07 0.13 ## Hardly interested 0.24 0.34 ## Quite interested 0.38 0.38 ## Very interested 0.31 0.15 ## Sum 1.00 1.00 Die Funktion CrossTable() ist Teil des Paketes “gmodels” und ermöglicht die flexible und detailreiche Arbeit mit Kreuztabellen. Versuchen Sie mittels dieser Funktion die Kreuztabelle aus Aufgabe 2 nachzubauen. #install.packages(&quot;gmodels&quot;) library(gmodels) ## Warning: package &#39;gmodels&#39; was built under R version 4.0.3 ?CrossTable ## starting httpd help server ... done Ein Blick in die Hilfeseite zeigt uns, dass wir vor allem diejenigen Argument anders definieren müssen, die wir nicht in unserer Tabelle haben wollen. Das heisst wir müssen angeben (FALSE), dass wir keine Spalten- und Tabellenprozente sowie keine Angaben zum Chi-Quadrat-Beiträgen haben möchten. Leider scheint aber kein Argument vorhanden zu sein, mit der wir die absoluten Zahlen entfernen und die Randverteilung auch noch gemäss der Tabelle oben darstellen könnten. CrossTable(polintr, gndr, digits = 2, prop.r = F, prop.t = F, prop.chisq = F, ) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Col Total | ## |-------------------------| ## ## ## Total Observations in Table: 200 ## ## ## | gndr ## polintr | male | female | Row Total | ## ----------------------|-----------|-----------|-----------| ## Not at all interested | 7 | 13 | 20 | ## | 0.07 | 0.13 | | ## ----------------------|-----------|-----------|-----------| ## Hardly interested | 24 | 35 | 59 | ## | 0.24 | 0.34 | | ## ----------------------|-----------|-----------|-----------| ## Quite interested | 37 | 39 | 76 | ## | 0.38 | 0.38 | | ## ----------------------|-----------|-----------|-----------| ## Very interested | 30 | 15 | 45 | ## | 0.31 | 0.15 | | ## ----------------------|-----------|-----------|-----------| ## Column Total | 98 | 102 | 200 | ## | 0.49 | 0.51 | | ## ----------------------|-----------|-----------|-----------| ## ## Anschliessend detachen wir unseren Datensatz wieder. detach(daten_ess) Mittels CrossTable() lassen sich nicht nur Tabellen erstellen, sondern auch Chi-Quadrat Tests durchführen. Speichern Sie dazu das ganz Ergebnis Ihrer Arbeit mit der Funktion in Aufgabe 4 als ein neues Objekt CT_Ergebnis. Greifen Sie anschliessend auf die darin enthaltenen Angaben zum Chi-Quadrat Test zu und interpretieren Sie diese -– sowohl deskriptiv- als auch inferenzstatistisch. Zuerst weisen wir alle Ergebnisse der Funktion einem neuen Objekt zu: CT_Ergebnis &lt;- CrossTable(daten_ess$polintr, daten_ess$gndr, chisq = T) Danach können wir auf die einzelnen Inhalte des Chi-Quadrat Tests zugreifen: str(CT_Ergebnis) #hier sehen wir, was alles in den Objekt enthalten ist. ## List of 5 ## $ t : &#39;table&#39; int [1:4, 1:2] 7 24 37 30 13 35 39 15 ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ x: chr [1:4] &quot;Not at all interested&quot; &quot;Hardly interested&quot; &quot;Quite interested&quot; &quot;Very interested&quot; ## .. ..$ y: chr [1:2] &quot;male&quot; &quot;female&quot; ## $ prop.row: &#39;table&#39; num [1:4, 1:2] 0.35 0.407 0.487 0.667 0.65 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ x: chr [1:4] &quot;Not at all interested&quot; &quot;Hardly interested&quot; &quot;Quite interested&quot; &quot;Very interested&quot; ## .. ..$ y: chr [1:2] &quot;male&quot; &quot;female&quot; ## $ prop.col: &#39;table&#39; num [1:4, 1:2] 0.0714 0.2449 0.3776 0.3061 0.1275 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ x: chr [1:4] &quot;Not at all interested&quot; &quot;Hardly interested&quot; &quot;Quite interested&quot; &quot;Very interested&quot; ## .. ..$ y: chr [1:2] &quot;male&quot; &quot;female&quot; ## $ prop.tbl: &#39;table&#39; num [1:4, 1:2] 0.035 0.12 0.185 0.15 0.065 0.175 0.195 0.075 ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ x: chr [1:4] &quot;Not at all interested&quot; &quot;Hardly interested&quot; &quot;Quite interested&quot; &quot;Very interested&quot; ## .. ..$ y: chr [1:2] &quot;male&quot; &quot;female&quot; ## $ chisq :List of 9 ## ..$ statistic: Named num 8.83 ## .. ..- attr(*, &quot;names&quot;)= chr &quot;X-squared&quot; ## ..$ parameter: Named int 3 ## .. ..- attr(*, &quot;names&quot;)= chr &quot;df&quot; ## ..$ p.value : num 0.0317 ## ..$ method : chr &quot;Pearson&#39;s Chi-squared test&quot; ## ..$ data.name: chr &quot;t&quot; ## ..$ observed : &#39;table&#39; int [1:4, 1:2] 7 24 37 30 13 35 39 15 ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ x: chr [1:4] &quot;Not at all interested&quot; &quot;Hardly interested&quot; &quot;Quite interested&quot; &quot;Very interested&quot; ## .. .. ..$ y: chr [1:2] &quot;male&quot; &quot;female&quot; ## ..$ expected : num [1:4, 1:2] 9.8 28.9 37.2 22.1 10.2 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ x: chr [1:4] &quot;Not at all interested&quot; &quot;Hardly interested&quot; &quot;Quite interested&quot; &quot;Very interested&quot; ## .. .. ..$ y: chr [1:2] &quot;male&quot; &quot;female&quot; ## ..$ residuals: &#39;table&#39; num [1:4, 1:2] -0.8944 -0.9132 -0.0393 1.693 0.8767 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ x: chr [1:4] &quot;Not at all interested&quot; &quot;Hardly interested&quot; &quot;Quite interested&quot; &quot;Very interested&quot; ## .. .. ..$ y: chr [1:2] &quot;male&quot; &quot;female&quot; ## ..$ stdres : &#39;table&#39; num [1:4, 1:2] -1.3202 -1.5229 -0.0699 2.6929 1.3202 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ x: chr [1:4] &quot;Not at all interested&quot; &quot;Hardly interested&quot; &quot;Quite interested&quot; &quot;Very interested&quot; ## .. .. ..$ y: chr [1:2] &quot;male&quot; &quot;female&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;htest&quot; CT_Ergebnis$chisq ## ## Pearson&#39;s Chi-squared test ## ## data: t ## X-squared = 8.827, df = 3, p-value = 0.03168 Anschliessend greifen wir auf die einzelnen Elemente zurück, zuerst die deskriptivstatistischen Angaben: CT_Ergebnis$chisq$method ## [1] &quot;Pearson&#39;s Chi-squared test&quot; CT_Ergebnis$chisq$statistic ## X-squared ## 8.82701 CT_Ergebnis$chisq$parameter ## df ## 3 CT_Ergebnis$chisq$data.name ## [1] &quot;t&quot; Wir erhalten einen Chi-Quadrat Wert gemäss Pearson von 8.82701. Das heisst dass ein Zusammenhang zwischen den beiden Variablen Geschlecht und Politikinteresse besteht. Allerdings ist die Grössenordnung dieses Wertes nicht nur durch eine Stärke des Zusammenhangs bestimmt, sondern auch durch das Tabellenformat (Diaz-Bone 2019, 86). Dieses Tabellenformat wird über die Freiheitsgrade (df) deutlich: (i-1) x (j-1). Möchte man hingegen verschiedene Zusammenhänge und deren Stärke miteinander vergleichen, so kann man auf Cramer’s V zurückgreifen (etwa über die Funktion cramersV() im Paket „lsr“). Der letzte Wert, der über das Unterobjekt data.name aufgerufen wird, gibt uns Angaben zu den verwendeten Daten. Diese Idee entstammt der eigentlichen Funktion für den einen Chi-Quadrat Test, chisq.test(), und würde normalerweise die Namen der Variablen angeben, die für den Test verwendeten wurden. Da hier aber der Chi-Quadrat Test im Rahmen einer anderen Funktion erfolgt, wird einfach auf „t“ verwiesen, was der Tabelle entspricht, welche die CrossTable() Funktion generiert generiert. CT_Ergebnis$chisq$p.value ## [1] 0.03168154 Das inferenzstatistische Ergebnis unseres Chi-Quadrat Tests ist ein p-Wert von 0.03168154. Unser berechnete Prüfwert Chi-Quadrat fällt auf einem 5% Signifikanzniveau in den Ablehnungsbereich bzw. wir können auf einem 95% Signifikanzniveau die Alternativhypothese annehmen, dass auch in der Grundgesamtheit ein Zusammenhang zwischen den beiden Variablen besteht (vgl. auch Manderscheid 2017, 165f). Der p-Wert dreht hierbei die klassische Logik etwas um: Der Test bestimmt einen Signifikanzwert (oder eben eine Überschreitungswahrscheinlichkeit oder einen propability-value). Man kann sich das umgekehrt vorstellen: Ein p-Wert von 0.05 würde dann unserem Signifikanzniveau von 5% entsprechen – oder eben mit 95% Wahrscheinlichkeit würde dann die Nullhypothese nicht zutreffen. Es geht aber hier nicht mehr darum, dass Signifikanzniveau festzulegen und dann einen Testentscheid zu treffen, sondern den Testentscheid zu treffen (Nullhypothese triff zu) und dann zu schauen, wir wahrscheinlich der Testwert dann wäre (die konventionellen Grenzen für das Signifikanzniveau kann mal allerdings weiter als Referenzpunkte verwenden). Der p-Wert gibt daher an, wie wahrscheinlich das Ergebnis ist (d.h. der Chi-Quadrat-Wert) unter der Annahme der Nullhypothese. Das heisst, dass es in unserem Beispiel rund eine 3% Chance gibt, dass wir den Chi-Quadrat Wert von 8.3 zufällig erhalten, während die zwei kategoriale Variablen in der Grundgesamtheit statistisch voneinander unabhängig sind. Damit beschreibt der Wert auch gleich dem Alpha- bzw. Fehler 1.Art (Diaz-Bone 2019, 172): Die Chance, dass wir die Nullhypothese aufgrund unseres Test verwerfen, obwohl sie eigentlich die Grundgesamtheit richtig beschreiben würde, ist also bei rund 3%. References "],
["wochenplan-09.html", "9 Wochenplan 09 9.1 Lernziele WP09 9.2 Aufgaben WP09", " 9 Wochenplan 09 …zu den Einheiten vom 19. &amp; 26.11.2020, Einführung Faktoren 9.1 Lernziele WP09 Aufbauend auf unserer bisherigen Arbeit mit Kreuztabellen besteht das Lernziel für diese Woche darin, dass Sie sich mit einem für die tägliche Arbeit mit R wichtigen Datenformat vertraut machen: Faktoren. Faktoren sind eine geeignete Art, kategoriale – d.h. nominale und ordinale – Daten in R zu repräsentieren. Der Umgang mit dieser Art von Daten ist allerdings nicht ganz leicht und erfordert ein wenig Übung. Die folgenden konkreten Lernziele lassen sich für ein erstes Kennenlernen von Faktoren formulieren: Sie können dafür geeignete Variablen in Faktoren umwandeln. Sie verstehen den Unterschied zwischen geordneten und ungeordneten Faktoren und können beide in R erstellen. Sie verstehen die spezifische Zwei-Ebenen-Struktur von Faktoren und insbesondere die Rolle von “Levels”. Sie wissen, wie man die Levels eines Faktors definiert. Sie wissen, wie man Levels neu benennt, umsortiert, zusammenfasst und/oder nicht benötigte Levels entfernt. 9.2 Aufgaben WP09 Laden Sie Ihr Datenimport-Skript und fügen Sie einen Abschnitt zur Definition und Aufbereitung einzelner Variablen als Faktoren ein. Arbeiten Sie anschliessend in einem R Markdown weiter. Ein solcher Abschnitt wird am einfachsten über die Kommentarfunktion ausgewiesen: Das Datenimport-Skript soll also einen Abschnitt zum Dateneinlesen (#Daten einlesen), einen Abschnitt für die Definition der fehlende Werte bei metrischen Variablen (#Fehlende Werte definieren) und neu einen Abschnitt zur Definition der Faktoren enthalten (#Faktoren definieren). Gehen Sie unseren Übungsdatensatz konzentriert durch und überlegen Sie, welche Variablen als geordnete oder ungeordnete Faktoren gespeichert werden sollten. Folgenden Variablen sollten wir als ungeordnete Faktoren definieren: “gndr”, “chldhm”, “pdwrk”, “edctn” und “uemp3m”. Als geordnete Faktoren sollten wir folgende Variablen definieren: “polintr”, “edulvla” und “eisced”. Die restlichen Variablen sind entweder metrisch – wie etwa “agea” – oder wir belassen Sie bewusst als metrische Variable, obschon sie bei ganz genauer Betrachtung eigentlich ordinale Variablen wären (z.Bsp. “happy” oder “hinctnta”). Definieren Sie nun jede dieser Variablen als Faktor (die einzige Ausnahme soll die Variable “isco08” sein). Im Folgenden wird lediglich der Code zur Definition der Faktoren aufgeführt (der dann auc. Zuvor sollte man allerdings immer eine Tabelle zu einem Vektor aufrufen und überprüfen, welche Levels auch wirklich vorhanden sind. Mit diesem Code-Chunk definieren wir die ungeordneten Faktoren: #gndr daten_ess$gndr &lt;- factor(daten_ess$gndr) levels(daten_ess$gndr) &lt;- c(&quot;male&quot;, &quot;female&quot;) #chldhm daten_ess$chldhm[daten_ess$chldhm==9] &lt;- NA daten_ess$chldhm &lt;- factor(daten_ess$chldhm) levels(daten_ess$chldhm) &lt;- c(&quot;lives with children&quot;, &quot;Does not&quot;) #pdwrk daten_ess$pdwrk &lt;- factor(daten_ess$pdwrk) levels(daten_ess$pdwrk) &lt;- c(&quot;not marked&quot;, &quot;paid work (in the last 7 days)&quot;) #edctn daten_ess$edctn &lt;- factor(daten_ess$edctn) levels(daten_ess$edctn) &lt;- c(&quot;not marked&quot;, &quot;education (in the last 7 days)&quot;) #uemp3m daten_ess$uemp3m &lt;- factor(daten_ess$uemp3m) levels(daten_ess$uemp3m) &lt;- c(&quot;Has been unemployed&quot;, &quot;Has never been unemployed&quot;, &quot;Refusal&quot;, &quot;Don&#39;t know&quot;, &quot;No answer&quot;) Mit diesem Code-Chunk definieren wir die geordneten Faktoren: #polintr daten_ess$polintr &lt;- factor(daten_ess$polintr, ordered = T) levels(daten_ess$polintr) &lt;- c(&quot;Very interested&quot;, &quot;Quite interested&quot;, &quot;Hardly interested&quot;, &quot;Not at all interested&quot;) ##Änderung der Reihenfolge der Levels daten_ess$polintr &lt;- ordered(daten_ess$polintr, levels = c(&quot;Not at all interested&quot;, &quot;Hardly interested&quot;, &quot;Quite interested&quot;, &quot;Very interested&quot;)) #edulvla daten_ess$edulvla &lt;- factor(daten_ess$edulvla, ordered = T) levels(daten_ess$edulvla) &lt;- c(&quot;ISCED 0-1&quot;, &quot;ISCED 2&quot;, &quot;ISCED 3&quot;, &quot;ISCED 4&quot;, &quot;ISCED 5-6&quot;) #eisced daten_ess$eisced &lt;- factor(daten_ess$eisced, ordered = T) levels(daten_ess$eisced) &lt;- c(&quot;ES-ISCED I&quot;, &quot;ES-ISCED II&quot;, &quot;ES-ISCED IIIb&quot;, &quot;ES-ISCED IIIa&quot;, &quot;ES-ISCED IV&quot;, &quot;ES-ISCED V1&quot;, &quot;ES-ISCED V2&quot;) Überprüfen Sie mittels Kreuztabellen, ob Ihre Faktoren sinnvoll und fehlerfrei definiert sind. Hierzu können wir die einzelen Faktoren mit noch vorhandenen numerischen Ausprägungen “hinter” den Levels vergleichen, wie zum Beispiel bei derletzten, ungeordneten Variable “uemp3m”. table(daten_ess$uemp3m, as.numeric(daten_ess$uemp3m)) ## ## 1 2 ## Has been unemployed 38 0 ## Has never been unemployed 0 162 ## Refusal 0 0 ## Don&#39;t know 0 0 ## No answer 0 0 Inkludieren Sie die einwandfrei funktionierenden Lösungen zur Faktorendefinition in Ihr Datenimportskript. Testen Sie danach, ob das Skript funktioniert: Löschen Sie Ihre Environment (und alle Objekte), laden Sie mittels source() den Datensatz neu und rufen Sie dann – als Test – eine als Faktor definierte Variable auf. Eine einfache Variante, wie die gesamte Environment gelöscht werden kann, biete folgende Ergänzung der rm()-Funktion: rm(list=ls()) Nachdem Sie das Skript zum Datenimport (inkl. der Definition der Faktoren) ausgeführt haben können Sie anschliessend auf einen Faktor zugreifen – z.Bsp. über is.factor(): source(file = &quot;Data/ess_import.R&quot;) is.factor(daten_ess$eisced) ## [1] TRUE "],
["wochenplan-10.html", "10 Wochenplan 10 10.1 Lernziele WP10 10.2 Aufgaben WP10", " 10 Wochenplan 10 …zu den Einheiten vom 26.11. &amp; 03.12.2020, Einführung lineare Regression 10.1 Lernziele WP10 Nachdem wir unsere Daten aufbereitet, ersten Analysen mittels Kreuztabellen gemacht und die Faktoren definiert haben gehen wir in der kommenden Woche sukzessive zur Regressionsanalyse über. Dabei behalten wir die Arbeit mit unterschiedlichen Datenformaten (inklusive Faktoren) im Auge. Wir beginnen mit bivariaten Regressionsmodellen und machen uns Schritt für Schritt mit der Definition und Interpretation dieser Modelle in R vertraut. Konkret lassen sich für den 10. Wochenplan folgende Lernziele definieren: Sie sind mit der Funktion lm() in ihrer Grundstrukturen vertraut. Sie kennen die Notation, mit der in R Regressionsmodelle in Formeln dargestellt werden. Sie verstehen, wie in einem linearen Regressionsmodell sowohl metrische als auch kategoriale und binominale unabhängige Variablen einfliessen können. Sie haben einen ersten Eindruck von der Struktur des “Ergebnisobjekts”, das lm() erzeugt. Sie wissen, wie Sie den Modelloutput einer linearen Regression in R interpretieren müssen und die Güte eines Modells einschätzen sollten. 10.2 Aufgaben WP10 Laden Sie Ihr Datenimport-Skript. Arbeiten Sie anschliessend in einem R Markdown-Dokument weiter. setwd(&quot;C:/Users/SchweglG/R_Daten/4_HS20/R-Seminar_HS20/Einheit12&quot;) source(file = &quot;ess_import.R&quot;) Wichtig: Der Dateipfad, wo Ihr Skript liegt, muss nochmals angegeben werden (z.Bsp. über den setwd()-Befehl) und diese beiden Befehle müssen sich im selben Code-Chunk des R Markdown Dokument befinden. Formulieren Sie zwei inhaltliche Fragestellungen, die Sie anhand unserer ESS-Daten mit einem einfachen (bivariaten) Regressionsmodell beantworten können. Eine der beiden Fragestellungen soll eine kategoriale Variable als unabhängige Variable enthalten. Nutzen Sie je ein Streudiagramm, um erste Hinweis zu Ihren Fragestellungen zu erhalten. Die bivariate Regressionsanalyse berechnet die gerichtete (asymmetrische) Beziehung zwischen zwei metrischen Variablen, in der einfachsten Form einer abhängigen Variablen Y auf nur eine unabhängige Variable X. Dabei repräsentiert Y die Wirkung und X die Ursache (Diaz-Bone 2019, 96f). Im Rahmen einer solchen Regressionsanalyse können auch kategoriale Variablen als unabhängige Variablen hinzugezogen werden. Bei kategorialen Variablen mit mehreren Ausprägungen wird allerdings ein multiples Modell generiert, das für die jeweiligen Ausprägungen einmal eine Referenzkategorie und für den Rest eine Dummy-Variable erstellt. Hier werden nun sieben Beispiele für bivariate Regressionsmodelle und Forschungsfragen von Studierenden aufgelistet: Bsp 1: Inwiefern sind die Ausprägungen der Variable zum Einkommen des gesamten Haushalts auf die Anzahl der Ausbildungsjahre zurückzuführen bzw. lassen sich anhand von dieser Variable erklären? Wie gross ist der Einfluss von Ausbildungsjahren der befragten Person auf das Einkommen ihres gesamten Haushalts? (Diethelm) plot(daten_ess$eduyrs, daten_ess$hinctnta) plot(daten_ess$eduyrs[daten_ess$hinctnta&lt;60], daten_ess$hinctnta[daten_ess$hinctnta&lt;60]) Anhand des ersten Streudiagramms wird nicht wirklich klar, ob der Zusammenhang nur schlecht sichtbar ist, oder ob dieser nicht wirklich vorhanden ist. Deshalb werden im darauffolgenden Plot die Extremwerte ausgeschlossen. Bsp 2: Welchen Einfluss hat das Alter auf die Glückseligkeit? (Fäs) …oder auch: Welchen Einfluss hat das Alter (agea) auf die Zufriedenheit (happy)? (Ineichen) …oder auch: Wie hängt Glücklichsein vom Alter ab? (Balakrishnan) plot(daten_ess$agea, daten_ess$happy) Im Rahmen dieses Beispiel zeigt sich nun vor allem ein kaum vorhandenere Zusammenhang, der aber noch nicht die Linearitätsannahme verletzten würde. Bsp 3: Hat das Geschlecht einen Einfluss auf die Glücklichkeit? (Nguyen) plot(daten_ess$gndr, daten_ess$happy) Da wir die Variable “gndr” als Faktor definiert haben zeigt uns R nun die Verteilung der abhängigen Variable (“happy”) in zwei Bloxplots gemäss den Ausprägungen der unabhängigen Variable an. Wichitg: Die Variable “gndr” kann direkt im Modell integriert werden als Faktor, und muss nicht zuerst noch als numerische Variable definiert werden (etwa as.numeric()). Bsp 4: Wie hängt Glücklichsein (Y, die Wirkung) vom politischen Interesse (X, die Ursache) ab? (Schürmann) plot(daten_ess$polintr, daten_ess$happy) Auch in diesem Beispiel gibt uns R wieder verschiedenen Boxplots aus – und wir sehen, dass anscheinend ein Politikinteresse (oder eine Politikverdrossenheit) auf das Glücksein einwirkt. Bsp 5: Wie stark beeinflusst das Ausbildungsniveau einer Person deren Grad an Glücklichkeit? (Tomaschett) plot(daten_ess$edulvla, daten_ess$happy) Hier ist nun nicht nur ein Hinweis auf einen eher geringeren Zusammenhang drin, sondern die verschiedenen Boxplots zeigen auch auf, dass womöglich kein linearer Zusammenhang vorhanden ist. Bsp 6 &amp; 7: Die letzten beiden Beispiele sind keine linearen Regressionen. Ersteres hat nämlich eine binominale Variablen als abhängige Variable (und wäre somit eine logistische Regression), und letzteres eine kategoriale Variable als abhängige Variable (dies kann nicht über eine Regression gerechet werden). Welchen Einfluss hat das Bildungsniveau darauf, dass Kinder im Haushalt leben oder nicht? (Roth) plot(daten_ess$edulvla, daten_ess$chldhm) Ich möchte untersuchen, inwiefern die Variable “polintr” durch das Geschlecht beeinflusst wird. (Kurmann) plot(daten_ess$gndr, daten_ess$polintr) Formulieren und überprüfen Sie Ihre beiden Modelle mittels der Funktion lm(). Machen Sie sich dazu vorab mit dieser Funktion vertraut (Hilfeseite der Funktion, Online-Tutorials …). Erläutern Sie lm() kurz in eigenen Worten! ?lm Die Funktion lm() ist der Befehl in R zur Berechnung von linearen Modellen allgemein (Manderscheid 2017, 109f). Sie “übersetzt” die normale Regressionsgleichung \"y = b0 + b1*x + e\" zu “y ~ x” oder eben response ~ term. Mit der Formel können dann auch multiple, lineare Regressionen dargestellt werden, response ~ term + term, oder um Interaktionsterme ergänzt werden, response ~ term + term * term. Die weiteren Argumente (neben der eigentlichen Formel sowie der Angabe des Datensatzes) erlauben etwa die Daten zu unterteilen, Gewichtungen einzubauen, den Umgang mit fehlenden Werten genauer zu bestimmen, usw. Im Folgenden brauchen wir die Funktion und ihre Argumente aber nicht weiter zu spezifizieren, sondern die Standardeinstellungen der Argumente reichen aus. Nachfolgend werden wir mit den Beispielen 2 und 4 weiterarbeiten: lm(formula = happy ~ agea, data = daten_ess) lm(happy ~ polintr, daten_ess) Weisen Sie Ihre Regressionsanalysen jeweils einem “Ergebnisobjekt” zu! Wie ist dieses Objekt aufgebaut? Welche Komponenten und Informationen beinhaltet es? modell1 &lt;- lm(happy ~ agea, data = daten_ess) modell2 &lt;- lm(happy ~ polintr, data = daten_ess) #Am Beispiel des ersten Modells: str(modell1) ## List of 12 ## $ coefficients : Named num [1:2] 7.94763 0.00502 ## ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;(Intercept)&quot; &quot;agea&quot; ## $ residuals : Named num [1:200] -0.209 0.676 1.746 -0.169 0.701 ... ## ..- attr(*, &quot;names&quot;)= chr [1:200] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ effects : Named num [1:200] -115.895 -1.271 1.731 -0.141 0.664 ... ## ..- attr(*, &quot;names&quot;)= chr [1:200] &quot;(Intercept)&quot; &quot;agea&quot; &quot;&quot; &quot;&quot; ... ## $ rank : int 2 ## $ fitted.values: Named num [1:200] 8.21 8.32 8.25 8.17 8.3 ... ## ..- attr(*, &quot;names&quot;)= chr [1:200] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ assign : int [1:2] 0 1 ## $ qr :List of 5 ## ..$ qr : num [1:200, 1:2] -14.1421 0.0707 0.0707 0.0707 0.0707 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:200] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. .. ..$ : chr [1:2] &quot;(Intercept)&quot; &quot;agea&quot; ## .. ..- attr(*, &quot;assign&quot;)= int [1:2] 0 1 ## ..$ qraux: num [1:2] 1.07 1.1 ## ..$ pivot: int [1:2] 1 2 ## ..$ tol : num 1e-07 ## ..$ rank : int 2 ## ..- attr(*, &quot;class&quot;)= chr &quot;qr&quot; ## $ df.residual : int 198 ## $ xlevels : Named list() ## $ call : language lm(formula = happy ~ agea, data = daten_ess) ## $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language happy ~ agea ## .. ..- attr(*, &quot;variables&quot;)= language list(happy, agea) ## .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1 ## .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. .. ..$ : chr [1:2] &quot;happy&quot; &quot;agea&quot; ## .. .. .. ..$ : chr &quot;agea&quot; ## .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;agea&quot; ## .. ..- attr(*, &quot;order&quot;)= int 1 ## .. ..- attr(*, &quot;intercept&quot;)= int 1 ## .. ..- attr(*, &quot;response&quot;)= int 1 ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; ## .. ..- attr(*, &quot;predvars&quot;)= language list(happy, agea) ## .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:2] &quot;numeric&quot; &quot;numeric&quot; ## .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;happy&quot; &quot;agea&quot; ## $ model :&#39;data.frame&#39;: 200 obs. of 2 variables: ## ..$ happy: int [1:200] 8 9 10 8 9 9 10 10 8 7 ... ## ..$ agea : int [1:200] 52 75 61 44 70 52 36 29 74 72 ... ## ..- attr(*, &quot;terms&quot;)=Classes &#39;terms&#39;, &#39;formula&#39; language happy ~ agea ## .. .. ..- attr(*, &quot;variables&quot;)= language list(happy, agea) ## .. .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1 ## .. .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. .. .. ..$ : chr [1:2] &quot;happy&quot; &quot;agea&quot; ## .. .. .. .. ..$ : chr &quot;agea&quot; ## .. .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;agea&quot; ## .. .. ..- attr(*, &quot;order&quot;)= int 1 ## .. .. ..- attr(*, &quot;intercept&quot;)= int 1 ## .. .. ..- attr(*, &quot;response&quot;)= int 1 ## .. .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; ## .. .. ..- attr(*, &quot;predvars&quot;)= language list(happy, agea) ## .. .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:2] &quot;numeric&quot; &quot;numeric&quot; ## .. .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;happy&quot; &quot;agea&quot; ## - attr(*, &quot;class&quot;)= chr &quot;lm&quot; Die Funktion lm() generiert uns eine Listenobjekt mit 12 bzw. 13 weiteren Unterobjekten. Die genauen Details, was in diesen Ergebnisobjekten zu finden ist, können wir der Hilfeseite der unter Value entnehmen. “An object of class ‘lm’ is a list containing at least the following components: coefficients: a named vector of coefficients residuals: the residuals, that is response minus fitted values. fitted.values: the fitted mean values. rank: the numeric rank of the fitted linear model. weights: (only for weighted fits) the specified weights. df.residual: the residual degrees of freedom. call: the matched call. terms: the terms object used. contrasts: (only where relevant) the contrasts used. xlevels: (only where relevant) a record of the levels of the factors used in fitting. offset: the offset used (missing if none were used). y: if requested, the response used. x: if requested, the model matrix used. model: if requested (the default), the model frame used. na.action: (where relevant) information returned by model.frame on the special handling of NAs.” Wie immer können wir auf diese Elemente zugreifen, z.Bsp. um uns ein Streudiagramm der Residuen ausgeben zu lassen: plot(modell1$model$happy, modell1$residuals) hist(modell1$residuals) Beim Ergebnisobjekt des zweiten Modells sehen wir nun eine Veränderung, nämliche dass als weiteres Element der Liste “contrasts” hinzugekommen (als Hinweis auf die “Kontrastierung” bei der kategorialen, unabhängigen Variable, siehe auch unten). modell1$constrats ## NULL modell2$contrasts ## $polintr ## [1] &quot;contr.poly&quot; Interpretieren Sie die Ergebnisse Ihrer beiden Regressionsanalysen. Für die Interpretation des Modells bzw. des Objektes verwenden wir die summary()-Funktion: summary(modell1) ## ## Call: ## lm(formula = happy ~ agea, data = daten_ess) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.2540 -1.0305 -0.0983 0.8577 1.9670 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.947628 0.289850 27.420 &lt;2e-16 *** ## agea 0.005023 0.005532 0.908 0.365 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.399 on 198 degrees of freedom ## Multiple R-squared: 0.004147, Adjusted R-squared: -0.0008828 ## F-statistic: 0.8245 on 1 and 198 DF, p-value: 0.365 Die Funktion gibt uns anschliessend verschiedenste Kennzahlen aus. Die Auflistung beginnt mit der Formel (Call), geht über zu einer kurzen Beschreibung der Verteilung der Residuen (Residuals), bevor dann die Koeffizienten und anschliessend eine Übersicht zum gesammten Modell folgen. Bei den Koeffizienten (Coefficients) finden wir vier Kennzahlen und eine Ergänzung: Zuerst sind es die eigentlichen Werte der Koeffizienten (Estimate), d.h. der b0-Wert (Intercept) und der Koeffizient der unabhängigen Variable. Ersterer gibt den Vorhersagewert an, wenn die unabhängige Variable 0 wäre (7.947628), während letzterer die Zunahme des Vorhersagewertes bei einer Zunahme der unabhängigen Variable um 1 angibt (0.005023). Die zweite Kennzahl ist der Standardfehler (Std.Error), also die durchschnittliche Abweichung der Koeffizienten, da diese ja auf Stichprobendaten beruhen. Das heisst wir würden einen neuen Wert bekommen, wenn wir eine neue Stichprobe ziehen würden und dieser Wert würde im Durchschnitt um 0.005532 abweichen. Daumenregel: Das Ergebnis von Regressionskoeffizient – (2*SE) sollte nicht 0 überschreiten bzw. im Vergleich zum Koeffizienten das Vorzeichen wechseln. Sonst wäre das ein Hinweis, dass kein Einfluss der unabhängigen Variable besteht (da jeweils rund +2x &amp; -2x Standardfehler rund 95% der Ausprägungen einer Normalverteilung repräsentieren). Es gilt also den Standardfehler im Zusammenhang zum Regressionskoeffizient zu betrachten. Die dritte Kennzahl ist der T-Wert (t value), was der eigentlichen Testwerte für die t-Verteilung repräsentiert (Diaz-Bone 2019, 224). Als Daumenregel: T Wert, die grösser als 2 sind, werden meistens signifikant sein. Der p-Wert (Pr(&gt;|t|)) ist der Signifikanzwert für einzelne Regressionskoeffizienten. Wiederum: Die Kennzahl gibt uns an, wie wahrscheinlich es ist, einen solchen (oder grösseren) Wert für den Regressionskoeffizienten zu erhalten unter der Annahme, dass die Nullhypothese in der Grundgesamtheit zutreffen würde.10 Zusätzlich findet sich auch jeweils noch der Hinweis zu den Sternchen als Signifikanzcodes. Anschliessend folgen weitere Kennwerte zum gesamten Modell: Die Standardabweichung der Residuen wird ausgegeben (Residual standard error) sowie die Freiheitsgrade (degrees of freedom), d.h. N - J - 1, wobei J die Anzahl unabhängiger Variablen ist. Danach folgen das multiple R-Quadrart (Multiple R-squared) sowie (Adjusted R-squared). Ersteres gibt uns die Erklärungsleistung des Modells sowie zugleich die Stärke des gerichteten, statistischen Zusammenhangs zwischen X und Y an. Letzteres ist ein Gütemass, dass die Erklärungsleistung im Bezug zu den Anzahl Variablen betrachtet und den Wert für Stichprobendaten etwas anpasst (Diaz-Bone 2019, 223). Schliesslich finden sich noch die Angaben zum F-Test (F-statistic). Dieser Wert weisst aus, ob das Modell auch über eine Erklärungsleistung in der Grundgesamtheit verfügt. Hier wird global getestet, ob der Regressionsansatz insgesamt etwas aussagt (Diaz-Bone 2019, 221). Dazu gibt uns das Modell sowohl den konkreten F-Test-Wert (und wiederum die Freiheitsgrade der Verteilung) als auch den p-Wert an. Dieser sagt uns nun aus, ob das Modell auch in der Grundgesamtheit eine Erklärungsleistung besitzt (wiederum im Sinne wie wahrscheinlich es ist, einen solchen oder grösseren R-Quadrat Wert zu erhalten unter der Annahme, dass das die Nullhypothese zutrifft). summary(modell2) ## ## Call: ## lm(formula = happy ~ polintr, data = daten_ess) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.0169 -0.8500 -0.0169 0.7895 2.1500 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.15826 0.11125 73.334 &lt;2e-16 *** ## polintr.L 0.51659 0.25652 2.014 0.0454 * ## polintr.Q 0.08904 0.22249 0.400 0.6895 ## polintr.C 0.02791 0.18222 0.153 0.8784 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.391 on 196 degrees of freedom ## Multiple R-squared: 0.02599, Adjusted R-squared: 0.01108 ## F-statistic: 1.743 on 3 and 196 DF, p-value: 0.1595 Im Unterschied zum ersten Modell sehen wir beim zweiten Modell, dass die einzelnen Ausprägungen der kategorialen Variable eigene Koeffizienten und zugehörige Kennwerte erhalten. R wandelt kategoriale, unabhängige Variablen automatisch in Dummy-Variablen um, die jeweils mit 0 und 1 kodiert sind (Manderscheid 2017, 197). Jeder dieser Koeffizienten der Dummy-Variablen gibt die Veränderung gegenüber der Referenzkategorie an (“Not at all interested”), die wäre jetzt auch b0 entspricht. Im Umgang mit kategorialen Variablen mit mehreren Variablen wird so ein multiples lineares Modell erstellt. References "],
["wochenplan-11.html", "11 Wochenplan 11 11.1 Lernziele WP11 11.2 Aufgaben WP11 11.3 Ergänzung: ein schlechtes Beispiel", " 11 Wochenplan 11 …zu den Einheiten vom 03. &amp; 10.12.2020, multiple lineare Regression und Residuenanlayse 11.1 Lernziele WP11 Im Rahmen des 11. Wochenplans wollen wir uns einem multiplen linearen Regressionsmodell widmen. Wie immer möchten wir uns dazu als erstes einen Überblick verschaffen und eine Fragestellung formulieren, bevor dann ein Modell berechnet und interpretiert werden kann. Anschliessend gilt es über eine grafische Residuenanalyse die Anwendungsvoraussetzungen für die multiple lineare Regression zu testen sowie über den Ausschluss von gewissen Fällen das Modell zu verbessern. Konkret lassen sich folgende Ziele für diesen letzten Wochenplan festlegen: Sie verstehen, wie Sie mittels Korrelationstabellen erste Ideen für ein multiples Modell sammeln können. Sie können multiple lineare Regressionsanalysen in R korrekt modellieren, durchführen und interpretieren. Sie verstehen, wie die Anwendung von plot() auf unser Ergebnisobjekt bei der Analyse der Qualität und Aussagekraft eines Regressionsmodells helfen kann. Sie können Ausreisser in einem Regressionsmodell ausschliessen und die damit erreichten Veränderungen im Modell einschätzen. 11.2 Aufgaben WP11 Laden Sie Ihr Datenimport-Skript. Arbeiten Sie anschliessend in einem R-Markdown-Dokument weiter. setwd(&quot;C:/IhrDateiPfad/IhrArbeitsordner&quot;) source(file = &quot;ess_import.R&quot;) Berechnen Sie mittels der Funktion rcorr(), die Teil des Paketes “Hmisc” ist, eine Korrelationstabelle zwischen den Variablen “wkhtot”, “agea”, “edulvla” und “chldhm”. attach(daten_ess) summary(wkhtot) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.00 30.00 42.00 88.31 49.25 888.00 summary(agea) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 16.00 33.75 50.00 49.24 62.00 91.00 summary(edulvla) ## ISCED 0-1 ISCED 2 ISCED 3 ISCED 4 ISCED 5-6 ## 3 29 83 8 77 summary(chldhm) ## Respondent lives with children at household grid ## 65 ## Does not ## 135 detach(daten_ess) Über den summary()-Befehl wird deutlich, dass die Variable “wkhtot” noch fehlende Werte enthält. Diese gilt es zuerst noch auszuschliessenn – die erste der beiden folgenden Code-Zeile kann dann wiederum im R-Skript integriert werden: daten_ess$wkhtot[daten_ess$wkhtot&gt;100] &lt;- NA summary(daten_ess$wkhtot) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00 26.25 42.00 36.47 45.00 90.00 14 Anschliessend können wir die Korrelationstabelle berechnen. Da rcorr() eine Tabelle als Input benötigt erstellen wir diese in der Funktion selber über cbind(). attach(daten_ess) library(Hmisc) ## Warning: package &#39;Hmisc&#39; was built under R version 4.0.3 ## Loading required package: lattice ## Loading required package: survival ## Loading required package: Formula ## Warning: package &#39;Formula&#39; was built under R version 4.0.3 ## Loading required package: ggplot2 ## Warning: package &#39;ggplot2&#39; was built under R version 4.0.3 ## ## Attaching package: &#39;Hmisc&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## format.pval, units rcorr(cbind(wkhtot, agea, edulvla, chldhm), type = &quot;pearson&quot;) ## wkhtot agea edulvla chldhm ## wkhtot 1.00 0.05 0.14 0.20 ## agea 0.05 1.00 0.06 0.12 ## edulvla 0.14 0.06 1.00 -0.10 ## chldhm 0.20 0.12 -0.10 1.00 ## ## n ## wkhtot agea edulvla chldhm ## wkhtot 186 186 186 186 ## agea 186 200 200 200 ## edulvla 186 200 200 200 ## chldhm 186 200 200 200 ## ## P ## wkhtot agea edulvla chldhm ## wkhtot 0.5362 0.0561 0.0074 ## agea 0.5362 0.4033 0.0974 ## edulvla 0.0561 0.4033 0.1700 ## chldhm 0.0074 0.0974 0.1700 detach(daten_ess) Die Funktion gibt uns nun drei verschiedene Tabellen aus: Zuerst die eigentlichen Korrelationswerte (1), dann die Fallzahlen (2), wo die 14 fehlende Werte der Variable “wkhtot” nun einen Unterschied ausmachen und schliesslich die p-Werte für die einzelnen Korrelationen. Schnell wird auch deutlich dass die Tabelle redundante Informationen besitzt, da sie über die diagonale Achse gespiegelt ist (d.h. der jeweilige Wert von „edulvla“ und „agea“ ist natürlich derselbe Werte wie derjenige von „agea“ und „edulvla“). Interpretieren Sie die Korrelationstabelle kurz: Wo sehen Sie bereits Zusammenhänge, wo nicht? Formulieren Sie anschliessend eine Fragestellung sowie eine Hypothese zu diesen Variablen für ein multiples lineares Regressionsmodell. In der Tabelle finden wir vier von sechs Zusammenhängen, die uns von ihrer Stärke her interessieren sollten. Dies sind die Werte zwischen den Variablen „wkhtot“ und „edulvla“, „edulvla“ und „chldhm“, „agea“ und „chldhm“ sowie „wkhtot“ und „chldhm“. Letzterer Wert ist ebenfalls auch signifikant. Die weiteren Werte weisen keine beachtenswerten R-Quadrat-Wert aus (und sind auch nicht signifikant). Fragestellungen könnten dann wie folgt formuliert werden… …ein Beispiel von Frau Stöckli: Welchen Einfluss haben das Alter, das Bildungsniveau und die Tatsache, ob eine Person mit Kindern zusammenwohnt oder nicht, auf die wöchentliche Anzahl Arbeitsstunden? Und als These: Es wird vermutet, dass der Wechsel von der Gruppe, die mit Kindern zusammenleben, zu der Gruppe, die ohne Kinder leben, zu einem höheren Vorhersagewert für die Anzahl wöchentlicher Arbeitsstunden (“wkhtot”) führt. Es wird vermutet, dass je höher das Bildungsniveau (“edulvla”) desto höher die Anzahl wöchentlicher Arbeitsstunden. Es wird vermutet, dass das Alter ein geringer, positiver Einfluss auf die Anzahl wöchentlicher Arbeitsstunden hat. …ein Beispiel von Herr Ineichen: Wie wirkt sich die Tatsache ob Kinder im eigenen Haushalt leben und das Ausbildungsniveau auf die Anzahl gearbeiteter Wochenstunden aus? Kontrollvariable: Alter. Hypothese: Menschen mit höherem Bildugnsniveau und ohne Kinder haben arbeiten mehr Stunden pro Woche als Menschen mit tiefem Bildungsniveau und ohne Kinder. …ein Beispiel von Herr Schürmann: Wirken die unabhängigen Variablen “agea”, “edulvla” und “chldhm” einzeln Betrachtet auch inder GG auf die abhängige Variable “wkhtot” ein? Nullhypothese: Alter, Bildungsniveau und Kinder zu Hause beeinflussen positiv die Arbeitsstunden pro Woche. Alternativhypothese: Alter, Bildungsniveau und Kinder zu Hause haben keinen Effekt auf die Arbeitsstunden pro Woche. Bei drei Beispielen wird deutlich, dass uns die Variable zu den Arbeitsstunden als abhängige Variable interessiert, und wir die Effekte darauf von den anderen, unabhängigen Variablen beschreiben möchten. Berechnen Sie anschliessend Ihr Regressionsmodell mittels der Funktion lm(). Speichern Sie das Modell wie immer als Objekt ab und interpretieren Sie es anschliessend. Modellm_1 &lt;- lm(wkhtot ~ agea + edulvla + chldhm, data = daten_ess) summary(Modellm_1) ## ## Call: ## lm(formula = wkhtot ~ agea + edulvla + chldhm, data = daten_ess) ## ## Residuals: ## Min 1Q Median 3Q Max ## -41.742 -9.298 4.363 10.367 47.374 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 31.11304 4.17227 7.457 3.67e-12 *** ## agea 0.01657 0.06952 0.238 0.81184 ## edulvla.L -1.72936 6.37703 -0.271 0.78656 ## edulvla.Q 9.06236 5.48903 1.651 0.10049 ## edulvla.C -4.07729 5.33272 -0.765 0.44553 ## edulvla^4 3.21044 3.63580 0.883 0.37842 ## chldhmDoes not 7.69041 2.61913 2.936 0.00376 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 16.15 on 179 degrees of freedom ## (14 observations deleted due to missingness) ## Multiple R-squared: 0.07828, Adjusted R-squared: 0.04738 ## F-statistic: 2.534 on 6 and 179 DF, p-value: 0.02228 Die Koeffizienten unseres Modells lassen sich wie folgt interpretieren: Wenn jemand 0 Jahre alt ist, auf dem „ISCED 0-1“ Bildungsniveau ist und Kinder zuhause hat, dann würde diese Person rund 31h pro Woche arbeiten (Intercept). Mit jeden zusätzlichen Altersjahr arbeitet eine Person gemäss dem Modell 0.02h mehr. Wechselt man hingegen vom ersten auf das zweite Bildungsniveau so arbeitet man 1.7h weniger, beim Wechsel auf das dritte Bildungsniveau wiederum 9h mehr, usw. Keine Kinder im Haushalt zu haben führt wiederum dazu, dass man fast 8h mehr Arbeitet pro Woche. Von den Koeffizienten ist lediglich derjenige für die Variable zu Kinder im Haushalt signifikant. Alle anderen Koeffizienten haben hingegen hohe Wahrscheinlichkeiten zufällig aufzutreten, wenn sie keinen Effekte hätten in der Grundgesamtheit (etwa der Koeffizient von „agea“ hat eine Wahrscheinlichkeit von über 80%, völlig zufällig aufgetreten zu sein). Dies sieht man auch anhand der Standardfehler, die alle sehr grosse zufällige Streuung der Koeffizienten ausweisen. Über die Tabelle wird weiter auch nochmals die Berechnung der Werte für die t-Tests der Regressionskoeffizienten deutlich (Diaz-Bone 2019, 223). Der jeweilige t value ergibt sich nämlich dadurch, dass ein Koeffizient durch den Standardfehler geteilt wird, also etwa 0.01657 / 0.06952 = 0.238. Dieser Werte fällt wiederum bei einer t-Verteilung von 179 Freiheitsgraden und einem 95% Signifikanzniveau in den Annahmebereich der 0-Hypothese. Unser Modell bzw. unsere unabhängigen Variablen erklären rund 8% bzw. 5% der Varianz von der Variable „wkhtot“. Dieser Wert selber ist aber leicht signifikant – oder umgekehrt formuliert: Es gibt lediglich eine Chance von etwas mehr als 2% dass wir diesen R-Quadrat Wert erhalten würden, obschon das Modell in der Grundgesamtheit keine Erklärungsleistung besitzt. Prüfen Sie dann die Anwendungsvoraussetzungen (vgl. Diaz-Bone 2019, 202f) mittels plot(). Wie beurteilen Sie die Qualität Ihres Modells? Die plot()-Funktion weisst uns fünf verschiedenen Plots zur grafisch gestützten Residuenanalyse aus: plot(Modellm_1) Die erste Grafik ist ein Streudiagramm der Residuen und der Vorhersagwerte. Da hier der Vorhersagwerte (und nicht die abhängige Variable) lassen sich hier alle unabhängigen Variablen überprüfen, und zwar auf die Frage der Linearität hin.11 Ein spezifisches Muster im Plot würde auf fehlende Linearität hinweisen oder auch darauf, dass es weitere erklärende Variablen gibt, die noch nicht im Modell eingeschlossen sind (Manderscheid 2017, 194). In unserem Fall scheint die Linearitätsannahme nicht verletzt zu sein und es zeigt sich auch kein (starkes) spezifisches Muster zu zeigen. Die zweite Grafik, der Normal-Q-Q Plot, gibt uns Hinweise zu der Normalverteilung der Residuen. Dazu werden die empirischen Verteilungen der Residuen (die Standardabweichungen der Werte) gegen die Quantile geplottet, die bei einer Normalverteilung zu erwarten wären. Quantile sind hier die Wahrscheinlichkeiten: In der theoretischen Normalverteilung liegen links und rechts von 0 je 50%. Je weiter die Werte von Null abweichen, desto weniger Fälle liegen dort (links von -1 wären noch rund 13% der Fälle). Die theoretische Normalverteilung – die gerade, gestrichelte Linie – wird dann mit der empirischen Verteilung verglichen – die über Kreise gebildete, “wackelige” Linie. Ziel sollte es sei, dass eben im mindestens im Bereich + 1 x Standardabweichung und - 1 x Standardabweichung (68% der Fälle) die beiden Linien ziemlich deckungsgleich wären. Bei grösseren Stichproben werden die Abweichungen immer weniger problematisch (vgl. auch Diaz-Bone 2019:231).12 Die dritte Grafik der Scale-Location prüft die Homoskedaszität in dem die Vorhersagwerte gegen die standardisierten Residuen geplottet werden. Die Punkte sollten hier horizontal in einheitlicher Breite variieren (Manderscheid 2017, 194). Wie bereits bei der ersten Grafik findet sich hier eine rote Linie, ein sogenannter Smoother, ist eine immer aufs neue geschätzte Line, die im optimalfall gerade verlaufen sollte. Hier sehen wir nun, dass das wir vor allem bei grösseren Vorhersagewerte auch grössere Residuen haben (unabhängig davon ob diese positiv oder negativ sind). Hier scheint also eine gewisse Heteroskedaszität vorzuliegen. Die vierte und letzte Grafik stellt die Residuen dem sogenannten Leverage Wert gegenüber. Dieser Wert gibt an, wie stark ein jeweiliger Punkt auf das Modell wirkt (dessen Hebelwirkung). Je stärker ein solcher Leverage-Wert eines Punktes ist – je weiter rechts er ist – desto geringer sollte die Residue sein. Insbesondere die Werte ausserhalb der Cook’s Distance sollten wir ausschliessen. Die Anwendungsvoraussetzung der geringen Multikollinearität konnten wir bereits in der Korrelationstabelle oben überprüfen.13 Bonusaufgabe: Versuchen Sie, ein neues Modell zu berechnen, in dem mögliche Ausreisser fehlen, die Ihnen die grafische Residuenanalyse aufgezeigt hat. Wie verändert sich die Modellgüte bzw. die Erklärungsleistung Ihres Modells? In allen vier Grafiken, welche die Funktion plot()von unserem Modell erzeugt hat, wurden immer wieder bestimmte Fälle ausgewiesen, die das Modell womöglich stark beeinflussen. Diese Fälle – insbesondere diejenige im letzten Plot – können wir versuchsweise ausschliessen und ein neues Modell rechnen. Modellm_2 &lt;- lm(wkhtot ~ agea + edulvla + chldhm, data = daten_ess[-c(15, 50, 57, 77, 80, 172),]) summary(Modellm_2) ## ## Call: ## lm(formula = wkhtot ~ agea + edulvla + chldhm, data = daten_ess[-c(15, ## 50, 57, 77, 80, 172), ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -37.328 -7.769 3.694 9.830 35.360 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 31.09923 4.04704 7.684 1.1e-12 *** ## agea 0.02215 0.06632 0.334 0.73882 ## edulvla.L -2.19703 6.04937 -0.363 0.71691 ## edulvla.Q 8.35237 5.21162 1.603 0.11084 ## edulvla.C -3.47810 5.34504 -0.651 0.51609 ## edulvla^4 2.67714 3.72067 0.720 0.47278 ## chldhmDoes not 8.02351 2.46417 3.256 0.00136 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.1 on 173 degrees of freedom ## (14 observations deleted due to missingness) ## Multiple R-squared: 0.0841, Adjusted R-squared: 0.05234 ## F-statistic: 2.648 on 6 and 173 DF, p-value: 0.01755 Der Auschluss dieser Fälle hat dazu geführt, das die Erkläungsleitung unseres neuen Modells gegenüber dem alten um ein halbes Prozent gestiegen ist. 11.3 Ergänzung: ein schlechtes Beispiel Als Ergänzung zum Inhalt in der Einheit wird hier noch ein künstliches, besonders negatives Beispiel konstruiert. Dieses verletzt besonders stark die die Anwendungsvoraussetzungen, also die Linearitätsannahme, die Varianzhomogenität und die Normalverteilung der Residuen. Dazu ordnen wir zuerst den Datensatz nach der Grösse der Variable „wkhtot“ und konstruieren uns eine Variable, die ein bestimmtes, extremes Muster aufweist: daten_ess &lt;- daten_ess[order(daten_ess$wkhtot),] daten_ess$testV &lt;- 10 daten_ess$testV[1] &lt;- 400 daten_ess$testV[25:74] &lt;- 20 daten_ess$testV[75:99] &lt;- 2000 daten_ess$testV[100:123] &lt;- 90 daten_ess$testV[124] &lt;- 2 daten_ess$testV[125:75] &lt;- 100 daten_ess$testV[150:200] &lt;- 3 Anschliessend berechnen wir ein neues Modell und betrachten die Grafiken, die uns plot()generiert, und wie sich hier dieses extreme Muster auswirk: Modellm_t &lt;- lm(wkhtot ~ testV, data = daten_ess) plot(Modellm_t) References "],
["references.html", "References", " References "]
]
