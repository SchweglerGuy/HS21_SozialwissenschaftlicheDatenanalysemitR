[["index.html", "Sozialwissenschaftliche Datenanalyse mit R Einführung", " Sozialwissenschaftliche Datenanalyse mit R Kenneth Horvath &amp; Guy Schwegler HS 2021 Einführung Das Seminar Sozialwissenschaftliche Datenanalyse mit R bietet eine systematische Einführung in das Statistikpaket R sowie die Benutzeroberfläche RStudio. R ist eine Open Source Software, die sich unter anderem durch Flexibilität sowie durch vielfältige Möglichkeiten der numerischen und grafischen Datenanalyse auszeichnet. Das Seminar führt auf der einen Seite allgemein in den Aufbau des Programms und dessen Funktionsweisen ein. Auf der anderen Seite werden gewisse statistische Verfahren auch in inhaltliche Abstimmung mit der Vorlesung Grundlagen der multivariaten Statistik vermittelt. Anhand der Funktionsweisen und der Verfahren werden dann Techniken des effizienten Datenmanagements, Möglichkeiten zur eigenständigen Programmierung von kleinen Funktionen sowie Formen der grafischen Datenanalyse und Ergebnisdarstellung besprochen. Das vorliegende Dokument ist ein sogenanntes Bookdown (Xie 2020), siehe auch hier, und dient der Ergebnissicherung im Seminarverlauf. Das heisst dass die im Seminar besprochene Themen hiernochmals schriftlich festgehalten, diskutiert und allenfalls mit Literatur ergänzt werden (siehe für allgemeine Literatur etwa Diaz-Bone (2019), Kabacoff (2015) oder Manderscheid (2017)). Das Bookdown wird laufend aktualisiert. Ebenfalls werden in diesem Bookdown die Lösungen für die im Seminar beziehungsweise in den Wochenplänen gestellten Aufgaben präsentiert (Falllösungen). References "],["wocheplan-01-einführung.html", "1 Wocheplan 01: Einführung 1.1 Sozialwissenschaftliche Datenanalyse 1.2 Ziel des Kurses 1.3 R als Programm &amp; RStudio 1.4 Lernziele der ersten Woche 1.5 Aufgaben der ersten Woche 1.6 Ergänzung: Standardabweichung zwischen Grundgesamtheit und Stichprobe", " 1 Wocheplan 01: Einführung im Rahmen der 01. und 02.Einheit. 1.1 Sozialwissenschaftliche Datenanalyse Das Seminar sozialwissenschaftliche Datenanalyse mit R versucht eine Realität des statistischen Arbeitens zu vermitteln und ergänzt so die Vorlesung Grundlagen der multivariaten Statistik gleich in zweierlei Hinsicht: Erstens wird eine Auswahl der gelernten statistischen Verfahren konkret angewendet (und so auch nochmals repetiert). Zweitens zeigt sich neben den eigentlichen Verfahren ein weiterer, impliziter Teil der Statistik: ein Umgang mit Daten, deren Aufbereitung und Verarbeitung sowie die damit einhergehenden Herausforderungen. Hinter dem Seminar steht eine bestimmte Vorstellung der sozialwissenschaftlichen Datenanalyse, die folgende Teile enthält (Wickham and Grolemund 2016): Figure 1.1: Modell Datenanalyse Als erster Schritt müssen die Daten eingelesen bzw. importiert werden. Die importierten Daten gilt es dann aufzubereiten und aufzuräumen. Das bedeutet, dass sie in einer konsistenten Form gespeichert werden sollen (z.Bsp. dass jede Zeile einer Person und jede Spalte einer Variable entspricht). Dieser zweite Schritt ist im Rahmen von Sekundärdaten (wie auch wir sie verwenden werden) oft bereits erfolgt. Ein weiterer Schritt ist es dann, die Daten zu transformieren. Das heisst die Fälle und ihre Ausprägungen werden auf ein bestimmes Interesse eingegrenzt (z.Bsp. auf alle Personen die über ein bestimmtes Einkommen verfügen), neue Variablen werden erstellt (die Funktionen bestehender Variablen sind, etwa Einkommensklassen), und eine Reihe von zusammenfassenden Statistiken werden berechnet (verschiedene univariate Kennwerte). Das Aufbereiten und Transformieren ist ein grosser Teil der statistischen Analyse (es ist ein Kampf mit den Daten, Wickham and Grolemund 2016, Kap.1.1). Ziel dieser Arbeit ist es, die Daten in eine passende Form zu bringen, um optimal mit ihnen arbeiten zu können. Wenn die Daten (voerst) in einer optimalen Form vorliegen gibt es zwei Hauptmotoren der Wissensgenerierung (Wickham and Grolemund 2016, Kap.1.1): Visualisierung und Modellierung. Mit Visualisierungen lässt sich schnell eine Übersicht gewinnen (z.Bsp. könnte es überhaupt einen Zusammenhang zwischen zwei Variablen geben?). Modellierungen wiederum ergänzen diese ersten Einsichten, indem präzise Antworten auf Fragen möglich sind (wie gross ist der Zusammenhang genau?). Das Transformieren, Visualisieren und Modellieren der Daten ist dabei keineswegs ein linearer Prozess, sondern es ergeben sich in ihm immer wieder Wechselwirkungen, Rückbezüge und dadurch neue Wege, um an die Daten heranzutreten. Der letzte Schritt der Datenanalyse ist die Kommunikation. Es gilt also sowohl das Vorgehen (zumindest teilweise) als inbesondere auch die Ergebnisse der Analyse anderen mitzuteilen. Diese Prozesse der Datenanalyse finden alle in einem bestimmen Rahmen statt (vgl. auch Sauer 2019, 3). Dies ist auf der einen Seite die Idees des Programmierens im Vorgehen selber (vgl. Wickham and Grolemund 2016, Kap.1.1). Auf der anderen Seite bilden aber die Sozialwissenschaften selber auch einen Rahmen, anhand dessen etwa Datenstrukturen (z.Bsp. dass eine Person ein Fall und damit eine Zeile ist) oder angemessene Ziele der Analyse (ab wann ist ein Zusammenhang etwa gross?) vorgegeben werden. 1.2 Ziel des Kurses Das Seminar verfolgt zwei miteinander verzahnte, übergeordnete Lernziele. Einerseits sollen die Studierenden sich Grundkenntnisse der statistischen Datenanalyse mit R aneignen. Andererseits werden ausgewählte Inhalte der Vorlesung praktisch angewandt und damit auch veranschaulicht.1 Konkret sollen die Studierenden am Ende des Semesters einen ersten Einblick in Abläufe und Anforderungen softwaregestützter Datenanalyse haben, typische Herausforderungen statistischen Arbeitens eigenständig bewältigen können, die allgemeine Funktionsweise und die Struktur von R verstehen, die Umsetzung ausgewählter multivariater Verfahren in R beherrschen, dabei auch grafische Verfahren als zentrale Bausteine aktueller Datenanalyse einsetzen können sowie die Grundlage dafür erworben haben, flexibel eigene Analysestrategien in R umzusetzen. 1.3 R als Programm &amp; RStudio R als Programmiersprache wurde von Beginn an für die Statistik beziehungsweise für die Statistiklehre entwickelt. Die Anfänge des Programms fanden in den 1990er Jahre an der Universität Auckland in Neuseeland statt, wo R von Ross Ihaka und Robert Gentleman entwickelt wurde (Manderscheid 2017, 1). Der Buchstabe R als Name geht sowohl auf eine ältere Grundlage zurück  die Programmiersprache S  als auch auf die Vornamen der beiden Entwickler (ebd., vgl. auch Sauer 2019, 13f). Das R-Projekt wurde in der Zusammenareit mit weiteren Wissenschaftler_Innen voran getrieben und bald auch unter der General Public Licence (GNU) veröffentlicht (Manderscheid 2017, 1). R ist daher frei zugänglich, kostenlos und darf von allen verändert werden. Es ist insbesondere auch diese Open Source Idee, die R zu seiner Verbreitung half  und die sicherstellt, dass die neusten Entwicklungen in und mit der Software stattfinden. R als Programm ist in Paketen organisiert und präsentiert sich als Statistikumgebung (Manderscheid 2017, 1). Ausgehend von der Basisversion bzw. des Basispaketes kann R beliebig erweitert werden. Unter https://cran.r-project.org/ findet sich eine beständig wachsende und umfangreiche Sammlung von Paketen, die sowohl Lösungen für allgemeine Verfahren anbieten (etwa Pakete für die multiple Korrespondenzanalyse, siehe soc.ca) als auch für spezifische Probleme (etwa für Atomic Force Microscope Image Analysis beim Paket AFM). Diese Pakete können installiert werden und es gilt sie dann jeweils noch zu laden, bevor sie verwendet werden können. Nach dem Beenden des Programms werden die verwendeten Pakete wieder versorgt und es gilt sie beim nächsten Mal erneut zu laden (die Pakete beleiben aber installiert). Letzterer Vorgang stellt sicher, dass R schlank bleibt, d.h. nur immer die benötigen Dinge auch ausgeführt werden. install.packages(&quot;soc.ca&quot;) #...installiert das Paket library(soc.ca) #...lädt das Paket Neben dieser Open Source Idee und der daraus folgenden, beständigen Aktualisierung und Erweiterungen des Programms zeichnet R sich weiter durch dessen Stärke im Bereich der Visualisierung aus. Es bieten sich unbegrenzte Möglichkeiten für Grafiken und Diagramme, sowohl bereits in der Basisversion als insbesondere auch mit spezifischen Paketen (siehe Chang et al. 2020). Neben der Basisversion von R und R als eigentlicher Programmiersprache gibt es grafische Benutzeroberflächen (GUIs), um mit der Programmiersprache umzugehen. Im Zentrum unseres Seminars steht RStudio, die am weitesten verbreitete grafische Benutzeroberfläche von R. Diese Oberfläche bietet einige praktische Zusatzfunktionen und erleichtert so das Arbeiten mit R durch Autovervollständigkeitsfunktionen, automatische Einrückungen, Syntaxhervorhebung, integrierte Hilfsfunktion, Informationen zu Objekten im Workspace, menügestützten Oberflächen und Daten-Viewer (Manderscheid 2017, 18). Die eigentliche Arbeit verrichtet aber weiterhin R selber, und R wird automatisch gestartet wird, wenn Sie RStudio starten (Sauer 2019, 21). Man kann diese Arbeitsteilung mit einem Auto vergleichen: R ist der Motor des Autos, während RStudio das Amaturenbrett ist, vor dem Sie sitzen und das Auto lenken. 1.4 Lernziele der ersten Woche Die erste Seminarwoche dient dazu, die technischen Voraussetzungen für die gemeinsame Arbeit im Seminar zu prüfen und mit der geplanten Arbeitsweise vertraut zu werden. Das Seminar zielt nicht nur auf einen Frontalunterricht ab, sondern ist als eine Art flipped classroom konzipiert. Sie bekommen also von Woche zu Woche konkrete Arbeitsaufträge (die Falllösungen). Diese sollen Sie eigenständig bewältigen und alle Probleme und Unklarheiten notieren, die sich im Arbeitsprozess ergeben. Die gemeinsamen Sitzungen dienen dann dazu, Lösungswege zu den Aufgaben zu präsentieren, offene Fragen zu klären, Konzepte vertiefend zu erläutern und die nächsten Schritte vorzubereiten. Für jede Woche werden Lernziele und Arbeitsaufträge definiert. Für die erste Seminarwoche lassen sich als Lernziele festhalten: Sie wissen, wie Sie die aktuellen Versionen von R und RStudio auf Ihrem Computer installieren Sie wissen, wie man R-Pakete installiert und in R lädt Sie können eine Funktion aufrufen Sie haben einen soliden ersten Eindruck, wie man mit R kommuniziert und einfache Operationen durchführt Sie haben eine erste Orientierung zu Unterstützungsangeboten, die man online findet (auch wenn diese teilweise noch überfordernd wirken) 1.5 Aufgaben der ersten Woche Installieren Sie die aktuellen Versionen von R und RStudio auf Ihrem Endgerät! Sie sollten sich Notizen machen, wenn es Probleme gibt  und für das nächste Mal gleich festhalten, wie Sie diese gelöst haben. Da die Details der Installation vom Betriebssystem und den Spezifikationen des Endgeräts abhängen, ist es normal, dass dieser Prozess manchmal erst auf den zweiten Versuch funktioniert. Mittels der Funktion version (ACHTUNG: ohne Klammern) lässt sich die Version von R abrufen. Ob dies der aktuellsten Version von R entspricht lässt sich auf der R Projektseite überprüfen. RStudio lässt sich über die Menüsteuerung updaten: Help &gt; Check for Updates. Wichtig: RStudio als grafische Benutzeroberfläche ist nicht dasselbe wie R. Ein Update von RStudio ist also nicht nicht gleich ein Update von R, sondern letzteres muss manuell erfolgen. Das Paket installr und dessen Funktion updateR() ermöglicht auf Windows dass sowohl R als auch die installierten Pakete geupdated werden. Ebenfalls bietet RStudio über die Menüsteuerung eine Möglichkeit, die Pakete zu installieren (Tools&gt;Check for Packages Updates). Zwei Unklarheiten, die aufgetaucht sind: Es sind mehrere Versionen von R auf meinen Computer installiert Ist das ein Problem? Nein, denn RStudio arbeitet automatisch mit der neusten Version. Aber es können auch ältere Versionen verwendet beziehungsweise zwischen den Versionen gewählt werden (siehe Tools &gt; Global Options &gt; General &gt; R Version). Was passiert mit meinen installierten Paketen? Die installierten Pakete bleiben grundsätzlich erhalten. Mittels der Funktion .libPaths() sehen Sie auch, wo diese installiert sind.2 Weiter ist der Umgang mit Paketen kein wirkliches Problem. Da Sie jeweils in Ihrem Code auch spezifizieren, welche Pakete Sie installieren und laden, würde Ihnen ein Fehler sofort auffallen. Dies kann zum Beispiel wiefolgt gemacht werden als Kode in einem Markdown: #install.packages(&quot;swirl&quot;) library(swirl) swirl() #install.packages(&quot;soc.ca&quot;) library(soc.ca) soc.mca(Datenset) Der install.packages()-Befehl ergibt eine Fehlermeldung beim sogenannten knitten, wenn dieser nicht als Kommentar formatiert ist. Der Fehler verweist darauf, dass R kein Repository via Markdown automatisch aufrufen kann. Zwei Möglichkeiten von Josias Bruderer, dies trotzdem zu umgehen (und den install.packages()-Befehl nicht einfach als Kommentar zu setzen): Eine Möglichkeit den Fehlerbefehl auszuschalten ist es, ein Repository anzugeben: install.packages(&quot;swirl&quot;, repos = &quot;https://cran.rstudio.com/&quot;) Eine andere und erweiterte Möglichkeit ist einen Befehl zu ergänzen, der jeweils nur dann ein Paket installiert, wenn dieses benötigt und noch nicht installiert ist: if (!require(&quot;swirl&quot;)) install.packages(&quot;swirl&quot;)   Installieren Sie das Paket swirl und laden Sie es. swirl ist eine in R implementierte interaktive Einführung in die Grundlagen von R! Hier ein Tipp von Julien Lattmann: Sollte dies nach dem Update von R und RStudio womöglich nicht funktioniert haben, dann lohnt es sich nochmals alles zu schliessen, ein paar Moment zu warten und dann nochmals neu zu probieren.   Rufen Sie die Funktion swirl() auf und spielen Sie ein wenig damit. Rufen Sie sich in Erinnerung, was Sie aus dem letzten Semester noch über die Arbeit mit R wissen! Notieren Sie sich, was Ihnen Sie noch kennen, was Ihnen neu vorkommt, und so weiter. Grundsätzlich ging es in dieser Teilaufgabe darum, einige Aspekte von R (erneut) kennenzulernen. Zwei zusammenhängende Dinge sollen hervorgehoben werden: Erwähnt wurde, dass das Programmieren verlangt, sehr genau zu schreiben und kleinste Ungenauigkeiten zu Fehlern führen (etwa ein fehlendes \"). Hier bietet RStudio eine Hilfe an, in dem der Kode farbig gekennzeichnet wird. Diese Hilfe war jedoch um Umgang mit swirl noch nicht ersichtlich, da im Paket direkt in der R Konsole gearbeitet wird. Dies führt zum zweiten Aspekt: Dieser Fokus auf die Konsole beim Paket swirl() ist eher die Ausnahm - und der Hauptteil des Kondierens erfolgt eigentlich immer im einem Skript beziehungsweise in einem Markdown-Dokument (siehe WP02).   Verwenden Sie ein wenig Zeit darauf, online nach R Tutorials, Foren, etc. zu suchen. Halten Sie die URLs von Seiten und Ressourcen fest, die Ihnen hilfreich und/oder wichtig vorkommen (aber unter Umständen noch etwas schwer zu durchschauen) ! Einige Hilfseiten aus Ihren Falllösungen: r-statistics.co: Einführung zu spezifischen Verfahren R Tutorial: viele kurze einführende Beiträge zu Aspekten von R. R Statistik Tutorial: Deutsche Beiträg zu einzelnen Verfahren. RStudio Community: Forum für RStudio R lernen: einige einführende Beiträge auf Deutsch R Forum: deutschsprachiges Forum zu R Einführung in R: ein weiteres, deutsches Bookdown als Einführung in R The R Graph Gallery: für fortgeschrittene Grafiken, insbesondere mit ggplot2 stack overflow: jede Menge Hilfe für alles um R (und Programmieren allgemein) github: eine andere Plattform  aber wiederum: jede Menge Hilfe für alles um R (und Programmieren allgemein) datacamp: kostenpflichte Tutorials für R 1.6 Ergänzung: Standardabweichung zwischen Grundgesamtheit und Stichprobe In der Einheit haben wir gesehen, dass sich eine von R berechnet Standardabweichung leicht von einer selbst berechneten Variante unterscheidet: y &lt;- c(2, 3, 4, 7, 8, 9) sd(y) ## [1] 2.880972 sqrt( sum((y - mean(y))^2) / (length(y)) ) ## [1] 2.629956 In der Hilfeseite der Funktion finden wir den Hinweis darauf, dass mit sd() ähnlich wie mit var() der sogennante unbiased estimator berechnet wird, der von n - 1 ausgeht. help(sd) Like var this uses denominator n - 1. [] The denominator n - 1 is used which gives an unbiased estimator of the (co)variance for i.i.d. observations. Die Standardabweichung wird berechnet, als würde man den Schätzwert für einer Grundgesamtheit aus einer Stichprobe berechnet (i.i.d = Independent and identically distributed random variables). Dies entspricht einer sogenannten Punkteschätzung (Diaz-Bone 2019, 155f): Bei der Punktschätzung errechnet man aus der Stichprobe einen Stichprobenkennwert und schätzt damit die entsprechende Maßzahl in der Grundgesamtheit. Der Stichprobenkennwert, mit dem die Schätzung erfolgt, heißt auch Schätzer. [] Die Standardabweichung eines metrischen Merkmals in der Stichprobe s ist dagegen keine erwartungstreue Schätzung für die Standardabweichung des metrischen Merkmals in der Grundgesamtheit. Aus einer Stichprobe kannman aber mit folgender Formel erwartungstreu schätzen: \\({\\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}}\\) Dabei wird n - 1 verwendet, da bei kleineren Stichprobenumfängen die Standardabweichung überschätzt wird. Bei grösseren Stichprobenumfängen wird dies dann immer weniger wichtig: z &lt;- rnorm(1000) #eine standardnormalverteilte Varialbe mit n = 50 sd(z) ## [1] 1.014677 sqrt( sum((z - mean(z))^2) / (length(z)) ) ## [1] 1.014169 References "],["wochenplan-02-grundlagen-i.html", "2 Wochenplan 02: Grundlagen I 2.1 Lernziele 2.2 Aufgaben", " 2 Wochenplan 02: Grundlagen I im Rahmen der 02. und 03.Einheit. 2.1 Lernziele In der zweiten Seminarwoche geht es darum, die Grundlagen von R und RStudio zu repetieren und zu erweitern.3 Für den weiteren Verlauf wollen wir R als Sprache auffassen  sowohl als Programmiersprache als auch als Sprache in einem metaphorischen Sinn. Wir wollen also ein komplexes System zur Kommunikation kennenlernen. Wie bei einer anderen Sprache gibt es auch hier Zeichen mit Bedeutungen (ähnliche wie Nomen, Verben, . . . ) und Regeln zur Verknüpfung dieser Zeichen (ähnlich wie eine Grammatik). Diese Grundlagen gilt es alle erstmal kennenzulernen und zu verstehen. Am Anfang wird vieles schwer fallen, mit der Zeit gewinnt man aber Sicherheit. Der zentrale Punkt in dieser Vorstellung von R als Sprache ist dabei folgender: Wir lernen eine Sprache dadurch, dass wir sie immer wieder anwenden, Probleme lösen und vor allem auch Fehler machen. R als Software und als Programmiersprache hat eine steile Lernkurve und zu Beginn werden viele Probleme auftauchen. Im Umgang mit den Problemen soll allerdings auch eine eigene Arbeitsweise mit dem Programm erlernt werden (Fehlermeldungen lesen, Lösungsstrategien im Codieren erlernen, selber Hilfe suchen, . . . ). Für die beständige Erweiterung der Grundlagen und das Erlernen der Sprache R besteht eine Herausforderung darin, den Weg zwischen scheinbarer Trivialität und überfordernder Komplexität zu finden: Es gilt die kleinen Schritte ernstzunehmen, sonst werden die grossen Schritte sehr schnell mühsam. Für die zweite Seminarwoche lassen sich folgende Seminarziele festhalten: Sie können die verschiedenen Funktionsweisen der vier Fenster in RStudio erläutern. Sie verstehen den Unterschied zwischen der Arbeit in der Konsole und im Skript. Sie verstehen, wie und wozu man im Skript kommentiert. Sie haben R-Markdown als erweitertes Skript und Arbeitsinstrument kennengelernt. Sie wissen, was ein Arbeitsverzeichnis in R ist und wozu es gut ist. Sie verstehen das erste Grundelement der Sprache R: Funktionen Sie wissen, wie Funktionen aufgebaut sind Sie wissen, wie Sie sich Hilfe zu Funktionen holen; Sie wissen, was Argumente in einer Funktion bewirken. Sie verstehen das zweite Grundelement der Sprache R: Objekte Sie verstehen, was es bedeutet, dass in R alles ein Objekt ist; Sie wissen, wie man sich die jeweils aktuell verfügbare Objekte anzeigen lässt; Sie haben das Zusammenspiel von Funktionen und Objekte kennengelernt; Sie kennen bereits drei verschiedenen Arten von Objekten. 2.2 Aufgaben Fassen Sie noch einmal für sich und in eigenen Worten die Funktionen der vier Fenster von RStudio zusammen. Oben links findet sich in R-Studio das Skript-Fenster, in dem Befehle eingegeben und kommentiert werden können. Ausgeführt werden diese Befehle erst, wenn Sie Ctrl und Enter drücken (bzw. Cmd &amp; Enter). Diese Eingabe von Kodezeilen wird ergänzt durch die direkte Eingabe in der Konsole. In diesem Fenster läuft das eigentliche Programm R (es ist also dieselbe Ansicht wie wenn Sie R ohne grafische Benutzeroberfläche starten würden).4 Im Gegensatz zum Skript können hier Befehle nur immer einzeln eingegeben und sie müssen dann direkt ausgeführt werden. Dies ermöglicht ein schneller ausprobieren, aber eben kein wirklich speichern, beständiges überarbeiten, kommentieren und eine klare Dokumentation des Ablaufs, wie dies im Skript erfolgen kann. Die beiden Fenster zur Eingabe von Kode werden vom Environment-Fenster ergänzt. Hier finden sich die abgespeicherten Objekte sowie in den weiteren Reitern die bisher ausgeführten Befehle (History), aber auch eine erweitere Netzwerk- bzw. Serverumgebung (Connections, Build, ), falls Sie z.Bsp. mit weiteren Personen an einem Projekt arbeiten. Im vierten Fenster werden Grafiken, Hilfeseiten, die Vorschau für geknittete Dokumente und auch die Ordnerstruktur angezeigt. Sie finden diverse Einstellungsoptionen zu den vier Fenster und deren Anordnung unter Tools &gt; Gobal Options. Dort können Sie etwa unter dem Reiter General die Option zum Save Workspace to RData on exit zu Never wechseln. Dies führt dazu, dass Ihre Environment beim Verlassen von R immer gelöscht wird. Dies ist nicht etwa ein Nachteil, sondern eine Technik die Sie dazu veranlasst, alle benötigen Schritte in Ihrem Kode unterzubringen. Weiter könne Sie unter dem Reiter Spelling auch noch die Rechtschreibefunktion deaktiveren, da das Feature noch nicht wirklich für die deutsche Rechtschreibung zu funktionieren scheint (bzw. die Ergänzung von neuen Wörterbüchern nicht fehlerfrei abläuft). Die Ordnerstruktur im Reiter Files des vierten Fensters hängt mit Ihrem aktuellen Arbeitsverzeichnis zusammen. Ein Arbeitsverzeichnis ist der Ort, auf den R immer als Erstes zugreift und wo Dinge automatisch abgelegt werden. Dieses können über die Menüsteuerung Session &gt; Set Working Directory &gt; Choose Directory oder über den Befehl setwd() definieren (erstere Variante ist etwas einfacher). Der getwd() Befehl wiederum gibt das aktuell festgelegte Verzeichnis aus. Speichern Sie jeweils Ihr aktuelles Arbeitsverzeichnis als Teil des Markdowns, z.Bsp. so: setwd(&quot;C:/Users/SchweglG/R_Daten/HS20/E3&quot;) #Dies dient in einem Skript oder einem Markdown als Erinnerung, ... #...wo Ihr Arbeitsverzeichnis liegt (und damit wo Sie Ihre Daten wiederfinden)   Was ist der Vorteil der Arbeit im Skript gegenüber dem Schreiben von Code direkt in der Konsole? Und was könnten dann die Vorteile davon sein, mit R Markdown zu arbeiten? Und wann könnten Sie trotzdem besser mit einem klassischen Skript arbeiten? Schauen Sie sich auch die Formatierungsmöglichkeiten für Fliesstext in den Cheatsheets zu R Markdown an (siehe hier)! Vorteile der Arbeit im Skript: Da Kodezeilen nicht sofort ausgeführt werden müssen ermöglicht das Skript die Strukturierung und Abspeicherung von Befehlen (Delia Bazzigher). Ein weiterer Vorteil kann es sein, komplexe Funktion, die man z.B. in einem RMarkdown braucht, in einem Skript zu hinterlegen, um im RMarkdown nicht zu viel Code einzubauen, damit sich die Leser:innen nicht darum kümmern müssen (Fabio Keller). Vorteile der Arbeit in RMarkdown: RMarkdown bietet die Möglichkeit, Kodezeilen (die Chunk) auf vielfältige Weise mit Text zu ergänzen und diesen Text zu formatieren: bspw. Bold, Italics, Aufzählungszeichen, Übertitel in unterschiedlichen Grössen, etc. []. Auch zwingt es den User, genau zu arbeiten, da [ein RMarkdown] geschlossen funktioniert (bspw. ein Objekt kann sich im Environment befinden, aber nicht im Universum des Markdowns) (Valentina Meyer). Vorteile der Konsole: In der Konsole kann über die beiden Pfeiltasten runter und rauf durch bisher ausgeführte Befehle gescrollt werden. So können Sie die Rechnung erneut aufrufen und dem Objekt x zuweisen. Ebenfalls können in der Konsole Dinge ausprobiert oder Hilfefunktionen aufgerufen werden.   Öffnen Sie eine neue R Markdown Datei. Versuchen Sie Ihre bisherigen Notizen zu den Aufgaben und Ihren Code in dieser Datei unterzubringen (falls Sie dies nicht schon gemacht haben). Arbeiten Sie für die folgenden Aufgaben mit dieser Datei weiter.   Versuchen Sie allgemein zu beschreiben, was Funktionen und was Objekte sind. Fügen Sie eine kurze Erläuterung in Ihr Markdown-Dokument zur Frage ein, was im folgenden Code jeweils Funktionen und was Objekte (und wenn letzteres, welche Art von Objekt) sind: sqrt(x) Funktion(en): sqrt() Objekt(e): x - eine undefiniertes Objekt help(&quot;sqrt&quot;) Funktion(en): help() Objekt(e): sqrt() - ein Funktion als Objekt, hier im Charakterformat ausgeschrieben Warum erfolgt jetzt die Schreibweise von sqrt in der Art und Weise? Gehen wir dazu kurz in die Hilfefunktion selber: ?help Funktion(en): ? als andere Schreibweise von help() Objekt(e): help() - ein Funktion als Objekt, hier aber nicht im Charakterformat ausformuliert. Dort finden wir die Spezifizierung bei Topic: usually, a name or character string specifying the topic for which help is sought. A character string (enclosed in explicit single or double quotes) is always taken as naming a topic. Was heisst jetzt ein Charakter-String? Es gibt eben nicht nur numerische Daten x1 &lt;- 5 sondern auch textliche oder Charakterdaten x2 &lt;- &quot;fünf&quot; class(x1) ## [1] &quot;numeric&quot; typeof(x1) ## [1] &quot;double&quot; class(x2) ## [1] &quot;character&quot; typeof(x2) ## [1] &quot;character&quot; Warum zeigt R uns zwei verschiedene Merkmale bei x1 an? R speichert Zahlen auf eine bestimmte Art und Weise ab, nämlich normalerweise als sogennante doubles oder als Zahlen mit Nachkommastellen. Man könnte die 5 auch als Integer (also als Zahl ohne Kommata) abspeichern: x3 &lt;- 5L class(x3) ## [1] &quot;integer&quot; typeof(x3) ## [1] &quot;integer&quot; Das brauchen wir allerdings kaum im alltäglichen Umgang mit R. y &lt;- c(1, 3, 4, 5, 6, 7, NA) #und z &lt;- c(7, 8, 10, 11) Funktion(en): &lt;-, c() Objekt(e): y, z - ein Vektor; 1, 3, 4, 5, 6, 7, 7, 8, 10, 11 - jeweils numerische Objekte; NA - ein fehlender Wert Berechnen des arithmetischen Mittels: (7+ 8+ 10+ 11) / 4 ## [1] 9 sum(z) / length(z) ## [1] 9 mean(z) ## [1] 9 #Was passiert hier? mean(y) ## [1] NA mean(y, na.rm = T) ## [1] 4.333333 Hier haben wir wiederum ein Argument einer Funktion spezifiziert, um die fehlenden Werte auszuklammern. Wichtig: NA ist jetzt nicht dasselbe wie ein Charakter-Datum NA: y1 &lt;- c(1, 3, 4, 5, 6, 7, NA) y2 &lt;- c(1, 3, 4, 5, 6, 7, &quot;NA&quot;) mean(y1, na.rm = TRUE) ## [1] 4.333333 mean(y2, na.rm = TRUE) ## Warning in mean.default(y2, na.rm = TRUE): Argument ist weder numerisch noch ## boolesch: gebe NA zurück ## [1] NA und was ist jetzt mit dem Vektor y1 passiert? Wir sehen, dass Vektoren nur ein Datenformat enthalten können. Ansonsten müssten wir den Vektor als Liste speichern. Listen sind daher ein weiterer Objekttyp (mit dem wir aber nur selten arbeiten werden). is.logical(TRUE) Funktion(en): is.logical() Objekt(e): TRUE - ein logisches Datenformat Was ist TRUE? typeof(TRUE) ## [1] &quot;logical&quot; class(TRUE) ## [1] &quot;logical&quot; TRUE und FALSE Dies sind logische Daten  also Daten die angeben ob etwas wahr oder falsch ist. Wir können auch daraus einen Vektor machen: lv &lt;- c(TRUE, TRUE, FALSE, FALSE, TRUE) Und was passiert wohl hierbei? TRUE + FALSE ## [1] 1 Es sind also einfach auch Werte: 1 oder 0. mean(y1, na.rm = 1) ## [1] 4.333333 Zusammenfassung Wir haben jetzt bereits drei verschiedene Datenarten kennengelernt: numerische Daten Charakterdaten logische Daten. Weiter haben wir bereits drei verschiedene Objekttypen kennengelernt: einzelne Werte Vektoren (als eine Reihe von Werten derselben Datenart) Listen (also Reihe von Werten derselben Datenart) Diese drei Objekttypen werden in der 5. Aufgabe ergänzt von den sogenannten Matrizen.   Sehen Sie sich die Hilfeseite der Funktion matrix() an. Wozu dient diese Funktion? Welche Argumente akzeptiert / benötigt sie und wozu dienen diese? Illustrieren Sie die Funktionsweise anhand von einem Beispiel. Die Funktion matrix() erlaubt es uns ein zweidimensionales Objekt aus einem Set von Werten (einem Vektoren) zu erstellen: Ein Beispiel von Vanesse Leutener: Beispiel: Bei einer kleinen Umfrage wurden 6 Befragte nach Geschlecht und der Höhe des monatlichen Einkommens befragt. Geschlecht 1 = weiblich 2 = männlich 3 = divers Monatliches Einkommen 1 = weniger als 800 CHF 2 = ab 800-1500 CHF 3 = ab 1500-3000 CHF 4 = ab 3000-6000 CHF 5 = mehr als 6000 CHF Geschlecht &lt;- c(2, 3, 1, 1, 1, 2) Einkommen &lt;- c(3, 5, 4, 1, 5, 5) #so? matrix(Geschlecht, Einkommen) ## [,1] [,2] ## [1,] 2 1 ## [2,] 3 1 ## [3,] 1 2 #so? matrix(c(Geschlecht, Einkommen)) ## [,1] ## [1,] 2 ## [2,] 3 ## [3,] 1 ## [4,] 1 ## [5,] 1 ## [6,] 2 ## [7,] 3 ## [8,] 5 ## [9,] 4 ## [10,] 1 ## [11,] 5 ## [12,] 5 #so sollte das passen: matrix(c(Geschlecht, Einkommen), nrow = 6, ncol = 2) ## [,1] [,2] ## [1,] 2 3 ## [2,] 3 5 ## [3,] 1 4 ## [4,] 1 1 ## [5,] 1 5 ## [6,] 2 5 #Warum stimmt das nicht? matrix(c(Geschlecht, Einkommen), nrow = 6, ncol = 2, byrow = T) ## [,1] [,2] ## [1,] 2 3 ## [2,] 1 1 ## [3,] 1 2 ## [4,] 3 5 ## [5,] 4 1 ## [6,] 5 5 Bei der Anordnung von Zeilen und Spalten wollen wir jeweils Fälle/Personen als Zeilen und Variablen als Spalten darstellen. Und wie könnte wir diese Matrix beschriften? LV_G &lt;- matrix(c(Geschlecht, Einkommen), nrow = 6, ncol = 2) colnames(LV_G) &lt;- c(&quot;Geschlecht&quot;, &quot;Einkommen (Kat.)&quot;) rownames(LV_G) &lt;- c(&quot;F1&quot;, &quot;F2&quot;, &quot;F3&quot;, &quot;F4&quot;, &quot;F5&quot;, &quot;F6&quot;) LV_G ## Geschlecht Einkommen (Kat.) ## F1 2 3 ## F2 3 5 ## F3 1 4 ## F4 1 1 ## F5 1 5 ## F6 2 5 Ein Beispiel von Delia Bazzigher: Eine Studie untersucht die mediale Präsenz der aktuellen Geschehnisse in Afghanistan in der deutschsprachigen Schweiz im September. Dazu werden z.B. die Tageszeitungen untersucht. Es wird eine Häufigkeitszählung in den Zeitungen Tages-Anzeiger, NZZ, Luzerner Zeitung und Blick durchgeführt. Gezählt werden die Anzahl Artikel sowie deren jeweilige Position, d.h. neutral/sachlich, negati/verurteilend oder positiv/unterstützend (die Codierung dieser Kategorien wird hier nicht erläutert). mTageszeitungen &lt;- matrix(c(7,3,0, 12,7,0, 5,3,0, 8,2,3), nrow = 3, ncol = 4, byrow = FALSE, dimnames = list(c(&quot;negativ&quot;, &quot;neutral&quot;, &quot;positiv&quot;), c(&quot;Tages-Anzeiger&quot;, &quot;NZZ&quot;, &quot;Luzerner Zeitung&quot;, &quot;Blick&quot;))) mTageszeitungen ## Tages-Anzeiger NZZ Luzerner Zeitung Blick ## negativ 7 12 5 8 ## neutral 3 7 3 2 ## positiv 0 0 0 3 Was zeigt sich hier für ein Datenformat? Das ist eigentlich bereits abstrahiert! Wir könnten diese Kombination und das erstellen einer Matrix auch mit anderen Funktionen erreichen: LV_G1 &lt;- cbind(Geschlecht, Einkommen) LV_G2 &lt;- rbind(Geschlecht, Einkommen) LV_G1 ## Geschlecht Einkommen ## [1,] 2 3 ## [2,] 3 5 ## [3,] 1 4 ## [4,] 1 1 ## [5,] 1 5 ## [6,] 2 5 LV_G2 ## [,1] [,2] [,3] [,4] [,5] [,6] ## Geschlecht 2 3 1 1 1 2 ## Einkommen 3 5 4 1 5 5 typeof(LV_G1) ## [1] &quot;double&quot; class(LV_G1) ## [1] &quot;matrix&quot; &quot;array&quot; Was ist noch ein spezifische Eigenschaft einer Matrix? Geschlecht_c &lt;- c(&quot;m&quot;, &quot;d&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;) LV_G3 &lt;- cbind(Geschlecht_c, Einkommen) LV_G3 ## Geschlecht_c Einkommen ## [1,] &quot;m&quot; &quot;3&quot; ## [2,] &quot;d&quot; &quot;5&quot; ## [3,] &quot;f&quot; &quot;4&quot; ## [4,] &quot;f&quot; &quot;1&quot; ## [5,] &quot;f&quot; &quot;5&quot; ## [6,] &quot;m&quot; &quot;5&quot; typeof(LV_G3) ## [1] &quot;character&quot; class(LV_G3) ## [1] &quot;matrix&quot; &quot;array&quot; Matrizen können nur ein Datenformat speichern - und wandeln daher verschiedene Datenarten in das Niedrigste um. Deshalb arbeiten wir eher mit dem Datenformat bzw. Objekttyp des sogennanten data.frames (oder in ggplot dann tybbles). Dieser Objekttyp ermöglicht ein Speichern von verschiedenen Datenarten in einem zweidimensionalen Format: LV_G4 &lt;- data.frame(Geschlecht_c) LV_G4 &lt;- cbind(LV_G4, Einkommen) LV_G4 ## Geschlecht_c Einkommen ## 1 m 3 ## 2 d 5 ## 3 f 4 ## 4 f 1 ## 5 f 5 ## 6 m 5 typeof(LV_G4) ## [1] &quot;list&quot; class(LV_G4) ## [1] &quot;data.frame&quot; Falls Sie noch Probleme bei der Installation von R bzw. RStudio oder sonstige technische Schwierigkeiten haben, können Sie sich gerne weiterhin per E-Mail melden! Neuere Versionen von RStudio bieten im Fenster der Konsole auch die Möglichkeit, sogenannte Jobs auszuführen (ein Tpp von Josias Bruderer), siehe [hier]:(https://www.youtube.com/watch?v=EBlk1kRbKeU). "],["wochenplan-03-grundlagen-ii.html", "3 Wochenplan 03: Grundlagen II 3.1 Lernziele 3.2 Aufgaben 3.3 Ergänzung: Seitenumbruch im RMarkdown", " 3 Wochenplan 03: Grundlagen II im Rahmen der 03. und 04.Einheit. 3.1 Lernziele In der dritten Seminarwoche vertiefen wir Elemente der Programmiersprache R, die Sie bereits kennengelernt haben, und betten sie in neue Zusammenhänge ein. Vertieft wird nochmals der Umgang mit verschiedenen Datenarten und Objekttypen: Zahlen, Text und logische Werte sowie einzelne Werte, Vektoren und Matrizen. Neu wollen wir einige Möglichkeiten kennenlernen, über bestimmte Funktionen systematisch Vektoren zu definieren. Diese Grundlagen wollen wir dann nutzen, um erste statistische Inhalte bzw. Methoden einführen, nämlich die bivariaten Zusammenhänge von metrischen Variablen (Korrelation). Zusammenfassend lassen sich damit folgende Seminarziele festhalten: Sie können die drei bisher kennengelernten Objekttypen (einzelne Zahl, Vektor, und Matrix) kombinieren. Sie kennen die drei verschiedenen Datenarten von R: Sie verstehen die Rolle von numerischen Daten, Sie verstehen die Rolle von textförmigen Daten Sie verstehen die Rolle von logischen Daten. Sie können Vektoren mittels der Funktionen   seq()  sowie rep() definieren  und diese auf alle drei Arten von Daten anwenden. Sie verstehen, was eine Korrelation von zwei Variablen bedeutet und können bivariate Zusammenhänge in R berechnen und interpretieren. 3.2 Aufgaben Erstellen Sie vier verschiedene Vektoren mit je einer Länge von vier und verbinden Sie diese zu einer 4x4-Matrix. Die Funktion apply() erlaubt Ihnen, eine Funktionen wie z.B. mean() oder var() auf diese Matrix anzuwenden. Nur wie genau? Sehen Sie sich die Hilfe zu apply() an, probieren Sie die Funktion aus und versuchen Sie zu verstehen, wie sie genau funktioniert. Erläutern Sie apply() dann in eigenen Worten und mit Hilfe der von Ihnen erzeugten 4x4-Matrix! Zuerst können vier Vektoren erstellt werden: m1 &lt;- c(2,4,7,9) m2 &lt;- c(8,9,5,2) m3 &lt;- c(3,5,5,5) m4 &lt;- c(9,8,7,6) Nachdem die vier Vektoren erstellt wurden lassen sie sich über die rbind() und cbind() Funktionen auf zwei verschiedene Weisen zu einer Matrix verbinden, entweder zeilen- oder spaltenweise (siehe die Objekte ma und mb). ma &lt;- cbind(m1,m2,m3,m4) mb &lt;- rbind(m1,m2,m3,m4) ma ## m1 m2 m3 m4 ## [1,] 2 8 3 9 ## [2,] 4 9 5 8 ## [3,] 7 5 5 7 ## [4,] 9 2 5 6 mb ## [,1] [,2] [,3] [,4] ## m1 2 4 7 9 ## m2 8 9 5 2 ## m3 3 5 5 5 ## m4 9 8 7 6 Als nächster Schritt wendet apply() dann eine bestimmte Funktion auf ein Objekte an. Bei einem Objekt des Typs Matrix muss allerdings noch spezifiziert werden, ob die Funktion Zeilen- oder Spalteweise angewendet wird. Zeilen oder Spalten werden über die Zahlen 1 bzw. 2 definiert. apply(ma, 1, mean) ## [1] 5.5 6.5 6.0 5.5 apply(ma, 2, mean) ## m1 m2 m3 m4 ## 5.5 6.0 4.5 7.5 apply(mb, 1, mean) ## m1 m2 m3 m4 ## 5.5 6.0 4.5 7.5 apply(mb, 2, mean) ## [1] 5.5 6.5 6.0 5.5 So ergeben sich insgesamt vier Möglichkeiten, für die Berechnung der des Durchschnitts  allerdings finden sich nur zwei unterschiedliche Ergebnisse. Was passiert nun hier? apply(ma, c(1,2), mean) ## m1 m2 m3 m4 ## [1,] 2 8 3 9 ## [2,] 4 9 5 8 ## [3,] 7 5 5 7 ## [4,] 9 2 5 6 ma ## m1 m2 m3 m4 ## [1,] 2 8 3 9 ## [2,] 4 9 5 8 ## [3,] 7 5 5 7 ## [4,] 9 2 5 6 apply(ma, c(1,2), var) ## m1 m2 m3 m4 ## [1,] NA NA NA NA ## [2,] NA NA NA NA ## [3,] NA NA NA NA ## [4,] NA NA NA NA Hier wird eine Funktion sowohl auf Spalten als auch auf Zeilen angewendet - das heisst einfach auf die einzelnen Werte. Von einem einzelnen Wert kann man dann das arithmetische Mittel berechen, hingegen nicht die Varianz: var(2) ## [1] NA mean(2) ## [1] 2 Als Ergänzung finden Sie hier noch ein sozialwissenschaftliches Beispiel einer solchen Matrix von Katrin Oesch: Alter &lt;-c(32,61,45,29) Arbeitsjahre &lt;- c(14,33,20,2) Monatseinkommen &lt;- c(5500,8700,10200,3750) Ausbildungsjahre &lt;- c(3,12,9,7) DS_1 &lt;-c(Alter, Arbeitsjahre, Monatseinkommen, Ausbildungsjahre) Matrix_1 &lt;- matrix(DS_1, nrow = 4, ncol = 4, byrow = F) colnames(Matrix_1) &lt;- c(&quot;Alter&quot;,&quot;Ausbildungsjahre&quot;,&quot;Monatseinkommen&quot;,&quot;Ausbildungsjahre&quot;) apply(Matrix_1, MARGIN=2, FUN = mean) ## Alter Ausbildungsjahre Monatseinkommen Ausbildungsjahre ## 41.75 17.25 7037.50 7.75   Erstellen Sie je einen Vektor mit numerischen Daten, textförmigen Daten und logischen Daten. Die Funktionen as.numeric(), as.character() und as.logical() lassen Sie eine Datenarten in eine andere zwingen bzw. als eine andere Datenart interpretieren. Wann funktioniert dies? Und wo sind die Grenzen dieses Zwingens? a &lt;- c(0, 1, 2) b &lt;- c(TRUE, FALSE, T) #Die logischen Objekte können sowohl ausgeschreiben als auch lediglich als T und F aufgeführt werden c &lt;- c(&quot;null&quot;, &quot;eins&quot;, &quot;zwei&quot;) #die Anführungs- und Schlusszeichen beachten Nachdem wir die Vektoren definiert haben, können wir deren Typ bestimmen und mit den as.-Funktionen spielen: is.numeric(a) ## [1] TRUE is.logical(b) ## [1] TRUE is.character(c) ## [1] TRUE as.numeric(b) ## [1] 1 0 1 as.numeric(c) ## Warning: NAs durch Umwandlung erzeugt ## [1] NA NA NA as.logical(a) ## [1] FALSE TRUE TRUE as.logical(c) ## [1] NA NA NA as.character(a) ## [1] &quot;0&quot; &quot;1&quot; &quot;2&quot; as.character(b) ## [1] &quot;TRUE&quot; &quot;FALSE&quot; &quot;TRUE&quot; Die Grenzen dieses Zwingens der as.-Funktionen finden sich auf der einen Seite im Zusammenhang zu den logischen Daten: Dies funktioniert nur mit numerischen Daten  und alles über 1 wird als TRUE interpretiert. Auf der anderen Seite zeigt sich die Grenze bei ausgeschriebenen Zahlen in einem Charakter-Vektor. Diese können von R nicht in numerische oder auch logische Daten umformuliert werden. Allerdings gilt es eine wichtige Ausnahme zu beachten, nämlich wenn Zahlen als Charakter aufgeführt wurden: c2 &lt;- c(&quot;0&quot;, &quot;1&quot;, &quot;2&quot;) is.character(c2) ## [1] TRUE as.numeric(c2) ## [1] 0 1 2 as.logical(as.numeric(c2)) ## [1] FALSE TRUE TRUE Insbesondere beim Importieren von Datensätzen, die nicht in einem Rohformat gespeichert sind (etwa .sav Dateien), kann es passieren, dass metrische Variablen als Charakter-Vektor eingelesen werden.   Definieren Sie folgende Vektoren mittels der Funktionen seq() und rep(): Vektor 3a: 1 2 3 4 5 6 7 8 9 10 seq(from = 1, to = 10, by = 1) ## [1] 1 2 3 4 5 6 7 8 9 10 #oder einfacher noch: seq(1, 10, 1) ## [1] 1 2 3 4 5 6 7 8 9 10 Vektor 3b: 1 1 1 2 2 2 3 3 3 c(rep(1,3), rep(2,3), rep(3,3)) ## [1] 1 1 1 2 2 2 3 3 3 Vektor 3c: Die Zahlen des Vektors b als ausgeschriebene Wörter c(rep(&quot;Eins&quot;,3), rep(&quot;Zwei&quot;,3), rep(&quot;Drei&quot;,3)) ## [1] &quot;Eins&quot; &quot;Eins&quot; &quot;Eins&quot; &quot;Zwei&quot; &quot;Zwei&quot; &quot;Zwei&quot; &quot;Drei&quot; &quot;Drei&quot; &quot;Drei&quot; Vektor 3d: 1 4 7 10 13 seq(1, 13, 3) ## [1] 1 4 7 10 13 Vektor 3e: 1 4 9 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 1100 14899 678999 V3e &lt;- c(1, 4, 9, seq(50, 100, 1), 1100, 14899, 678999) V3e ## [1] 1 4 9 50 51 52 53 54 55 56 ## [11] 57 58 59 60 61 62 63 64 65 66 ## [21] 67 68 69 70 71 72 73 74 75 76 ## [31] 77 78 79 80 81 82 83 84 85 86 ## [41] 87 88 89 90 91 92 93 94 95 96 ## [51] 97 98 99 100 1100 14899 678999   Was macht das Argument trim für die Funktion mean()? Wozu könnten Sie dieses Argument in einem sozialwissenschaftlichen Kontext nutzen? Spezifizieren Sie das Argument auf sinnvolle Weise, um das arithmetische Mittel des in Aufgabe 3e erstellten Vektors zu berechnen! Mittels des Arguments trim können bestimmte Anteile der Elemente eines Vektors für die Berechnung entfernt werden. Dies ermöglicht es Extremwerte (sowohl sehr hohe als auch sehr tiefe Werte) aus der Berechnung zu entfernen. mean(V3e) ## [1] 12260.3 mean(V3e, trim = 0.1) ## [1] 75 Was erfolgt nun genau mit dem Wert 0.1? Eine Formel von Fabio Keller: V3e ## [1] 1 4 9 50 51 52 53 54 55 56 ## [11] 57 58 59 60 61 62 63 64 65 66 ## [21] 67 68 69 70 71 72 73 74 75 76 ## [31] 77 78 79 80 81 82 83 84 85 86 ## [41] 87 88 89 90 91 92 93 94 95 96 ## [51] 97 98 99 100 1100 14899 678999 #...die zwei kleinsten und grössten Werte 0.05 * length(V3e) ## [1] 2.85 mean(V3e, trim = 0.05) ## [1] 93.09434 #...die drei kleinsten und grössten Werte 0.06 * length(V3e) ## [1] 3.42 mean(V3e, trim = 0.06) ## [1] 75 Warum ändert sich allerdings nichts am Durchschnittswert, egal wie ich trim im Bereich zwischen 0.06 und 0.5 definiere (0.5 ist der Maximalwert)? mean(V3e, trim = 0.1) ## [1] 75 mean(V3e, trim = 0.49) ## [1] 75 Da der Vektor eine Zahlenreihe ist bleibt der Durchschnittswert immer gleich wenn jeweils von den Enden dieselbe Anzahlobjekte entfernt wird.   Sie sollen verschiedene Paare von Vektoren mittels der Funktionen c(), rep() und seq() definieren, die jeweils unterschiedlich korrelieren. Diese Vektoren  d.h. Variablen  und deren Korrelationen sollen sozialwissenschaftlichen Phänomenen entsprechen. Berechnen Sie jeweils den Korrelationskoeffizienten. Ein Paar von Vektoren (mit je einer Länge von rund 100) soll eine Korrelation von ca. 0.4 aufweisen, anhand eines Beispiels von Vanessa Leutner: Freundschaften &lt;- rep(c(2, 3, 10, 4, 1, 6, 3, 3, 3, 4, 4, 1, 9, 4, 2, 8, 5, 6, 3, 4), each = 5) Lebenszufriedenheit &lt;- rep(c(2, 2, 6, 3, 4, 5, 1, 4, 3, 5, 1, 5, 6, 1, 1, 4, 3, 4, 7, 1), each =5) cor(Freundschaften, Lebenszufriedenheit) ## [1] 0.403411 Ein Paar von Vektoren (mit je einer Länge von rund 100) soll eine sehr starke Korrelation aufweisen, anhand eines Beispiels von Dario Haab: VektorA &lt;- seq(1, 100, by = 1) VektorB &lt;- rep(1:50, each = 2) cor(VektorA, VektorB) ## [1] 0.99985 Wie könnte dieses Beispiel sozialwissenschaftlich interpretiert werden? Die Verteilung könnte beispielsweise von einer Firma stammen, die ihre Löhne alle zwei Jahre bei den Mitarbeiter*innen erhöht. Ein Paar von Vektoren (mit je einer Länge von rund 100) soll eine schwache negative Korrelation aufweisen, anhand eines Beispiels von Julien Lattmann: Alter &lt;- rep(seq(15, 65, 2), 4) Nutzungsdauer_Smartphone &lt;- c(rep(seq(120, 30, -10), 3), rep(70, 20), rep(50, 20), rep(30, 20), rep(10, 14)) cor(Alter, Nutzungsdauer_Smartphone) ## [1] -0.1453604 3.3 Ergänzung: Seitenumbruch im RMarkdown In der Einheit wurde noch kurz besprochen, wie es in RMarkdown möglich ist, einen Seitenumbruch für das geknittete PDF einzufügen. Josias Bruderer schlug vor \\newpage im Text einzufügen. Eine weitere Möglichkeiten könnten auch \\pagebreaksein (siehe hier). "],["wochenplan-04-grafiken.html", "4 Wochenplan 04: Grafiken 4.1 Lernziele 4.2 Aufgaben", " 4 Wochenplan 04: Grafiken im Rahmen der 04. und 05.Einheit. 4.1 Lernziele Nachdem wir uns bereits verschiedenste Grundlagen für die Arbeit mit R Studio erarbeitet und erste statistische Inhalte kennengelernt haben stehen nun einige ergänzende Aspekte an. Mittels diesen Aspekten wollen wir uns immer näher an die tatsächliche Arbeit der sozialwissenschaftlichen Datenanalyse bewegen: grafische Techniken sollen ausprobiert, Zufallsvariablen kennengelernt und weitere Datensätze erstellt werden. Der vierte Wochenplan soll uns so nicht zuletzt vorbereiten, das Prinzip der Inferenzstatistik mittels R zu verstehen. Konkret lassen sich folgende Seminarziele festhalten: Sie können zwei Variablen in einem Streudiagramm darstellen und die Darstellungen interpretieren. Sie kennen den Unterschied von Gleichverteilungen und Normalverteilungen und können in R entsprechend verteilte Zufallsvariablen erstellen. Sie können metrische Verteilungen in Histogrammen darstellen. Sie haben erste Techniken kennenglernt, wie Grafiken erweitert und kombiniert werden können. Sie verstehen, was ein Dataframe in R ist und können die Unterschiede zu einer Matrix benennen. Sie haben sich in R die Grundlagen für ein Verständnis von Inferenzstatistik allgemein und des Stichprobenfehlers im Besonderen erarbeitet. 4.2 Aufgaben Nutzten Sie plot() um die Verteilung eines Variablen-Paars darszustellen, das Sie im Rahmen des letzten Wochenplans und der Aufgabe zu den Korrelationen erstellt haben. Verwenden Sie weiter auch eine Farbe für den Plot und verweisen Sie im Titel sowie in den Achsenbeschriftungen auf das sozialwissenschaftliche Phänomen, das Sie darstellen. Als Beispiel dient hier ein Variablenpaar von Delia Bazzigher, das eine schwach-positive Korrelation aufweist: age &lt;- c(11, 16, rep(seq(12, 25, 3), 7), 17, 27, rep(seq(12, 70, 3), 3), 22) screenhours &lt;- c(rep(c(20, 25, 24, seq(23, 70, 3), 19, 20), 4), rep(40:47, 2)) cor(age, screenhours) ## [1] 0.2039158 Zuerst ein simpler Plot plot(age, screenhours) dann wir dann über diverse Elemente erweitern können: help(plot) Implizit haben wir immer schon einen Typ bestimmt, nämlich type= \"p\" davon können wir die Punktform und Grösse bestimmten: pch = &amp; lwd = Titel (auch mit Zeilenumbruch): main = Achsenbeschriftung: xlab = &amp; ylab = Grössen des Textes: cex.lab = Farbe (sowohl über eine schriftliche Bezeichnung als auch über Zahlen): col = () plot(age, screenhours, type = &quot;p&quot;, pch = 15, lwd = 2, main = &quot;Zusammenhang zwischen Alter und \\nZeit, die vor einem Bildschirm verbracht wurde&quot;, xlab = &quot;Alter (j)&quot;, ylab = &quot;Bildschirmzeit (h)&quot;, cex.lab = 1.2, col = c(1:100) )   Erstellen Sie Vektoren für eine Gleichverteilung mittels runif() und für eine Normalverteilung mittels rnorm(). Diese Vektoren sollen als Variablen Körpergrössen repräsentieren. Erstellen Sie die jeweils zwei Vektoren in unterschiedlichen Längen, und zwar m Folgenden werden die in der Aufgabe verlangten Verteilungen als Histogramme dargestellt. Insbesondere bei grösseren Fallzahlen werden so die Eigenschaften der Verteilungen deutlicher: Bei der Gleichverteilung hat jede Ausprägung dieselbe Auftrittswahrscheinlichkeit. Das heisst, dass jede Körpergrösse zwischen Minimal- und Maximalwert (die beiden Parameter der Funktion runif()) mit derselben Häufigkeit vorkommt. Dies entspricht aber nicht der empirischen Realität von Körpergrössen. Bei der Normalverteilung gruppieren sich die meisten Werte um den Mittelwert von 170cm, während kleine und grosse Werte mit zunehmender Abweichung immer weniger häufig auftreten. Dies wiederum entspricht stärker der tatsächlichen, empirischen Verteilungen von Körpergrössen. Unterschieden werden die Normalverteilungen (auch in der Funktion rnorm()) über die beiden expliziten Parameter des arithmetischen Mittels und der Standardabweichung (Diaz-Bone 2019, 140f). mit je 10 Fällen: summary(runif(10,150,210)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 157.5 178.4 184.8 184.3 197.0 207.2 summary(rnorm(10,180,10)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 163.8 169.7 177.8 181.5 192.7 203.6 hist(runif(10,150,210)) hist(rnorm(10,180,10)) mit je 30 Fällen: summary(runif(30,150,210)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 151.1 164.4 180.7 179.8 195.4 207.7 summary(rnorm(30,180,10)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 162.9 170.9 177.7 178.8 184.9 205.4 hist(runif(30,150,210)) hist(rnorm(30,180,10)) und mit je 1000 Fällen: summary(runif(1000,150,210)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 150.0 163.9 179.2 179.4 194.5 210.0 summary(rnorm(1000,180,10)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 151.8 172.6 180.1 180.0 186.6 211.6 hist(runif(1000,150,210)) hist(rnorm(1000,180,10)) Als Ergänzung: runif() und rnorm() werden noch von der Funktion rbinom() ergänzt, die zufällige binomiale Verteilungen erstellt. Dies sind Verteilungen die angeben, ob ein Ereignis bei einer bestimmten Wahrscheinlichkeit eingetreten ist oder nicht. Damit kann zum Beispiel aufgezeigt werden, wie oft Sie bei zehn Münzwürfen Kopf bekommen (Beispiel 1), oder auch wie oft Sie bei 100 Mal würfeln mit zwei Würfeln zwei Sechsen erzielen (Beispiel 2). rbinom(10, size = 1, prob = 0.5) #Beispiel 1 ## [1] 1 0 0 0 1 0 1 1 0 0 rbinom(100, 2, (1/6 * 1/6)) #Beispiel 2 ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 ## [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0   Als nächstes sollen Sie die beiden Verteilungen aus der Aufgabe 2, die 1.000 Fälle aufweisen, grafisch darstellen. Wie können die 10 Klassen erreicht werden? Die Variante breaks = 10 scheint nicht immer zu funktionieren, sondern ist lediglich ein zu erreichender Vorschlag (vgl. die Hinweise in der Hilfe zu hist() unter dem Punkt breaks). Wir können aber die Punkte, wo die Klassenumbrüche erfolgen sollen, selber bestimmen. Insbesondere bei Gleichverteilungen mit deren klaren Grenzen funktionerit dies einfach über eine Sequenz. Bei der Normalverteilung müssten wir die Punkte, wo die Breaks erfolgen, teilweise noch händisch bestimmen. hist(runif(1000,150,210), main = &quot;Körpergrösse, \\n gleichverteilt&quot;, xlab = &quot;Körgpergrösse (cm)&quot;, col = &quot;darkgreen&quot;, xlim = c(150,210), breaks = seq(from=150, to=210, by= ((210-150)/10))) hist(rnorm(1000,180,10), main = &quot;Körpergrösse, \\n normalverteilt&quot;, xlab = &quot;Körgpergrösse (cm)&quot;, col = &quot;orange&quot;, xlim = c(150,210), breaks = c(100, 156, 162, 168, 174, 180, 186, 192, 198, 204, 300)) Über die Funktion abline() kann dem aktuellen Plot eine Linie hinzugefügt werden. Fügen Sie jeweils einem Histogramm den Mittelwert der anderen Verteilung als vertikale Linie hinzu (ebenfalls in der entsprechenden Farbe). hist(runif(1000,150,210), main = &quot;Körpergrösse, \\n gleichverteilt&quot;, xlab = &quot;Körgpergrösse (cm)&quot;, col = &quot;darkgreen&quot;, xlim = c(150,210), breaks = seq(from=150, to=210, by= ((210-150)/10))) abline(v = mean(rnorm(1000,180,10)), col = &quot;orange&quot;, lwd = 3) hist(rnorm(1000,180,10), main = &quot;Körpergrösse, \\n normalverteilt&quot;, xlab = &quot;Körgpergrösse (cm)&quot;, col = &quot;orange&quot;, xlim = c(150,210), breaks = c(100, 156, 162, 168, 174, 180, 186, 192, 198, 204, 300)) abline(v = mean(runif(1000,150,210)), col = &quot;darkgreen&quot;, lwd = 3) Hinweis: Es konnte zu Problemen beim knitten führen, wenn die abline() in einem anderen Codechunk aufgerufen wurde. Das Argument add = TRUE lässt Sie eine neue Grafik über die aktuelle Grafik legen. Versuchen Sie, auf diese Art Ihre beiden Histogramme in einer Grafik darzustellen. Hierzu können noch drei weitere Anpassungen vorgenommen werden: Die Dimensionen der Y-Achse können angepasst werden über ylim =. Die Farben der Histogramme können transparent gemacht werden in dem wir auf RGB Werte zurückgreifen (ein Vorschlag von Felix Sigrist). Wir können der Grafik noch eine Legende hinzufügen (ein Vorschlag von Josias Bruderer). hist(runif(1000,150,210), main = &quot;Körpergrössen&quot;, xlab = &quot;Körgpergrösse (cm)&quot;, col = &quot;darkgreen&quot;, xlim = c(145,215), ylim = c(0, 250)) #Funktion um die RGB-Daten der Farbe &quot;Orange&quot; zu erhalten: col2rgb(&quot;orange&quot;) ## [,1] ## red 255 ## green 165 ## blue 0 hist(rnorm(1000,180,10), col = rgb(255, 165, 0, 255/4, maxColorValue = 255), add = T) abline(v = mean(rnorm(1000,180,10)), col = &quot;orange&quot;, lwd = 3) abline(v = mean(runif(1000,150,210)), col = &quot;darkgreen&quot;, lwd = 3) legend(195, 200, legend = c(&quot;gleichverteil&quot;, &quot;normalverteilt&quot;), fill = c(&quot;darkgreen&quot;,&quot;orange&quot;), cex = 0.75 )   Erzeugen Sie ein Dataframe, das aus fünf Variablen besteht und 100 Fälle umfasst. Nutzen Sie dazu die verschiedenen Funktionen, die wir bisher kennengelernt haben (Zufallsvariablen, rep(), seq(), ). Probieren Sie ebenfalls, dass die fünfte Variablen dem Character Datenformat entspricht. Überlegen Sie sich einen sozialwissenschaftlichen Kontext für dieses Dataframe und benennen Sie die Variablen dementsprechend. Mit diesem Datensatz werden wir im nächsten Wochenplan weiterarbeiten. Für diese Aufgabe gab es viele tolle Beispiele. Hier zwei Varianten, einmal von Vanessa Leutner und einmal von Josias Bruderer: #Beispiel von Vanessa Leutner Einkommen &lt;- c(seq(from = 2000, to = 12000, length.out = 100)) Krankheitstage &lt;- rnorm(100, mean = 6.5, sd = 3) Ausbildungsjahre &lt;- rep(c(3,6,9,3,13,4,6,7,10,5), each = 10) Anzahl_Kinder &lt;- rep(c(1,0,1,2,3,2,3,5,6,7,1,2,3,4,5,3,2,1,4,5), each = 5) Geschlecht &lt;- rep(c(&quot;weiblich&quot;, &quot;männlich&quot;, &quot;weiblich&quot;, &quot;mänlich&quot;, &quot;divers&quot;), each = 20) df_vl &lt;- data.frame(Einkommen, Krankheitstage, Ausbildungsjahre, Anzahl_Kinder, Geschlecht) #Beispiel von Josias Bruderer df_jb &lt;- data.frame(alter = round(runif(100, 18, 64)), geschlecht = sample(rep(c(seq(1,3),2,3), 20)), tvdauer = sample(round(rnorm(100, 3, 1), 0)), happiness = sample(c(sample(1:10, 80, replace = T), rep(NA, 20))), prog = sample(rep(c(&quot;Kinder&quot;,&quot;Unterhaltung&quot;,&quot;Unterhaltung&quot;,&quot;News&quot;,&quot;Sport&quot;), 20))) Die Funktion round()wurde von vielen anderen auch genutzt. Eine andere Variante zum Runden hat noch Fabio Keller vorgeschlagen, nämlich mittels as.integer(): alter = as.integer(runif(100, 18, 64)) alter ## [1] 28 51 20 38 51 53 52 40 45 32 42 59 24 61 62 45 20 21 34 56 47 33 25 58 45 ## [26] 59 26 48 49 59 29 49 37 49 21 19 47 27 23 46 62 47 18 54 54 40 45 43 42 29 ## [51] 40 61 24 38 45 23 22 49 45 27 32 20 26 34 34 28 56 21 54 61 40 31 45 31 62 ## [76] 45 56 63 55 55 55 25 37 50 37 47 42 45 41 20 38 43 42 26 54 56 18 45 58 19 Zur Wiederholung: Was ist nochmals ein Objekt des Typs data.frames (Manderscheid 2017, 39)? Während () Vektoren weitgehend dem entsprechen, was in der sozialwissenschaftlichen Statistik und in anderen Auswertungsprogrammen als Variable bezeichnet wird, enthält das Objekt Dataframe mehrere Variablen: Die Zeilen eines Dataframes enthalten die Beobachtungen als Fälle, die Spalten die Faktoren und Vektoren als Variablen. Damit entspricht ein Dataframe einem zweidimensionalen, tabellarisch darstellbaren Datensatz. Die Faktoren und Vektoren in den Spalten müssen dabei die selbe Länge, d. h. die selbe Anzahl von Elementen haben, können dabei aber sowohl Zahlen als auch Buchstaben enthalten.   Bonusaufgabe: Das Paket ggplot2 (als Teil der grösseren Paketsammlung tidyverse) ermöglicht gegenüber der Basisversion von R besonders komplexe grafische Lösungen. Versuchen Sie Ihre Variablenverteilung aus Aufgabe 1 auch mit diesem Paket darzustellen. Sie können den Beispielcode unten nutzen (die von Ihnen zu ergänzende Aspekte sind mittels den beiden Zeichen &gt;&lt; im Code versehen) und gleichzeitig etwas recherchieren, wie ggplot2 funktioniert. p1 &lt;- ggplot(data = data.frame(cbind(&gt;Variable1&lt;, &gt;Variable2&lt;)), mapping = aes(x = &gt;Variable1&lt;, y = &gt;Variable2&lt;)) p1 + geom_point(size = &gt;Zahl&lt;, color = &quot;&gt;Farbe&lt;&quot;) + ggtitle(&quot;&gt;Ihr Titel&lt;&quot;) Das Paket ggplot2 hat eine sehr eigene Logik davon, wie Grafiken erstellt werden. Im Seminar selber werden wir in diese Logik nicht vertieft einsteigen. Ein gute Einführung in das Paket und dessen Möglichkeiten bietet das Buch Data Visualization (Healy 2019). References "],["wochenplan-05-indizieren.html", "5 Wochenplan 05: Indizieren 5.1 Lernziele WP05 5.2 Hinweise Indizieren &amp; Subsetting 5.3 Aufgaben WP05", " 5 Wochenplan 05: Indizieren im Rahmen der 05. und 06.Einheit. 5.1 Lernziele WP05 In der kommenden Arbeitswoche geht es nun darum, zwei neue Aspekte der Arbeit mit R kennenzulernen und einzuüben: das Zugreifen auf einzelne Elemente und Teile von Datenobjekten (Indizieren und Subsetting) und die Kontrolle von längeren Befehlsabläufen mittels Schleifen (als erste einfache, eigene Funktionen). Für das Kennenlernen und Einüben der beiden Aspekte dient uns das data.frame-Objekt, das im Rahmen des letzten Wochenplans erstellt wurde. Folgende Lernziele lassen sich festhalten: Sie können gezielt auf einzelne Teile eines Datenobjekts zugreifen und kennen verschiedene Wege, das zu tun. Sie beginnen in der Arbeit mit R gezielt Funktionen zur Abfrage von Attributen von Objekten zu nutzen. Sie verstehen, wie eine for-Schleife funktioniert und können einfache Varianten davon selbst erstellen. 5.2 Hinweise Indizieren &amp; Subsetting Wie können wir nun mittels Indizieren (Auswählen) und Subsetting (Aufteilen) auf einzelne Elemente eines Objektes zugreifen? Teilweise haben wir diesen Aspekte im Umgang mit R bereits kennengelernt  aber nur implizit! Jetzt geht es darum, dies explizit zu machen. Dabei können wir drei Varianten, mit denen wir auf Daten zugreifen: Variante: [] Die erste Variante wählt nach den Dimensionen von Objekte die Elemente aus: vektor1 &lt;- c(1,2,10,5,13,20) vektor2[4] datensatz &lt;- data.frame(vektor1, c(1,1,1,1,2,1)) datensatz datensatz[1,1] datensatz[6,1] datensatz[,0] Variante: $ Die zweite Variante funktioniert in einer Logik von benannten Unterelementen eines Objektes, wie wir dies etwa als Variablen bei Datensätzen kennen: datensatz$vektor1 Variante: which() Mittels der dritten Variante fragen wir in einem Objekt die Eigenschaften von dessen einzelnen Elementen ab. Dies entspricht einer Vorstellung auf das Zugreifen von Fällen. Und wir tun dies über logische Bedingungen: which(datensatz$vektor1==20) Sie sehen bereits im letzten Beispiel, dass die Varianten kombinert werden können. Damit können wir uns Beispielweise einen Teildatensatz erstellen: datensatz[which(datensatz$vektor1==20),] datensatz[datensatz$vektor1==20,] #als einfachere Schreibweise 5.3 Aufgaben WP05 Wenden Sie die Funktionen dim(), names(), str(), class() und typeof() auf Ihren Datensatz an (d.h. das data.frame-Objekt an, welches Sie im Rahmen des letzten Wochenplans erstellt haben). Was sagen Ihnen diese Funktionen jeweils? Welcher Output dieser Funktionen leuchtet Ihnen ein, welcher weniger? Ein erster Datensatz von Valentina Meyer: #data.frame-Objekt von letzter Woche: #Vorbereitung Age &lt;- c(runif(100, min = 13, max = 18)) Alter &lt;- as.character(Age) #Dataframe erstellen df_VM &lt;- data.frame(ID=c(1:100), Alter, Gewicht= c(rep(15,4), rep(16,4), rep(17,8),rep(18,12), rep(19,14), rep(20,16), rep(21,18), rep(22, 12), rep(23, 8), rep(24,4)), Sport= c(seq(10,50,2), seq(50,100,3), seq(100,150,4), seq(150,200,3), seq(200,240,2), rep(120,5), rep(60,6)), Internetnutzung= c(seq(10,50,4), seq(50,100,3), seq(100,150,2), seq(150,200,3), seq(200,240,4), rep(100,6), rep(180,8), rep(60,4))) Über die Funktion view() oder indem wir im Environment-Fenster auf das Objekt des Datensatzes klicken können wir das erstelte Objekt auch betrachten. Was sagen uns nun die einzelnen Funktionen der Aufgabenstellung aus? dim(df_VM) ## [1] 100 5 die Dimensionen Ihres Datensatzes: die Zeilen- und Spaltenzahl, was der Anzahl Fällen und Variablen im Datensatz entspricht. Auf diese Dimensionen greifen Sie dann auch mittels den [] zu. names(df_VM) ## [1] &quot;ID&quot; &quot;Alter&quot; &quot;Gewicht&quot; &quot;Sport&quot; ## [5] &quot;Internetnutzung&quot; die Namen Ihrer Vektoren aus, also die Variablennamen im Datensatz. Diese können wir dann auch verändern: names(df_VM)[4] &lt;- &quot;Sportminuten&quot; Hingegen werden die Namen der Reihen (die Fallnummern) nicht ausgegeben. Diese können wir auch ändern falls wir wollten über rownames(). Die Änderung sehen wir dann im Datenviewer. rownames(df_VM)[2] &lt;- &quot;Fall-Zwei&quot; str(df_VM) ## &#39;data.frame&#39;: 100 obs. of 5 variables: ## $ ID : int 1 2 3 4 5 6 7 8 9 10 ... ## $ Alter : chr &quot;17.4648408435751&quot; &quot;14.7105051246472&quot; &quot;17.0479378274176&quot; &quot;14.3823081436567&quot; ... ## $ Gewicht : num 15 15 15 15 16 16 16 16 17 17 ... ## $ Sportminuten : num 10 12 14 16 18 20 22 24 26 28 ... ## $ Internetnutzung: num 10 14 18 22 26 30 34 38 42 46 ... die Struktur Ihres Datensatzes aus, das heisst die Unterobjekte bzw. Variablen im Datensatz, auf die Sie mittels dem Dollarzeichen zugreifen können. Diese Funktion bietet bieten eine schnelle Übersicht, etwa um Umkodierungen in einem Datensatz zu erkennen oder allgemein auszuweisen, was in einem Objekt enthalten ist. class(df_VM) ## [1] &quot;data.frame&quot; die Klasse Ihres Objektes, das heisst die von Ihnen zugewiesene Eigenschaft des Objektes. Für einen data.frame-Objekt bedeutet dies folgendes (aus der Hileseite der Funktion): A data frame is a structure in R that holds data and is similar to the datasets found in standard statistical packages (for example, SAS, SPSS, and Stata). The columns are variables and the rows are observations. You can have variables of different types (for example, numeric, character) in the same data frame. Data frames are the main structures youll use to store datasets. typeof(df_VM) ## [1] &quot;list&quot; und der Typ Ihres Objektes, das heisst die R-interne Art und Weise, die Daten Ihres Objektes abzuspeichern (hier zeigt sich also die Datenart, und nicht die Objektart/-typ). Listen ist die Speicherweise für verschiedenen Datenformen und kann verschiedene Objekte (bzw. verschiedene Klassen von Objekten) enthalten. Eine weitere nützliche Funktion (für data.frame Objekte und andere) ist summary(): summary(df_VM) ## ID Alter Gewicht Sportminuten ## Min. : 1.00 Length:100 Min. :15.00 Min. : 10.00 ## 1st Qu.: 25.75 Class :character 1st Qu.:18.00 1st Qu.: 59.75 ## Median : 50.50 Mode :character Median :20.00 Median :120.00 ## Mean : 50.50 Mean :19.84 Mean :120.38 ## 3rd Qu.: 75.25 3rd Qu.:21.00 3rd Qu.:186.75 ## Max. :100.00 Max. :24.00 Max. :240.00 ## Internetnutzung ## Min. : 10.00 ## 1st Qu.: 79.25 ## Median :123.00 ## Mean :124.96 ## 3rd Qu.:180.00 ## Max. :240.00 Die Funktion gibt uns für jede Variable eine Übersicht. Sie berechnet bei den metrischen Variablen die Mittelwerte sowie Streuungsmasse und bei Character-Daten die Klasse. Weiter würden auch noch NAs angezeigt, falls diese vorhanden wären. Die Funktion, so könnten wir zusammenfassen, berechnet also immer wieder dieselben Dinge für alle Variablen eines Datensatzes. Eine solche Repetition und Kontrolle von Befehlsabläufen können wir auch selber herstellen über sogenannte Schleifen. Grundsätzlich sind diese for-Schleifen so aufgebaut: for (Variation in Sequenz){ Funktion; } Im ersten Teil (vor den {}-Klammern) befindet sich der sogenannte Funktionskopf, in dem formale Elemente definiert sind, also mit welcher Variation einer Sequenz etwas ablaufen soll. Anschliessend folgt in den {}-Klammern der Funktionsrumpf, der bestimmt, was genau in der Schleife passieren soll. Hier ein einfaches Beispiel: for (i in 1:2) { print(i) } oder bezogen auf einen Datensatz: for (i in 1:2) { print(df_VM[,i]) } #bzw. for (i in 1:2) { print(df_VM[i,]) } Dies können wir auch erweitern, in dem wir nicht eine bestimmte Zahl definieren in den formalen Elementen, sondern die Anzahl Spalten oder Anzahl Zeilen abfragen  und uns so eigentlich den ganzen Datensatz ausgeben lassen: for (i in 1:ncol(df_VM)) { print(df_VM[,i]) } #bzw. for (i in 1:nrow(df_VM)) { print(df_VM[i,]) } Im folgenden Code-Chunk wird nun eine solche for-Schleife definiert, welche die vorherige Funktion für die metrischen Variable im Datensatz immitiert. Zuerst wird dies nur für mean() umgesetzt. Anschliessend werden noch min() und max() ergänzen im Funktionsrumpf. for (i in 3:ncol(df_VM)) { print(mean(df_VM[,i])) } summary(df_VM) #Ergänzung von min und max: for (i in 3:ncol(df_VM)) { print(names(df_VM[i])); print(c(&quot;Min&quot;, min(df_VM[,i]))); print(c(&quot;Mean&quot;, mean(df_VM[,i]))); print(c(&quot;Max&quot;, max(df_VM[,i]))); } #Verwendung von cat() anstelle von Print -- ein Vorschlag von Josias Bruderer for (i in 3:ncol(df_VM)) { cat(c(&quot;\\n&quot;, names(df_VM[i]))); cat(c(&quot; Min&quot;, min(df_VM[,i]))); cat(c(&quot; Mean&quot;, mean(df_VM[,i]))); cat(c(&quot; Max&quot;, max(df_VM[,i]))); }   2. Wählen Sie aus Ihrem Datensatz die Fälle 20 bis 30 sowie die zweite und dritte Variable aus (ohne einen Teildatensatz zu erstellen). Tun Sie dies auf zwei verschiedene Varianten! Variante 1 mittels [] und dem Zugreifen auf die Dimensionen des Datensatzes: df_VM[c(20:30), c(2,3)] #oder einfach: df_VM[20:30, 2:3] #oder df_VM[20:30, c(&quot;Alter&quot;, &quot;Gewicht&quot;)] Variante 2 mittels $ und dem Zugreifen auf die Dimensionen der Vektoren bzw. Variablen: cbind(df_VM$Alter[20:30], df_VM$Gewicht[20:30]) Wie könnten wir vorgehen, wenn wir genau diese Fälle und Variablen nicht wollten? Wir nutzen eine negative c()-Funktion: df_VM[-c(20:30), -c(2,3)]   3. Suchen Sie Fälle in Ihrem Datensatz unter der Verwendung von mindestens zwei Variablen. Diese Fälle sollen spezielle oder interessante Beispiele repräsentieren  begründen Sie Ihre Wahl! Falls Sie keine solche Fälle in Ihrem Datensatz finden können Sie vorhandene Ausprägungen auch anpassen. Ein Datensatz und Beispiel von Fabio Keller: gender &lt;- as.integer(runif(100, 0, 2)) income &lt;- rnorm(100, 10000, 4000) education &lt;- as.integer(rnorm(100, 2, 0.8)) age &lt;- runif(100, 18, 90) home &lt;- sample(c(rep(c(&quot;countryside&quot;, &quot;suburb&quot;, &quot;city&quot;), each = 33), &quot;city&quot;)) df_FK &lt;- data.frame(gender, income, education, age, home) #Ein spezieller Fall? which(df_FK$income &gt; 15000 &amp; df_FK$education == 0) ## [1] 75 #oder einfach: df_FK[df_FK$income &gt; 15000 &amp; df_FK$education == 0,] ## gender income education age home ## 75 1 16116.42 0 49.3708 countryside Ein Datensatz und Beispiel von Vanessa Leutener: Einkommen &lt;- c(seq(from = 2000, to = 12000, length.out = 100)) Krankheitstage &lt;- round(rnorm(runif(100, min = 3, max = 40), mean = 6.5, sd =3)) Ausbildungsjahre &lt;- (rep(c(3, 6, 9, 3, 13, 4, 6, 7, 10, 5), each = 10)) Anzahl_Kinder &lt;- rep(c(1, 0, 1, 2, 3, 2, 2, 0, 0 , 1, 7, 2, 3, 1, 0, 0, 2, 2,5, 0), each = 5) Geschlecht &lt;- sample(rep(c(&quot;weiblich&quot;, &quot;männlich&quot;, &quot;divers&quot;, &quot;männlich&quot;, &quot;weiblich&quot;), each = 20)) df_VL &lt;- data.frame(Einkommen, Krankheitstage, Ausbildungsjahre, Anzahl_Kinder, Geschlecht) # Eine erste Feststellung: plot(df_VL$Ausbildungsjahre, df_VL$Anzahl_Kinder) # Wie könnten wir jetzt spezielle Fälle finden? which(df_VL$Ausbildungsjahre&gt;mean(df_VL$Ausbildungsjahre) &amp; df_VL$Anzahl_Kinder&gt;mean(df_VL$Anzahl_Kinder)) ## [1] 21 22 23 24 25 26 27 28 29 30 81 82 83 84 85 86 87 88 89 90 # Hier weisen wir einfach noch dem 50 Fall (mit hoher Bildung) eine hohe Anzahl Kinder zu: df_VL$Anzahl_Kinder[50] &lt;- 8   4. Definieren Sie eine logische Bedingung die Ihnen erlaubt, Ihr Dataframe anhand der Character-Variable in zwei Gruppen zu teilen. Dies ist eine erste Variante um Teildatensätz zu erstellen (TD1 &amp; TD2). Teilen Sie dann als zweite Variante Ihren Datensatz anhand einer anderen logischen Bedingung in zwei andere Gruppen (TD3 &amp; TD4). Nutzen Sie hierfür wenn möglich zwei numerische Variablen. Ein Datensatz und Beispiel von Katrin Oesch: Wohnort &lt;-c(rep(1:6, times=5),3,4,2,2,6,5,1, rep(1,times=10),5,3,4,1,1,3, rep(seq(from=1, to=6, by=2),times=7), rep(3, times=8), rep(seq(from=1, to=6, by=3),times=9)) Lebenszufriedenheit &lt;-c(rep(seq(from=1,to=10, by=2),times=9), rep(4:6, times=12),10,9,4,4,8,7, rep(5,times=7),8,3,4, seq(from=2, to=9, by=3)) Alter_KO &lt;-c(rep(33:44),seq(from=28,to=66,by=4), 54,62,42,85,66,41,91,23, rep(43,times=7),87,44,43,65,31, rep(31:56),43,23,31,65,43,25,26,36, rep(seq(from=54,to=71,by=5),times=2), rep(28:35, each=2)) Arbeitsstatus &lt;-c(rep(1:8, times=5),4,2,2,6,5,7, rep(1,times=10),rep(2,times=4),5,3,4,1,3, rep(seq(from=1, to=8, by=2),times=), rep(3, times=8),5,6, rep(seq(from=1, to=8, by=3),times=7)) Abstimmungsberechtigung &lt;- c(rep(&quot;Ja&quot;,times=25), &quot;Nein&quot;,&quot;Nein&quot;,&quot;Ja&quot;,&quot;Nein&quot;,&quot;Nein&quot;,&quot;Ja&quot;,&quot;Nein&quot;, &quot;Nein&quot;,&quot;Ja&quot;,&quot;Nein&quot;,&quot;Nein&quot;, &quot;Ja&quot;, rep(&quot;Nein&quot;,times=15), &quot;Ja&quot;,&quot;Nein&quot;,&quot;Nein&quot;,&quot;Ja&quot;,&quot;Nein&quot;,&quot;Nein&quot;, &quot;Ja&quot;,&quot;Nein&quot;,&quot;Nein&quot;,&quot;Ja&quot;,&quot;Ja&quot;, rep(&quot;Ja&quot;,times=33),&quot;Nein&quot;,&quot;Ja&quot;,&quot;Nein&quot;,&quot;Ja&quot;) df_KO &lt;-data.frame(Wohnort, Lebenszufriedenheit, Alter_KO, Arbeitsstatus, Abstimmungsberechtigung) #und die vier Teildatensätz: berechtigt &lt;- which(df_KO$Abstimmungsberechtigung==&quot;Ja&quot;) TD1_KO &lt;- df_KO[berechtigt,] nichtberechtigt &lt;- which(df_KO$Abstimmungsberechtigung==&quot;Nein&quot;) TD2_KO &lt;- df_KO[nichtberechtigt,] staedtisch_alt &lt;- which(df_KO$Wohnort&lt;=3 &amp; df_KO$Alter&gt;=40) TD3_KO &lt;- df_KO[staedtisch_alt,] #Wie könnten wir den Vektor &#39;staedtisch_alt&#39; nutzen? TD4_KO &lt;- df_KO[-c(staedtisch_alt),] #Wir nehmen also einfach &quot;alle anderen&quot;.   5. Berechnen Sie jeweils die Standardabweichungen einer Variable bei den vier Teildatensätze, die Sie in Aufgabe 3 erstellt haben. Was wären (kurze) sozialwissenschaftliche Interpretationen Ihrer Ergebnisse? Ein Datensatz und Beispiel von Josias Bruderer: #Datensatz df_JB &lt;- data.frame(alter = round(runif(100, 18, 64)), geschlecht = sample(rep(c(seq(1,3),2,3), 20)), tvdauer = sample(round(rnorm(100, 3, 1), 0)), happiness = sample(c(sample(1:10, 80, replace = T), rep(NA, 20))), prog = sample(rep(c(&quot;Sandmännchen&quot;,&quot;Tagesschau&quot;, &quot;Akte-X&quot;,&quot;Simpsons&quot;,NA), 20))) TD1_JB &lt;- df_JB[which(!is.na(df_JB$prog)),] # Angabe zu Programm vorhanden TD2_JB &lt;- df_JB[which(is.na(df_JB$prog)),] # Angabe zu Programm nicht vorhanden #Berechnung der Standardabweichung für zwei Teildatensätze: for(t in list(TD1_JB, TD2_JB)){ print(mean(t$tvdauer, na.rm = T)); print(sd(t$tvdauer, na.rm = T)) } ## [1] 2.7875 ## [1] 1.087254 ## [1] 3 ## [1] 1.076055 Hier sehen wir ein komplexeres Beispiel einer Schleife, dass die Möglichkeit von Listen nutzt. Schleifen können daher beliebig komplex werden  und sie sind ein erster Schritt hin zum Schreiben von eigenen Funktionen. Wir nehmen nochmals die Schleife von oben und wenden diese Auf den Datensatz von Katrin Oesch an: for (i in 1:(ncol(df_KO)-1)) { print(mean(df_KO[,i])) } Was wäre wenn wir diesen Prozess nun auf beliebige Datensätze anwenden möchten, wie etwa die vier Teildatensätz aus Aufgabe 4? Hierfür können wir eine eigene Funktion programmieren: NameEigeneFunktion &lt;- function(argumente){ anweisungen(mit argumenten); weitere anweisung(mit argumenten) } Im sogenannten Funktionskopf innerhalb der runden Klammern auf function folgend werden die formalen Argumente benannt und durch Kommas voneinander getrennt. Damit wird festgelegt, welche Eingabeinformationen die Funktion benötigt. [] Alle im Funktionskopf enthaltenen Argumente müssen im Funktionsrumpf, der in geschweiften Klammern {} folgt, als Objekte definiert werden. Dabei können Argumente selbst andernorts definierte Funktionen sein []. Der Funktionsrumpf besteht also aus einer Reihe von Befehlen sowie gegebenenfalls durch # gekennzeichnete Kommentare (Manderscheid 2017, 240f). Hier nun als ausformulierte Funktion: mean_df &lt;- function(df, nr) { df &lt;- df[,-c(nr)]; for (i in 1:ncol(df)) { print(mean(df[,i])) } } #für Teildatensatz 1 mean_df(TD1_KO, 5) #für Teildatensatz 4 mean_df(TD4_KO, 5)   6. Berechnen Sie für eine Variable und bei einer der vier Möglichkeiten aus Aufgabe 3 den Mittelwert, nun allerdings in einem Schritt (das heisst ohne zuerst einen Teildatensatz zu erstellen). Ein Datensatz und Beispiel von Julien Lattmann: #Datensatz Alter_LJ &lt;- round(runif(100, 18, 30), 0) Dauer_Ausbildung &lt;- round(runif(100, 9, 20), 0) Einstiegsgehalt &lt;- round(rnorm(100, 6000, 1800), 0) Zufriedenheit &lt;- round(runif(100, 1, 10), 0) Branche &lt;- rep(c(&quot;Finanz-/Versicherungswesen&quot;, &quot;Information/Kommunikation&quot;, &quot;Gesundheits-/Sozialwesen&quot;, &quot;Erziehung/Unterricht&quot;, &quot;Dienstleistungsbereich&quot;), 20) df_LJ &lt;- data.frame(Alter_LJ, Dauer_Ausbildung, Einstiegsgehalt, Zufriedenheit, Branche) #Variante 1 mean(df_LJ[df_LJ$Einstiegsgehalt &gt;= 6000 &amp; df_LJ$Dauer_Ausbildung &gt;= 14,&quot;Einstiegsgehalt&quot;]) ## [1] 7603.192 #Variante 2 mean(df_LJ$Einstiegsgehalt[df_LJ$Einstiegsgehalt &gt;= 6000 &amp; df_LJ$Dauer_Ausbildung &gt;= 14]) ## [1] 7603.192 References "],["wochenplan-06-code-rezipieren.html", "6 Wochenplan 06: Code Rezipieren 6.1 Lernziele 6.2 Besprechung Skript standardfehler.R 6.3 ggplot2 Darstellung", " 6 Wochenplan 06: Code Rezipieren im Rahmen der 06. und 07.Einheit. 6.1 Lernziele Über die vergangenen Wochen haben wir zahlreiche grundlegende Aspekte der Arbeit mit R kennengelernt. Als Vorbereitung im Rahmen des Wochenplans 06 sollen Sie diese Inhalte noch einmal Revue passieren lassen. Im Rahmen dieser Repetition werden weiter zwei neue Dinge vermittelt werden: Auf der einen Seite erfolgt die Schulung aktiver Rezeptionsfertigkeiten (sprich: R-Code lesen und verstehen lernen). Auch dies ist eine wichtige Arbeitstechnik in R. Auf der anderen Seite sollen Sie mit dem rezipierten Code das Prinzip der Inferenzstatistik (und der Stichprobenverteilung sowie des Standardfehlers) in R veranschaulicht bekommen. Konkret lassen sich folgende Seminarziele festhalten: Sie können von einer anderen Person geschriebenen R-Code entziffern und mit Kommentaren versehen. Sie entwickeln dabei ein Gefühl für unterschiedliche Arten, Code lesbar zu gestalten. Sie wissen, wie selbstgeschriebene Funktionen in R aussehen und können diese Schritt für Schritt interpretieren. Sie entwickeln Ihr Verständnis davon weiter, wie in R Grafiken genutzt und angepasst werden, um statistische Inhalte zu visualisieren. Sie nutzen R um über Konzepte der Inferenzstatistik (wie den Standardfehler) nachzudenken. 6.2 Besprechung Skript standardfehler.R Der folgende Code ist ein Skript zur Simulation des Standardfehlers und soll helfen, das Prinzip der Inferenzstatistik, den Standardfehler sowie die damit zusammenhängenden Ebenen von Grundgesamtheit, einzelner Stichprobe und Stichprobenverteilung zu verstehen und zu veranschaulichen (vgl. Diaz-Bone 2019, 145f). Ebene der Grundgesamtheit: Unsere Grundgesamtheit bilden 1000 Ausprägungen einer beliebigen Variable, die hier nun gleichverteilt ist und sich zwischen 0 und 20 bewegt. Die Verteilung dieser Variable können wir dann in einem Histogramm darstellen. variable_population &lt;- runif(10000, min = 0, max = 20) mean(variable_population) ## [1] 10.0422 sd(variable_population) ## [1] 5.739485 hist(variable_population, breaks = 30, col=&quot;orange&quot;, main=&quot;Unsere Grundgesamtheit&quot;, xlim = c(0,20) ) Ebene der einzelnen Stichprobe: Als nächster Schritt wird nun eine Stichprobe gezogen mit der Funktion sample(), in der zufällig 100 Fälle der Grundgesamtheit landen. Von dieser Stichprobe können wir dann den Mittelwert berechnen und diesen Wert in das Histogramm der Grundgesamtheit einfügen. Der Mittelwert der Stichprobe (schwarz) unterscheidet sich natürlich minimal vom Mittelwert der Grundgesamt (rot).5 Letzterer wird dann über die Funktion abline() eingezeichnet.6 sample1 &lt;- sample(variable_population, 100) mean(variable_population) ## [1] 10.0422 mean(sample1) ## [1] 9.909766 hist(variable_population, breaks = 30, col=&quot;orange&quot;, main=&quot;Unsere Grundgesamtheit&quot;, xlim = c(0,20) ) points(rep(mean(sample1), 2), c(0, 100), type = &quot;l&quot;, col = &quot;black&quot;, lwd = 8) #Hier wird nun auch noch der Mittelwert der Grundgesamtheit eingefügt #...aber mittels der Funktion &#39;abline()&#39; abline(v=mean(variable_population), col = &quot;red&quot;, lwd = 3) Da wir nun mit einer fiktiven Grundgesamtheit und in einem Modell arbeiten können wir immer wieder neue Stichproben ziehen. Dies könnten wir realisieren, indem wir den Codechunk von oben immer wieder repetieren. Oder als elegantere Variante: Wir schreiben uns den Code für eine for-Schleife, die uns eine beliebige Anzahl Stichproben zieht (i), den Mittelwert der Stichprobe berechnet und diesen Mittelwert in das Histogramm der Grundgesamtheit einzeichnet. Ebene der einzelnen Stichprobe als Loop: Der folgende Code ruft nun nochmals das Histogramm der Grundgesamtheit auf. Anschliessend zieht die Schleife 100 Stichproben, berechnet deren Mittelwerte und fügt diese als Linie mit verschiedenen Farben in die Grafik ein. hist(variable_population, breaks = 30, col = &quot;orange&quot;, main=&quot;Unsere Grundgesamtheit&quot;, xlim = c(0,20) ) for (i in 1:100) { sample1 &lt;- sample(variable_population, 100) points(rep(mean(sample1), 2), c(0, 200), type = &quot;l&quot;, col = i, lwd = 1) } Wir sehen, dass mit jeder gezogenen Stichprobe der Mittelwert etwas abweicht von demjenigen Wert in der Grundgesamtheit. Die Abweichung ist nun auch deutlicher zu sehen als vorher. Stichprobenziehung als Funktion: Als nächster Schritt wird nun eine Funktion geschrieben, mit der wir eine beliebige Anzahl Stichproben mit einer beliebigen Grösse ziehen können. Die Grundstruktur von Funktionen entspricht folgender Form (Manderscheid 2017, 240f): eigene.funktion &lt;- function(argumente) { anweisung } Im Funktionskopf innerhalb der runden Klammern, die auf function() folgen, werden die formalen Argumente benannt und durch Kommas voneinander getrennt. Damit wird festgelegt, welche Eingabeinformationen die Funktion benötigt. In unserem Beispiel sind dies die selbstgewählten Begriffe x (Objekt, von dem die Stichprobe gezogen werden soll), n (Grösse der Stichprobe) und trials(Anzahl der zu ziehenden Stichproben). Alle im Funktionskopf enthaltenen Argumente müssen im Funktionsrumpf, der in geschweiften Klammern {} folgt, als Objekte definiert werden. Der Funktionsrumpf besteht aus einer Reihe von Befehlen. Die in unserem Code definierte Funktion definiert zuerst ein Sample von der Grösse n aus dem Objekt x. Anschliessend hängt eine Schleife die Anzahl trials-1 weitere Sample an das bereits gezogene Sample dran. Die Spezifikation von -1erfolgt da ja bereits eine erste Stichprobe gezogen wird, bevor dann die weiteren darangehängt werden (wenn also 200 Stichproben gemacht werden sollen, dann wird zuerst 1 gezogen und dann 199 angehängt). Die einzelnen Schritte, die im Funktionsrumpf festgelegt werden, erscheinen nicht in der Konsole. Nur das Ergebnis der letzten Funktion im Rumpf erscheint abschliessend als Rückgabewert. Da in unserem Beispiel kein Ergebnis erzeugt wurde stellt die Funktion return() sicher, dass das Resultat der Funktion (das Objekt variable sample) ausgegeben wird. meine_samples &lt;- function(x, n, trials) { variable_sample &lt;- sample(x, n) for (i in 1:(trials - 1)){ variable_sample &lt;- cbind(variable_sample, sample(x, n)) } return(variable_sample) } Im letzten Codestück wird dann vom unsere Grundgesamtheit (x) eine 30er Stichprobe gezogen (n), und zwar 200mal (trials). Das Objekt stichproben_200 ist also eine Matrix mit 30 Zeilen und 200 Spalten.7 stichproben_200 &lt;- meine_samples(variable_population, 30, 200) Ebene der Stichprobenverteilung: Bereits in den vorhergehenden beiden Schritten (Stichprobe als Schleife und Stichprobe als Funktion) ging es nicht mehr nur um eine einzelne Stichprobe, sondern um verschiedene Stichproben und deren jeweilige Mittelwerte. Wir gehen also über zu einer Stichprobenverteilung. Im folgenden Code werden zuerst die Stichprobenkennwerte berechnet (d.h. die Mittelwerte der 200 gezogenen Stichproben) und dann als Histogramm dargestellt: Das Histogramm entspricht der Stichprobenverteilung der Stichprobenkennwerte.8 mittelwerte &lt;- apply(stichproben_200, 2, mean) hist(mittelwerte, breaks = 10, col = &quot;blue&quot;, main = &quot;Unsere Stichprobenmittelwerte&quot;, xlim=c(3,17) ) Hierbei wird nun ersichtlich, dass diese Stichprobenverteilung der Normalverteilung folgt (ab einem Stichprobenumfang von 30). Das heisst eben auch, dass je stärker ein Stichprobenmittelwert vom Mittelwert der Grundgesamtheit abweicht, desto unwahrscheinlicher ist dieser Wert. Oder umgekehrt: Tritt ein sehr stark abweichender Wert auf ist dies mit hoher Wahrscheinlichkeit nicht zufällig. Gemeinsame Darstellung der Stichprobenverteilung in der Grundgesamtheit anhand der Histogramme: In den folgenden Codezeilen wird nun die Stichprobenverteilung in das Histogramm der Grundgesamtheit eingefügt. Die Idee hierbei ist, dass die Stichprobengrösse dank der eigenen Funktion variiert werden kann. Daraus wird ersichtlich, dass je grösser der Stichprobenumfang ist, desto schmaler wird die Stichprobenverteilung (das blaue Histogramm), das heisst: desto kleiner wird der Standardfehler. hist(variable_population, breaks = 30, col = &quot;orange&quot;, main = &quot;Grundgesamtheit mit Stichprobenverteilung&quot;, xlim = c(0, 20), ) # Hiweise hier stimmt der Titel zuerst natürlich noch nicht. # Erst wenn das zweite Histogram darüber gelegt wird stimmt auch der Titel. mittelwerte_samples &lt;- apply(meine_samples(variable_population, 30, 1000), 2, mean) hist(mittelwerte_samples, breaks = 10, col = &quot;blue&quot;, add = T) Mit der Darstellung können wir uns nochmals die expliziten Parameter der Stichprobenverteilung vergegenwärtigen. Auf der einen Seite haben wir das arithmetische Mittel der Stichprobenmittelwerte, das heisst: $ $ Dieser Wert entspricht (annährend) dem arithmetischen Mittelwert des metrischen Merkmals in der Grundgesamtheit: $ = $ mean(mittelwerte_samples) ## [1] 9.966293 mean(variable_population) ## [1] 10.0422 Auf der anderen Seite finden wir die Standardabweichung der Stichprobenverteilung, der Standardfehler (oder Stichprobenfehler): $ = $ Dieser Wert gibt das Ausmass der Streuung der einzelnen Stichprobenmittelwerte um den Mittelwert der Stichprobenverteilung beziehungweise der Grundgesamtheit an. sd(variable_population)/sqrt(30) und entspricht ungefähr der Standardabweichung unserer Stichprobenverteilung: sd(mittelwerte_samples) Die Stichprobenmittelwerte streuen umso geringer um den Mittelwert der Stichprobenverteilung (das heisst um den Wert in der Grundgesamtheit), je grösser der Umfang der Stichprobe ist. Das heisst die Stichprobenwerte werden immer genauer, da deren zufällige Abweichung verkleinert wird. hist(variable_population, breaks = 30, col = &quot;orange&quot;, main = &quot;Grundgesamtheit mit Stichprobenverteilung&quot;, xlim = c(0, 20), ) mittelwerte_samples &lt;- apply(meine_samples(variable_population, 120, 1000), 2, mean) hist(mittelwerte_samples, breaks = 10, col = &quot;blue&quot;, add = T) sd(mittelwerte_samples) ## [1] 0.5180478 #Der Standardfehler wurde halbiert, da die Stichprobengrösse vervierfacht wurde. Was hat uns nun dieses Skript und insbesondere die letzten Darstellungen gebracht (Diaz-Bone 2019, 240f)? Das blaue Histogram visualisiert das, was wir jeweils bei der Intervallschätzung sowie beim Testen in der Inferenzstatistik konstruieren, nämlich eine Stichprobenverteilung. Bei der Intervallschätzung konstruieren wir ein Konfidenzintervall mittels des Stichprobenkennwerts, also eine Stichprobenverteilung anhand des Wertes beziehungsweis um den Wert aus der Stichprobe. Beim Testen von Hypothesen konstruieren wir die Stichprobenverteilungen unter Annahme der 0-Hypothese und prüfen dann, ob unser Stichprobenkennwert in den Annahme- oder in den Rückweisungsbereich fällt. Weiter zeigt uns die Darstellung den Stichprobenfehler/Standardfehler. Diese Kennzahl wird uns jeweils bei inferenzstatistischen Berechnungen ebenfalls ausgegeben. Sie hilft uns gemeinsam mit dem Stichprobenkennwert und der Stichprobengrösse ein jeweiliges Ergebnis zu beurteilen und ergänzt so die Aussagen, dass ein Ergebnis einfach nur signifikant ist oder nicht. 6.3 ggplot2 Darstellung Wir könnten natürlich die letzte Darstellung, in der gemeinsam die Stichprobenverteilung in der Grundgesamtheit anhand der Histogramme abgebildet wird, ebenfalls mittels des Paketes ggplot2 darstellen. Dieses umfasst komplexere Möglichkeiten, Grafiken mittels R zu kreieren und ist eigentlich der Standard für Grafiken. Als erstes müssen wir hierfür einen Datensatz generieren, in dem beide Variablen enthalten sind (also die Verteilungen der Grundgesamtheit und die Stichprobenverteilung). Diese Daten sind allerdings in einer speziellen Form abgespeichert, nämlich in einem sogenannten long-Format (Healy 2019, 56): Social scientists will likely be familiar with the distinction between wide-format and longjormat data. In a long-format table, every variable is a column, and every observation is a row. In a wide-format table, some variables are spread out across columns. #Zur Veranschaulichung werden nochmals die beiden Variablen generiert... variable_population &lt;- runif(10000, min = 0, max = 20) mittelwerte_samples &lt;- apply(meine_samples(variable_population, 30, 1000), 2, mean) #...und dann in einem dataframe Objekt zusammengefasst. # Diese Daten sind nun in einem &#39;long&#39;-Format. daten_spv &lt;- data.frame(Verteilung = c(rep(&quot;Grundgesamtheit&quot;, 10000), rep(&quot;Stichprobenverteilung&quot;, 1000)), Werte = c(variable_population, mittelwerte_samples)) Anschliessend können wir die Grafik mittels der Funktion ggplot() erstellen. Zuerst wird das leere Grafikobjekt generiert. Dieses greift auf die Werte zu (x) und unterscheidet diese über die verschiedenen Verteilungen (fill). Erst anschliessend werden über die + die weiteren Aspekte hinzugefügt als Schichten oder Layer der Grafik (vgl. Healy 2019, 59f). library(ggplot2) p &lt;- ggplot(daten_spv, aes(x=Werte, fill=Verteilung)) p + geom_histogram(col = &quot;black&quot;, position = &#39;identity&#39;, bins = 40) + theme_minimal() + ylab(&quot;Anzahl&quot;) + scale_fill_manual(values=c(&quot;orange&quot;, &quot;blue&quot;)) References "],["wochenplan-07-datenaufbereitung.html", "7 Wochenplan 07: Datenaufbereitung 7.1 Lernziele 7.2 Aufgaben", " 7 Wochenplan 07: Datenaufbereitung im Rahmen der 07. und 08. Einheit. 7.1 Lernziele In der zweiten Hälfte des Semester möchten wir das statistische Arbeiten mit R anhand des Umgangs mit echten Daten kennenlernen. Dazu nutzen wir Daten aus dem European Social Survey (ESS). Am Beginn der Arbeit mit einem Datensatz steht das Kennenlernen der Daten: Es gilt die Variablen und ihre Ausprägungen zu verstehen, erste Plausibilitätsprüfungen durchzuführen und Daten für die spätere Analyse aufzubereiten. Diese Techniken sollen Sie im Rahmen des siebten Wochenplans kennenlernen und so die Grundlage schaffen, in den folgenden Einheiten statistische Analysen mit dem Datensatz vorzunehmen. Konkret lassen sich folgende Lernziele festhalten: Sie können einen CSV-Datensatz in R laden und kennen die Vor- und Nachteile dieses Dateiformats. Sie können einen geladen Datensatz anhand verschiedener Funktionen und in unterschiedlichem Detailliertheitsgrad beschreiben. Sie kennen erste Techniken zur Plausibilitätsprüfung von Datensätzen. Sie kennen erste Techniken zur Datenaufbereitung. Sie verstehen, wie und wozu man zufällige Teildatensätze erstellt. 7.2 Aufgaben Laden Sie sich den Datensatz ESS1-8e01.csv und ESS-Fragebogen (das Codebook) herunter (via OLAT) und speichern Sie die Dateien in Ihrem Arbeitsverzeichnis. Laden Sie anschliessend den Datensatz in RStudio über die Funktion read.csv(). Betrachten Sie den Datensatz über die Funktion View() oder indem Sie auf Ihr erstelltes Objekt im Fenster Environment klicken. Vergleichen Sie für einzelne Variablen die Informationen im Datensatz mit den entsprechenden Fragebogenfragen! setwd(&quot;C:/Users/SchweglG/R_Daten/06_HS21/R_Seminar-HS21&quot;) Als erster Schritt gilt es Ihr Arbeitsverzeichnis zu definieren, das heisst der Ort, an dem auch Ihre Daten abgelegt sind (ggfs. sind diese noch in einem weiteren Unterordner). In diesem Arbeitsverzeichnis würden auch allfällige weitere Daten liegen oder später automatisch von R abgespeichert (vgl. Aufgabe 5). Sobald Sie die Kodezeilen bis daten_ess &lt;- read.csv(file =\"\" eingegeben haben sollten Sie auf die Tabulatortaste klicken können und so Ihr Datenfile auswählen (und eben auch auf weitere Unterordner Ihres Arbeitsverzeichnis zurückgreifen). Hier gilt es allerdings folgendes zu beachten (vgl. auch diese Erläuterungen): Standardmässig ist das Arbeitsverzeichnis für R-Codechunks das Verzeichnis, in welchem Ihr RMarkdown-Dokument abgelegt ist. In RStudio können Sie diesen Standard auch über das Menü Tools -&gt; Global Options -&gt; R Markdown ändern. Hier gibt es nun zwei weitere Möglichkeiten: Sie können das aktuelle Arbeitsverzeichnis Ihrer R-Konsole (die Option Current) oder das Stammverzeichnis eines Projektes als Arbeitsverzeichnis verwenden (die Option Project).9 Mit Current sollte dann dasjene Verzeichnis ausgewählt werden, das wir auch über die Menüsteuerung zur Definition des Arbeitsverzeichnis auswählen. Jedoch wird damit noch nicht behoben, dass mit der Tabulatortaste weiter auf denjenigen Ordner zugegriffen wird, in dem das jeweils aktive RMardown-Dokument abgelegt ist. Deswegen ist es angebracht, vor dem Laden der Dateien auch nochmals das Arbeitsverzeichnis einzufügen: setwd(&quot;C:/Users/SchweglG/R_Daten/06_HS21/R_Seminar-HS21&quot;) daten_ess &lt;- read.csv(file = &quot;Daten/ESS1-8e01.csv&quot;) Eine weitere Alternative wäre das Arbeitsverzeichnis über einen Setup Kode anzupassen: &#39;&#39;&#39;{r, setup, include=FALSE} knitr::opts_knit$set(root.dir = &quot;C:/Users/SchweglG/R_Daten/06_HS21/R_Seminar-HS21&quot;) &#39;&#39;&#39; Und als Hinweis: Probieren Sie auch mal das Definieren und Laden des Datensatzes in einem RSkript (anstatt RMarkdown). Dort sollte dann auch die Tabulatortasten-Funktion möglich sein (unabhängig vom Speicherort des Skripts). Anschliessend können Sie den im Skript geschriebenen Kode in Ihr Markdown kopieren.   Öffnen Sie nun die Datei ESS1-8e01.csv in einem Tabellenkalkulations- oder einem Textbearbeitungsprogramm (z.B. Microsoft Excel, dem Windows-Editor oder TextEdit auf Mac). Vergleichen Sie die beiden Varianten (Ihr Dataframe in RStudio und die im Textbearbeitungsprogramm geöffnete Datei), um das Dateiformat CSV besser zu verstehen. Ersetzen Sie anschliessend im Tabellenkalkulations- oder Textbearbeitungsprogramm die Kommas in der ESS1-8e01.csv Datei mit Strichpunkten und speichern Sie diese neue Version ab. Gehen Sie zurück zu R Studio und versuchen Sie, die neue Version des Datensatzes zu laden. Welchen Paramter müssen Sie anpassen, damit das funktioniert? Was denken Sie könnten Vorteile des CSV-Formats sein? Welche möglichen Nachteile im Vergleich zu anderen Dateiformaten (Excel, SPSS ) sind denkbar? Das CSV-Dateiformat bietet eine sehr grundlegende Art und Weise um Daten oder eben Datensätze zu speichern. Indem die Kommas mit Strichpunkten ersetzt werden ändert sich das Zeichen, mit dem die Spalten bzw. Variablen getrennt werden. Die Fälle werden im Format jeweils über Zeilenumbrüche getrennt werden. ?read.csv Die read.csv-Funktion ermöglicht über das Argument sep = zu bestimmen, über welches Zeichen die Spalten eingelesen werden sollen. Wir definieren also über das Argument eine bestimmte Spezifizierung (es sind nicht die Standardwerte, die ,) mit dem einem bestimmten Paramater (es sind neu ;). Das CSV-Dateiformat hat vor allem zwei Vorteile: Auf der einen Seite  und wie in der Teilaufgabe oben deutlich wurde  können über beliebige Texteditoren direkt in diese Daten eingegriffen werden, da im Format lediglich Rohdaten in Textform abgespeichert sind. Bei anderen Dateiformaten für Datensätze wie etwa .sav (von der Statistiksoftware SPSS) ist dies nicht so einfach möglich. Damit ist das CSV-Format ein besonders transparenter Standard, mit dem fast alle Programme und Betriebssysteme umgehen können. Auf der anderen Seite ist das CSV-Dateiformat nicht nur transparent, sondern beansprucht auch nur sehr wenig Speicherplatz. Grosse Datensätze können so schnell von RStudio gelesen und bereitgestellt werden. Die grosse Datensätze werden dadurch nicht komplexer, sondern womöglich lediglich unübersichtlich. Dem gegenüber steht der Nachteil von CSV, dass das Dateiformat keine Möglichkeiten bietet, Informationen auf komplexere Art und Weise abzuspeichern (z.Bsp. Levels für kategoriale Variablenausprägungen oder das Speichern von mehr als zwei Dimensionen). CSV bleibt so ein Format für Rohdaten. Weiter besteht die Möglichkeit, dass Sonderzeichen in den Daten selbst zu Problemen führen (also etwa ein , als Teil einer Ausprägung, dass dann fälschlicherweise als Spaltenumbruch eingelesen würde).   Versuchen Sie mit Funktionen, die Sie bisher kennengelernt haben, den geladenen Datensatz kurz zu beschreiben. Wir haben uns hier ein Objekt der Klasse class(daten_ess) ## [1] &quot;data.frame&quot; geladen, dass Daten als typeof(daten_ess) ## [1] &quot;list&quot; abspeichert. Unser Datensatz umfasst.. dim(daten_ess)[1] ## [1] 1525 Fälle mit jeweils dim(daten_ess)[2] ## [1] 21 Variablen. Die zweite und die zweitletzte Variablen heisst etwa: names(daten_ess)[c(1,21)] ## [1] &quot;x&quot; &quot;hinctnta&quot; Die ersten fünf Variablen  alphabetisch geordnet  heissen: sort(names(daten_ess))[1:5] ## [1] &quot;agea&quot; &quot;chldhm&quot; &quot;cntry&quot; &quot;edctn&quot; &quot;edulvla&quot; Aktuell umfasst der Datensatz nur metrische Daten und eine eine Character-Variable als Konstante.10 Auch eine Variable wie gndr ist daher eine Zahl in dem Datensatz und wird aktuell von RStudio als ein numerischer Vektor behandelt. Die Funktion str() liefert hierfür eine Übersicht, in dem sie die interne Struktur eines R-Objekts aufzeigt. Für unseren Datensatz bedeutet dies: die Variablen (auf welche wir mit $ zugreifen können), deren Dateiformat sowie deren ersten paar Ausprägungen. str(daten_ess) ## &#39;data.frame&#39;: 1525 obs. of 21 variables: ## $ x : int 1 2 3 4 5 6 7 8 9 10 ... ## $ cntry : chr &quot;CH&quot; &quot;CH&quot; &quot;CH&quot; &quot;CH&quot; ... ## $ essround: int 8 8 8 8 8 8 8 8 8 8 ... ## $ idno : int 1 3 5 6 9 14 21 22 24 27 ... ## $ pspwght : num 0.9 1.1 1.02 1.09 1.03 ... ## $ pweight : num 0.465 0.465 0.465 0.465 0.465 ... ## $ polintr : int 2 2 2 3 2 2 2 3 2 1 ... ## $ happy : int 8 6 8 8 8 5 9 3 8 10 ... ## $ gndr : int 1 2 1 2 1 1 1 2 2 2 ... ## $ yrbrn : int 1960 1953 1987 1949 1963 1948 1961 1955 1980 1941 ... ## $ agea : int 56 63 29 67 53 68 55 61 36 75 ... ## $ chldhm : int 2 2 2 2 2 2 2 2 2 2 ... ## $ edulvla : int 3 3 5 2 3 3 3 2 5 4 ... ## $ eisced : int 3 3 7 2 3 3 3 2 6 5 ... ## $ eduyrs : int 9 12 18 9 10 9 9 8 13 16 ... ## $ pdwrk : int 1 1 1 0 0 0 1 0 1 0 ... ## $ edctn : int 0 0 0 0 0 0 0 0 0 0 ... ## $ wkhtot : int 60 20 50 37 44 46 45 44 24 55 ... ## $ isco08 : int 1200 5419 2210 5223 7112 4220 8300 8100 5223 1120 ... ## $ uemp3m : int 1 1 2 2 1 1 2 2 2 1 ... ## $ hinctnta: int 10 1 10 1 77 2 5 77 4 5 ... Über die summary()-Funktion wird auch nochmals deutlich, dass hier tatsächlich Rohdaten eingelesen wurden: Auch fehlende Werte gelten aktuell noch als gültige Zahlen (jemand wäre also im Jahr 7777 geboren). summary(daten_ess$yrbrn) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1922 1954 1968 1991 1984 7777 Weiter sehen wir, dass sogenannte Dummy-Variablen oft 1-2 anstatt 0-1 kodiert sind. summary(daten_ess$chldhm) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 1.000 2.000 1.656 2.000 2.000 oder wir sehen, dass Geschlecht ziemlich gleichmässig verteilt zu sein scheint: summary(daten_ess$gndr) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 1.000 1.000 1.483 2.000 2.000   In den Variablen zum Alter und den Bildungsabschlüssen der befragten Personen haben sich einige Fehler eingeschlichen. Was sind die Fehler? Und welche Fälle betrifft das (Fallnummer)? Wir können drei Stufen von Plausibilitätstests unterscheiden: Bei der ersten Stufe betrachten wir lediglich die Zahlen selber, wobei uns vor allem Häufigkeitsverteilungen und Extremwerten interessiert. Wir beginnen mit der Variable zum Alter: min(daten_ess$agea) ## [1] 14 max(daten_ess$agea) ## [1] 999 summary(daten_ess$agea) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 14.00 32.00 48.00 51.75 63.00 999.00 which(daten_ess$agea==999) ## [1] 121 196 320 717 791 800 which(daten_ess$agea&gt;120) ## [1] 121 132 196 320 717 791 800 Hier scheint nun ein erster Fehler sichtbar zu sein, nämlich beim Fall 132. Diese Person soll angeblich 320 Jahre alt sein: daten_ess$agea[132] ## [1] 320 Die Werte zu den Bildungsabschlüssen sind kategoriale Daten. Deshalb lohnt es sich hier mit Tabellen zu arbeiten, um Häufigkeitsverteilungen und Extremwerte zu betrachten: table(daten_ess$edulvla) ## ## 1 2 3 4 5 55 77 88 ## 51 265 665 45 491 3 1 4 table(daten_ess$eisced) ## ## 0 1 2 3 4 5 6 7 55 77 88 ## 1 50 265 543 122 226 114 197 3 1 3 Hier scheint noch nicht wirklich ein Fehler sichtbar zu sein. Auf der zweiten Stufe überprüfen wir nun, ob die Daten auch wirklich Sinn machen, das heisst im Rahmen der Codierungen gemäss Fragebogen. So wird schnell deutlich, dass wir natürlich fehlende Werte auf eine bestimmte Art und Weise in unserem Datensatz vorfinden. Beim Alter ist dies 999 (Not available), während es bei den Bildungsabschlüssen die Ausprägungen 55 (Other), 77 (Refusal), 88 (Dont know) und 99 (No answer) sind. Diese Ausprägungen können wir dann als fehlende Werte, sogenannte NAs definieren: daten_ess$agea[daten_ess$agea==999] &lt;- NA daten_ess$edulvla[daten_ess$edulvla&gt;5] &lt;- NA daten_ess$eisced[daten_ess$eisced&gt;7] &lt;- NA Ein Vergleich mit dem Codebuch zeigt dabei aber auch, dass erst Personen ab dem 15. Lebensjahr befragt wurden. Folgender Fall 352 macht also keinen Sinn: which(daten_ess$agea&lt;15) ## [1] 352 Hingegen ist die bei der Variable eisced auftauchende Ausprägung 0 ist durchaus zulässig und entspricht Not possible to harmonise into ES-ISCED. Auf der dritten Stufe vergleichen wir dann mittels Kreuztabellierungen und logischen Überlegungen jeweils (mindestens) zwei Variablen miteinander, um so nochmals Fehler zu entdecken. Dieses könnten wir mittels eines Plots machen, wie dies etwa Vanessa Leutner vorgeschlagen hat: plot(daten_ess$agea, daten_ess$yrbrn, xlim = c(10,100), ylim = c(1900,2017)) Dann können wir eine zusätzliche Variable berechnen, die als Fehlerindikator für die Angaben zum Alter dient. Diese zusätzliche Variable nutzen wir dann als logische Bedingung, um fehlerhafte Fälle zu entdecken: daten_ess$agea_l &lt;- daten_ess$agea + daten_ess$yrbrn daten_ess$agea_l[daten_ess$agea_l!=2016] ## [1] 2017 2017 2017 2017 2017 2017 2017 2017 NA 2017 2304 2017 NA 2017 2017 ## [16] 2017 NA 2017 2017 2017 1990 2017 2017 2017 2017 2017 2017 2017 2017 2017 ## [31] 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 NA ## [46] 2017 2017 2017 NA NA 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 ## [61] 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 2017 ## [76] 2017 2017 2017 2017 1962 2017 2017 2017 2017 2017 2017 2017 2017 2017 which(daten_ess$agea_l!=2016 &amp; daten_ess$agea_l!=2017) ## [1] 132 352 1414 #Identifikation der Fälle daten_ess[daten_ess$agea_l!=2016 &amp; daten_ess$agea_l!=2017 &amp; !is.na(daten_ess$agea_l),c(&quot;agea&quot;, &quot;yrbrn&quot;)] ## agea yrbrn ## 132 320 1984 ## 352 14 1976 ## 1414 23 1939 Neben den bereits bekannten beiden Fällen (132 &amp; 352) wir so auch noch der dritte falsche Fehler im Datensatz bei der Altersvariable deutlich, nämlich Fall 1414. Bei den Angaben zum Bildungsniveau wieder können wir stärker Kreuztabellen nutzen, um zwei Fehler zu entdecken: table(daten_ess$edulvla, daten_ess$eisced) ## ## 0 1 2 3 4 5 6 7 ## 1 0 50 0 0 0 1 0 0 ## 2 0 0 265 0 0 0 0 0 ## 3 0 0 0 543 122 0 0 0 ## 4 0 0 0 0 0 45 0 0 ## 5 0 0 0 0 0 180 114 197 table(daten_ess$eduyrs, daten_ess$edulvla) ## ## 1 2 3 4 5 ## 0 2 1 0 0 0 ## 1 1 0 1 0 0 ## 2 1 2 0 0 0 ## 3 1 0 0 0 0 ## 4 4 1 0 0 0 ## 5 2 4 0 0 0 ## 6 12 5 3 0 1 ## 7 5 1 3 0 0 ## 8 5 44 52 2 9 ## 9 13 125 302 24 91 ## 10 1 34 71 6 20 ## 11 1 22 30 1 12 ## 12 3 19 82 1 62 ## 13 0 4 43 2 38 ## 14 0 0 39 2 31 ## 15 0 3 12 3 32 ## 16 0 0 13 3 37 ## 17 0 0 5 1 48 ## 18 0 0 3 0 52 ## 19 0 0 4 0 29 ## 20 0 0 0 0 13 ## 21 0 0 0 0 6 ## 22 0 0 0 0 5 ## 23 0 0 1 0 3 ## 25 0 0 0 0 1 ## 26 0 0 0 0 1 ## 77 0 0 0 0 0 ## 88 0 0 1 0 0 table(daten_ess$eduyrs, daten_ess$eisced) ## ## 0 1 2 3 4 5 6 7 ## 0 0 2 1 0 0 0 0 0 ## 1 0 1 0 1 0 0 0 0 ## 2 0 1 2 0 0 0 0 0 ## 3 0 1 0 0 0 0 0 0 ## 4 0 4 1 0 0 0 0 0 ## 5 0 2 4 0 0 0 0 0 ## 6 0 12 5 3 0 1 0 0 ## 7 0 5 1 3 0 0 0 0 ## 8 0 5 44 49 3 9 0 2 ## 9 0 13 125 287 15 90 12 13 ## 10 0 1 34 67 4 22 3 1 ## 11 0 1 22 20 10 10 2 1 ## 12 1 2 19 55 27 23 15 26 ## 13 0 0 4 23 20 21 10 9 ## 14 0 0 0 18 21 14 8 11 ## 15 0 0 3 5 7 11 15 9 ## 16 0 0 0 6 7 11 15 14 ## 17 0 0 0 1 4 6 15 28 ## 18 0 0 0 1 2 6 13 33 ## 19 0 0 0 2 2 1 4 24 ## 20 0 0 0 0 0 1 1 11 ## 21 0 0 0 0 0 0 0 6 ## 22 0 0 0 0 0 0 1 4 ## 23 0 0 0 1 0 0 0 3 ## 25 0 0 0 0 0 0 0 1 ## 26 0 0 0 0 0 0 0 1 ## 77 0 0 0 0 0 0 0 0 ## 88 0 0 0 1 0 0 0 0 #Identifikation der Fälle which(daten_ess$edulvla==1 &amp; daten_ess$eisced==5) ## [1] 443 which(daten_ess$edulvla==3 &amp; daten_ess$eisced==3 &amp; daten_ess$eduyrs&lt;6) ## [1] 1482 daten_ess[c(which(daten_ess$edulvla==1 &amp; daten_ess$eisced==5), which(daten_ess$edulvla==3 &amp; daten_ess$eisced==3 &amp; daten_ess$eduyrs&lt;6)), c(&quot;edulvla&quot;, &quot;eisced&quot;)] ## edulvla eisced ## 443 1 5 ## 1482 3 3 Wir finden so zwei Fälle entdeckt, bei denen Fehler in den Bildungsabschlüssen drin stecken, nämlich Fall 443 und Fall 1482. Josias Bruderer hat für ein solches Fehlersuchen mittels logischer Bedingungen auch die Alters- und Bildungsvariablen kombiniert: # Menschen die vor 3 Jahren in die Schule kamen errors_a &lt;- which(daten_ess$agea &lt; (daten_ess$eduyrs) + 3 &amp; daten_ess$agea &lt; 999 &amp; daten_ess$eduyrs &lt; 77) # Menschen mit Fehler im Bildungsabschluss gem. Definition ES-ISCED errors_b &lt;- c() errors_b &lt;- c(errors_b, which(daten_ess$eisced == 1 &amp; daten_ess$edulvla &gt; 1 &amp; daten_ess$edulvla &lt; 55)) errors_b &lt;- c(errors_b, which(daten_ess$eisced == 2 &amp; daten_ess$edulvla != 2 &amp; daten_ess$edulvla &lt; 55)) errors_b &lt;- c(errors_b, which(daten_ess$eisced == 3 &amp; daten_ess$edulvla != 3 &amp; daten_ess$edulvla &lt; 55)) errors_b &lt;- c(errors_b, which(daten_ess$eisced == 4 &amp; daten_ess$edulvla != 3 &amp; daten_ess$edulvla &lt; 55)) errors_b &lt;- c(errors_b, which(daten_ess$eisced == 5 &amp; daten_ess$edulvla != 4 &amp; daten_ess$edulvla != 5 &amp; daten_ess$edulvla &lt; 55)) errors_b &lt;- c(errors_b, which(daten_ess$eisced == 6 &amp; daten_ess$edulvla != 5 &amp; daten_ess$edulvla &lt; 55)) errors_b &lt;- c(errors_b, which(daten_ess$eisced == 7 &amp; daten_ess$edulvla != 5 &amp; daten_ess$edulvla &lt; 55)) # Menschen mit &quot;unlogischem&quot; Alter für Bildungsabschluss gem. ES-ISCED errors_c &lt;- c() errors_c &lt;- c(errors_c, which(daten_ess$eisced == 1 &amp; daten_ess$agea &lt; 11)) errors_c &lt;- c(errors_c, which(daten_ess$eisced == 2 &amp; daten_ess$agea &lt; 14)) errors_c &lt;- c(errors_c, which(daten_ess$eisced == 3 &amp; daten_ess$agea &lt; 16)) errors_c &lt;- c(errors_c, which(daten_ess$eisced == 4 &amp; daten_ess$agea &lt; 16)) errors_c &lt;- c(errors_c, which(daten_ess$eisced == 5 &amp; daten_ess$agea &lt; 18)) errors_c &lt;- c(errors_c, which(daten_ess$eisced == 6 &amp; daten_ess$agea &lt; 20)) errors_c &lt;- c(errors_c, which(daten_ess$eisced == 7 &amp; daten_ess$agea &lt; 20)) sort(unique(c(errors_a, errors_b, errors_c))) ## [1] 352 443 Im Umgang mit diesen fehlenden Werten haben wir drei Möglichkeiten: Wir können die benötigen Angaben von anderen Werten ableiten (1), zum Beispiel das Alter vom Geburtsjahr. which(daten_ess$agea&gt;120) ## [1] 132 daten_ess$agea[132] ## [1] 320 daten_ess$agea[132] &lt;- 2016 - daten_ess$yrbrn[132] daten_ess$agea[132] ## [1] 32 Weiter können wir uns einen Wert berechnen bzw. diesen schätzen (2): which(daten_ess$edulvla==3 &amp; daten_ess$eisced==3 &amp; daten_ess$eduyrs&lt;6) ## [1] 1482 daten_ess$eduyrs[1482] &lt;- round(mean(daten_ess$eduyrs[daten_ess$edulvla==3], na.rm = T)) daten_ess$eduyrs[1482] ## [1] 11 Die letzte Option ist dann noch die falschen Angaben als fehlende Werte zu definieren (3): daten_ess$edulvla[443] &lt;- NA   Bonusaufgabe: In den folgenden Wochen möchten wir nicht mit dem ganzen Datensatz arbeiten, sondern lediglich mit einer zufälligen Auswahl von 200 Fällen. Versuchen Sie, einen solchen Teildatensatz randomisiert zu erstellen und speichern Sie ihn mittels der Funktion write.csv() unter dem Namen TD_ESS1-8e01.csv ab. Um den zufälligen Datensatz mit 200 Fällen zu erstellen bieten sich mindestens zwei Möglichkeiten an: Die erste Möglichkeit nutzt die sample()-Funktion und wählt zufällig 200 Zahlen einer Variable aus, die von 1 bis 1525 durchläuft. Eine solche Variable ist x im Datensatz  oder auch einfach die Zeilenzahl. Die zufällig ausgewählten Zahlen dieser Variable können als Angabe für die Zeilen in die eckige Klammerfunktion eingesetzt werden. daten_ess_HS20 &lt;- daten_ess[sample(row.names(daten_ess),200),] #oder auch: daten_ess_HS20 &lt;- daten_ess[sample(daten_ess$x,200),] Die zweite Möglichkeit berechnet eine Zufallsvariable und sortiert den Datensatz dann nach der Grösse dieser Zufallsvariable. Anschliessend können dann die  jetzt völlig zufällig  ersten 200 Fälle ausgewählt werden. daten_ess$zv &lt;- rnorm(1525) daten_ess &lt;- daten_ess[order(daten_ess$zv),] daten_ess_HS20 &lt;- daten_ess[1:200,1:22] Neben diesen beiden Möglichkeiten haben weitere Seminarteilnehmer*innen auch Ideen geliefert. Josias Bruder und Fabio Keller haben die erste Möglichkeit etwas ergänzt: daten_ess_HS20 &lt;- daten_ess[sample(1:nrow(daten_ess),200),] Ebenso wie Felix Sigrist: daten_ess_HS20 &lt;- daten_ess[sample(nrow(daten_ess),200),] Delia Bazzigher und Julien Lattmann schlagen weiter die Funktion sample_n aus dem dpylr Paket vor (dieses ist Teil der Kernpakete des tidyverse): #installed.packages(&quot;dplyr&quot;) library(dplyr) daten_ess_HS20 &lt;- sample_n(daten_ess, 200) Der zufällige erstellte Datensatz kann anschliessend wiederum als CSV-Datei abgespeichert werden (die Datei wird dann im Arbeitsordner abgelegt): write.csv(daten_ess_HS20, file = &quot;Daten/ESS1-8e01_HS21.csv&quot;) Die RStudio-Projekten erlauben es Ihnen, Ihre Arbeit in mehrere Kontexte aufteilen, die jeweils über ein eigenes Arbeitsverzeichnis, einen eigenen Arbeitsbereich, einen eigenen Verlauf und eigene Quelldokumente verfügen. Sie sind eben nochmals abgeschlossene Projekte innerhalb von RStudio und können über die Menüsteuerung File -&gt; New Project gestartet werden. Wiederum spezifiziert bei numerische Daten dass int ganze Zahlen und num komplexe Zahlen sind (mit Kommastellen, negative Werte, usw.) "],["wochenplan-08-kreuztabellen.html", "8 Wochenplan 08: Kreuztabellen 8.1 Lernziele 8.2 Aufgaben", " 8 Wochenplan 08: Kreuztabellen im Rahmen der 08. auf die 09.Einheit. 8.1 Lernziele Nachdem wir uns bereits etwas mit dem ESS-Datensatz und dessen Struktur vertraut gemacht haben, gehen wir nun zur Datenanalyse in R über. Dieser Übergang ist ein fliessender: Die Schritte zum Kennenlernen eines Datensatzes, die Plausibilitätsprüfung und die Suche nach Fehlern liefern häufig bereits erste Ergebnisse im Sinne einer explorativen Datenanalysen, die umgekehrt dann womöglich neue Schritte zur Aufbereitung des Datensatzes veranlassen. Der 08. Wochenplan fokussiert die Rolle von Kreuztabellen in diesem Prozess, inklusive der Berechnung von Chi-Quadrat-Tests. Von hier geht es dann in den Folgewochen schrittweise weiter zu Berechnung von Regressionsanalysen. Wir können daher folgende Lernziele festhalten: Sie verstehen den Nutzen (und auch die Gefahren) der Funktion attach(). Sie können Kreuztabellen auf verschiedene Weisen darstellen und inhaltlich zutreffend interpretieren. Sie lernen die Interpretation von inferenzstatstischen Test in R anhand von einem Chi-Quadrat Test kennen. Sie verstehen, wie man das Ergebnis von Funktionen als Objekt abspeichert, und wissen, wie man auf die Elemente solcher Ergebnisobjekte zugreift. Sie entwickeln ein vertieftes Verständnis davon, wie die Phasen der Datenaufbereitung, der Datenexploration und der Datenanalyse zusammenhängen. 8.2 Aufgaben Laden Sie den Datensatz in RStudio, der 200 zufällige Fälle aus dem ganzen ESS Datensatz enthält.11 Erfassen Sie den hierfür benötigten Code nicht nur im Markdown, sondern erstellen Sie zusätlich ein eigenes R-Skript (z.B. unter dem Namen ess_import.R). Dieses Skript sollte automatisch das richtige Arbeitsverzeichnis definieren, den Datensatz laden und alle notwendigen Datenaufbereitungen vornehmen. Sie können also auch bereits die ersten Definitionen von fehlenden Werten dort integrieren. Die Idee ist, dass dieses Skript über die kommenden Wochen umfangreicher wird, wenn wir Faktoren definieren, Variablenbezeichnungen ändern, neue Variablen berechnen, usw. Hier das Beispiel eines möglichen Importskripts: # Laden der Daten setwd(&quot;C:/Users/SchweglG/R_Daten/06_HS21/R_Seminar-HS21&quot;) #daten_ess &lt;- read.csv(file = &quot;Daten/ESS1-8e01.csv&quot;) #daten_ess &lt;- daten_ess[sample(row.names(daten_ess),200),] #write.csv(daten_ess, file = &quot;Daten/ESS1-8e01_HS21.csv&quot;) daten_ess &lt;- read.csv(file = &quot;Daten/ESS1-8e01_HS21.csv&quot;) # nicht benoetigte Variablen loeschen ## bisher keine nicht benoetigen Variablen vorhanden # Definieren von fehlenden Werten daten_ess$yrbrn[daten_ess$yrbrn==7777 | daten_ess$yrbrn==8888 | daten_ess$yrbrn==9999] &lt;- NA daten_ess$agea[daten_ess$agea==999] &lt;- NA daten_ess$edulvla[daten_ess$edulvla&gt;5] &lt;- NA daten_ess$eisced[daten_ess$eisced&gt;7] &lt;- NA daten_ess$eduyrs[daten_ess$eduyrs&gt;76] &lt;- NA # Fehler in den Faellen Korrigieren ## Fall 132 daten_ess$agea[daten_ess$x==132] &lt;- 2016 - daten_ess$yrbrn[daten_ess$x==132] daten_ess$agea[daten_ess$x==132] ## Fall 352 daten_ess$agea[daten_ess$x==352] &lt;- 2016 - daten_ess$yrbrn[daten_ess$x==352] ## Fall 443 daten_ess$edulvla[daten_ess$x==443] &lt;- NA ## Fall 1414 daten_ess$agea[daten_ess$x==1414] &lt;- NA daten_ess$yrbrn[daten_ess$x==1414] &lt;- NA ## Fall 1482 daten_ess$eduyrs[daten_ess$x==1482] &lt;- round(mean(daten_ess$eduyrs[daten_ess$edulvla==3], na.rm = T)) # Faktoren definieren ## folgt in der naechsten Einheit Das R-Skript kann nun laufend ergänzt werden, etwa mit der Definition von weiteren fehlenden Werte bei anderen Variablen oder mit der Erstellen von Faktoren (siehe nächster WP). Das hat den Vorteil, dass zu Beginn einer neuen Einheit (d.h. wenn Sie ein neues Markdown erstellen) jeweils nicht der ganze Code nochmals aufgeführt werden muss, sondern direkt dieses Skript ausgeführt werden kann. Das Ausführen eines Skriptes (oder auch einer sonstigen Textdatei) erfolgt über die Funktion source(). #Nochmals das Arbeitsverzeichnis definieren falls nötig: setwd(&quot;C:/Users/SchweglG/R_Daten/06_HS21/R_Seminar-HS21&quot;) #Aufrufen des R Skripts: source(file = &quot;ess_import.R&quot;) Zur Wiederholung: Wir laden uns also wiederum Rohdaten und müssen dann fehlende Werte definieren. Natürlich kennt die Funktion read.csv() ein Umgang mit fehlenden Werten über das Argument na.strings. Allerdings sind in unserem Rohdatensatz fehlende Werte eben auch als Zahlen definiert (z.Bsp. 7777) und werden  bevor dies nicht anders definiert haben  auch als numerische Werte eingelesen.   2. Erläutern Sie in eigenen Worten die Idee der Funktion attach(). Nutzen Sie diese Funktion für die Aufgaben 3 und 4. Wenden Sie den detach()-Befehl an, bevor Sie zu Aufgabe 5 übergehen. Die attach()-Funktion erlaubt es uns, einen bestimmten Datensatz (d.h. ein Dataframe-Objekt) zu aktivieren (Manderscheid 2017, 54). Dadurch können wir direkt auf diesen Datensatz zugreifen, ohne den Umweg des $-Zeichens: #Am Beispiel der Variable zu Geschlecht: ##...vor dem attachen: table(daten_ess$gndr) ## ## Male Female ## 100 100 ##die Funktion attach(daten_ess) ##...nach dem attachen: table(gndr) ## gndr ## Male Female ## 100 100 Ohne die attach()-Funktion würde die letzte Zeile des Code-Chunks oben zu einer Fehlermeldung führen. Fabio Keller hat dies nochmals etwas anders formuliert, um den Prozess der Funktion zu verdeutlichen: Standardmässig gibt es das Global Environment, also die Super-Umgebung, in der alle Objekte existieren, die wir bisher erstellt haben. Die Funktion attach() erstellt eine zusätzliche Umgebung, in der Objekte existieren können. Sie zerteilt ein DataFrame, das mit dem what Argument angegeben wird, in seine einzelnen Variablen und lässt sie als freie Objekte in der neuen Umgebung leben, dessen Name man mit dem name Argument spezifizieren kann. Wann sollten wir mit der Funktion arbeiten, wann nicht? Mit der Funktion zu arbeiten ist vor allem dann sinnvoll, wenn die Arbeit mit nur einem Datensatz im Zentrum steht und wenn keine Änderungen mehr am Datensatz selber vorgenommen werden. Bei der Arbeit mit mehreren Datensatzsätzen könnte die Funktion leicht für Verwirrung sorgen (gerade auch dann wenn Ihre Codezeilen von jemand anderem interpretiert werden sollen). Eine Hierarchie bei mehreren attachten Objekten könnte noch über das Argument postdefiniert werden. Dies macht allerdings kaum Sinn. Sollten Sie weiterhin Änderungen am Datensatz vornehmen kann die attach()-Funktion dazu führen, dass die von Ihnen gemachten Änderungen nicht sichtbar werden (beim Betrachten des Datensatzes) oder auch nicht in die weiteren Schritte/Berechnungen aufgenommen werden. Sobald also Änderungen vorgenommen werden gilt es auch den Datensatz wieder zu detachen. Nach folgender Befehlszeile kommt eine Warnung  was könnte das Problem sein? attach(daten_ess) Jetzt findet R verschiedene freie Objekte (Keller) mit dem selben Namen in der Environment. Dies aufgrund der Tatsache, dass eben ein Datensatz mehrmals attached wurde (ersichtlich über Global Enviroment). Wenn der Datensatz jetzt nur einmal detached wird kann immer noch direkt auf dieses Objekt zugreifen, weil es eben noch ein zweites Mal attached wurde: detach(daten_ess) table(gndr)   3. Erstellen Sie mittels der Funktion table() eine Kreuztabelle zwischen der Variable zu Geschlecht und derjenigen zum Interesse an Politik. Wie können Sie vorgehen, um eine Kreuztabelle mit Prozentwerten (relative statt absolute Häufigkeiten) zu erstellen, die ausserdem die Randsummen enthält? Suchen Sie eine Darstellung der Tabelle - d.h. der Anordnung der Variablen in Zeilen und Spalten sowie der Wahl der Randsummen -, die sinnvoll das Verhältnis von abhängiger und unabhängiger Variable wiedergibt. Was fällt Ihnen inhaltlich an der Tabelle auf? Wenn wir Tabellen (bzw. Kontingenztabellen) darstellen, so ist die Konvention dass bei gerichteten Beziehungen die Ausprägungen der unabhängigen Variablen (X) den Spalten zugeordnet werden und die Ausprägungen der abhängigen Variablen (Y) den Reihen zugeordnet werden (Diaz-Bone 2019, 70). Die Aufgabenstellung spricht zwar nicht von einer gerichteten Beziehung, aber gleichzeitig macht es Sinn, Geschlecht als unabhängige Variable anzusehen (und nicht dass das Interesse an Politik das Geschlecht einer Person bestimmt). Gemäss dieser Überlegung sollen dann auch die Randsummen ausgerichtet werden, da uns die Verteilungen des Politikinteresses je nach Geschlecht interessiert. Das heisst das pro Geschlecht jeweils auf 100% summiert werden soll. Zur Erinnerung: Dies sind Ausprägungen der beiden Variablen: gndr 1 Male 2 Female 9 No answer polintr 1 Very interested 2 Quite interested 3 Hardly interested 4 Not at all interested 7 Refusal 8 Dont know 9 No answer Die Verteilung von gndr haben wir oben bereits betrachtet, daher folgt hier nur noch die Variable polintr: table(polintr) ## polintr ## Not at all interested Hardly interested Quite interested ## 26 63 82 ## Very interested ## 29 # Womöglich verfügt die Variable &quot;polintr&quot; über fehlende Werte. Diese können wir definieren: daten_ess$polintr[daten_ess$polintr&gt;4] &lt;- NA #Dieser Befehl könnten wir dann ebenfalls in unser Import-Skript übernehmen. Anschliessend erstellen wir die benötige Tabelle. Hierbei lohnt es sich von innen nach aussen vorzugehen (vgl. auch Manderscheid 2017, 95): #als logischer Aufbau der Tabelle addmargins(table(polintr, gndr)) prop.table(table(polintr, gndr)) # Welche Variante macht nun mehr Sinn? prop.table(table(polintr, gndr),2) prop.table(table(polintr, gndr),1) #Randsummen ergänzen: addmargins(prop.table(table(polintr, gndr),2)) #Warum könnten wir eine der beiden Randsummen weglassen? Wieso nicht? # V1 addmargins(prop.table(table(polintr, gndr),2),1) round(addmargins(prop.table(table(polintr, gndr),2),1),2) #V2 round(addmargins(prop.table(table(polintr, gndr),2)),2) Hier wäre nun unsere benötigte Tabelle: round(addmargins(prop.table(table(polintr, gndr),2),1),2) ## gndr ## polintr Male Female ## Not at all interested 0.14 0.12 ## Hardly interested 0.28 0.35 ## Quite interested 0.40 0.42 ## Very interested 0.18 0.11 ## Sum 1.00 1.00 Diese verdeutlicht, dass zwischen den Variablen ein Zusammenhang bestehen könnte, da Männer anscheinend öfters angeben, sehr an Politik interessiert zu sein.   4. Die Funktion CrossTable() ist Teil des Paketes gmodels und ermöglicht die flexible und detailreiche Arbeit mit Kreuztabellen. Versuchen Sie mittels dieser Funktion die Kreuztabelle aus Aufgabe 2 nachzubauen. #install.packages(&quot;gmodels&quot;) library(gmodels) ?CrossTable ## starting httpd help server ... done Ein Blick in die Hilfeseite zeigt uns, dass wir vor allem diejenigen Argument anders definieren müssen, die wir nicht in unserer Tabelle haben wollen. Das heisst wir müssen angeben (FALSE), dass wir keine Spalten- und Tabellenprozente sowie keine Angaben zum Chi-Quadrat-Beiträgen haben möchten. Leider scheint aber kein Argument vorhanden zu sein, mit der wir die absoluten Zahlen entfernen und die Randverteilung auch noch gemäss der Tabelle oben darstellen könnten. CrossTable(polintr, gndr, digits = 2, prop.r = F, prop.t = F, prop.chisq = F, ) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Col Total | ## |-------------------------| ## ## ## Total Observations in Table: 200 ## ## ## | gndr ## polintr | Male | Female | Row Total | ## ----------------------|-----------|-----------|-----------| ## Not at all interested | 14 | 12 | 26 | ## | 0.14 | 0.12 | | ## ----------------------|-----------|-----------|-----------| ## Hardly interested | 28 | 35 | 63 | ## | 0.28 | 0.35 | | ## ----------------------|-----------|-----------|-----------| ## Quite interested | 40 | 42 | 82 | ## | 0.40 | 0.42 | | ## ----------------------|-----------|-----------|-----------| ## Very interested | 18 | 11 | 29 | ## | 0.18 | 0.11 | | ## ----------------------|-----------|-----------|-----------| ## Column Total | 100 | 100 | 200 | ## | 0.50 | 0.50 | | ## ----------------------|-----------|-----------|-----------| ## ## Hingegen bietet die Funktion über das Argument format noch eine schöne Darstellungsweise, die Josias Bruder genutzt hat: CrossTable(polintr, gndr, digits = 2, prop.r = F, prop.t = F, prop.chisq = F, format = &quot;SPSS&quot;) ## ## Cell Contents ## |-------------------------| ## | Count | ## | Column Percent | ## |-------------------------| ## ## Total Observations in Table: 200 ## ## | gndr ## polintr | Male | Female | Row Total | ## ----------------------|-----------|-----------|-----------| ## Not at all interested | 14 | 12 | 26 | ## | 14.00% | 12.00% | | ## ----------------------|-----------|-----------|-----------| ## Hardly interested | 28 | 35 | 63 | ## | 28.00% | 35.00% | | ## ----------------------|-----------|-----------|-----------| ## Quite interested | 40 | 42 | 82 | ## | 40.00% | 42.00% | | ## ----------------------|-----------|-----------|-----------| ## Very interested | 18 | 11 | 29 | ## | 18.00% | 11.00% | | ## ----------------------|-----------|-----------|-----------| ## Column Total | 100 | 100 | 200 | ## | 50.00% | 50.00% | | ## ----------------------|-----------|-----------|-----------| ## ## Hinweis: Die Funktion CrossTable() kennt keinen Weg mit fehlenden Wert umzugehen. Sollten Sie also einen fehlenden Wert in einer Variable haben müssen Sie diesen selber entfernen. Hier ein Beispiel, in dem ein fehlender Wert beim Fall 45 (z.Bsp. bei der Variable polintr) vorhanden wäre: CrossTable(polintr[-c(45)], gndr[-c(45)], digits = 2, prop.r = F, prop.t = F, prop.chisq = F, format = &quot;SPSS&quot;)   5. Mittels CrossTable() lassen sich nicht nur Tabellen erstellen, sondern auch Chi-Quadrat Tests durchführen. + Kopieren Sie Ihren Kode der Kreuztabelle aus Ausgabe 4, ergänzen Sie diesen um das nötige Argument für den Chi-Quadrat Test und speichern Sie dies als ein neues Objekt CT_Ergebnis. + Greifen Sie anschliessend auf die darin enthaltenen Angaben zum Chi-Quadrat Test zu und interpretieren Sie diese - sowohl deskriptiv als auch inferenzstatistisch. detach(daten_ess) CT_Ergebnis &lt;- CrossTable(daten_ess$polintr, daten_ess$gndr, digits = 2, prop.r = F, prop.t = F, prop.chisq = F, chisq = T) #Achtung: hier muss nun das Format &#39;SPSS&#39; entfernt werden ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Col Total | ## |-------------------------| ## ## ## Total Observations in Table: 200 ## ## ## | daten_ess$gndr ## daten_ess$polintr | Male | Female | Row Total | ## ----------------------|-----------|-----------|-----------| ## Not at all interested | 14 | 12 | 26 | ## | 0.14 | 0.12 | | ## ----------------------|-----------|-----------|-----------| ## Hardly interested | 28 | 35 | 63 | ## | 0.28 | 0.35 | | ## ----------------------|-----------|-----------|-----------| ## Quite interested | 40 | 42 | 82 | ## | 0.40 | 0.42 | | ## ----------------------|-----------|-----------|-----------| ## Very interested | 18 | 11 | 29 | ## | 0.18 | 0.11 | | ## ----------------------|-----------|-----------|-----------| ## Column Total | 100 | 100 | 200 | ## | 0.50 | 0.50 | | ## ----------------------|-----------|-----------|-----------| ## ## ## Statistics for All Table Factors ## ## ## Pearson&#39;s Chi-squared test ## ------------------------------------------------------------ ## Chi^2 = 2.67006 d.f. = 3 p = 0.4453394 ## ## ## summary(CT_Ergebnis) ## Length Class Mode ## t 8 table numeric ## prop.row 8 table numeric ## prop.col 8 table numeric ## prop.tbl 8 table numeric ## chisq 9 htest list str(CT_Ergebnis) ## List of 5 ## $ t : &#39;table&#39; int [1:4, 1:2] 14 28 40 18 12 35 42 11 ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ x: chr [1:4] &quot;Not at all interested&quot; &quot;Hardly interested&quot; &quot;Quite interested&quot; &quot;Very interested&quot; ## .. ..$ y: chr [1:2] &quot;Male&quot; &quot;Female&quot; ## $ prop.row: &#39;table&#39; num [1:4, 1:2] 0.538 0.444 0.488 0.621 0.462 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ x: chr [1:4] &quot;Not at all interested&quot; &quot;Hardly interested&quot; &quot;Quite interested&quot; &quot;Very interested&quot; ## .. ..$ y: chr [1:2] &quot;Male&quot; &quot;Female&quot; ## $ prop.col: &#39;table&#39; num [1:4, 1:2] 0.14 0.28 0.4 0.18 0.12 0.35 0.42 0.11 ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ x: chr [1:4] &quot;Not at all interested&quot; &quot;Hardly interested&quot; &quot;Quite interested&quot; &quot;Very interested&quot; ## .. ..$ y: chr [1:2] &quot;Male&quot; &quot;Female&quot; ## $ prop.tbl: &#39;table&#39; num [1:4, 1:2] 0.07 0.14 0.2 0.09 0.06 0.175 0.21 0.055 ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ x: chr [1:4] &quot;Not at all interested&quot; &quot;Hardly interested&quot; &quot;Quite interested&quot; &quot;Very interested&quot; ## .. ..$ y: chr [1:2] &quot;Male&quot; &quot;Female&quot; ## $ chisq :List of 9 ## ..$ statistic: Named num 2.67 ## .. ..- attr(*, &quot;names&quot;)= chr &quot;X-squared&quot; ## ..$ parameter: Named int 3 ## .. ..- attr(*, &quot;names&quot;)= chr &quot;df&quot; ## ..$ p.value : num 0.445 ## ..$ method : chr &quot;Pearson&#39;s Chi-squared test&quot; ## ..$ data.name: chr &quot;t&quot; ## ..$ observed : &#39;table&#39; int [1:4, 1:2] 14 28 40 18 12 35 42 11 ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ x: chr [1:4] &quot;Not at all interested&quot; &quot;Hardly interested&quot; &quot;Quite interested&quot; &quot;Very interested&quot; ## .. .. ..$ y: chr [1:2] &quot;Male&quot; &quot;Female&quot; ## ..$ expected : num [1:4, 1:2] 13 31.5 41 14.5 13 31.5 41 14.5 ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ x: chr [1:4] &quot;Not at all interested&quot; &quot;Hardly interested&quot; &quot;Quite interested&quot; &quot;Very interested&quot; ## .. .. ..$ y: chr [1:2] &quot;Male&quot; &quot;Female&quot; ## ..$ residuals: &#39;table&#39; num [1:4, 1:2] 0.277 -0.624 -0.156 0.919 -0.277 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ x: chr [1:4] &quot;Not at all interested&quot; &quot;Hardly interested&quot; &quot;Quite interested&quot; &quot;Very interested&quot; ## .. .. ..$ y: chr [1:2] &quot;Male&quot; &quot;Female&quot; ## ..$ stdres : &#39;table&#39; num [1:4, 1:2] 0.421 -1.066 -0.288 1.406 -0.421 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ x: chr [1:4] &quot;Not at all interested&quot; &quot;Hardly interested&quot; &quot;Quite interested&quot; &quot;Very interested&quot; ## .. .. ..$ y: chr [1:2] &quot;Male&quot; &quot;Female&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;htest&quot; Neben den verschiedenen Werten der Tabellen (Fallzahlen sowie den Spalten-, Reihen- und Tabellenprozenten) finden wir auch das Unterobjekt des Chi-Quadrattests: chisq. Auf dieses können wir sowohl insgesamt zugreifen CT_Ergebnis$chisq ## ## Pearson&#39;s Chi-squared test ## ## data: t ## X-squared = 2.6701, df = 3, p-value = 0.4453 #oder etwas umständlicher, aber auch möglich: CT_Ergebnis[&quot;chisq&quot;] ## $chisq ## ## Pearson&#39;s Chi-squared test ## ## data: t ## X-squared = 2.6701, df = 3, p-value = 0.4453 als eben auch auf die Unterelemente: CT_Ergebnis$chisq$statistic ## X-squared ## 2.67006 CT_Ergebnis$chisq$parameter ## df ## 3 CT_Ergebnis$chisq$p.value ## [1] 0.4453394 CT_Ergebnis$chisq$method ## [1] &quot;Pearson&#39;s Chi-squared test&quot; CT_Ergebnis$chisq$data.name ## [1] &quot;t&quot; CT_Ergebnis$chisq$observed ## y ## x Male Female ## Not at all interested 14 12 ## Hardly interested 28 35 ## Quite interested 40 42 ## Very interested 18 11 CT_Ergebnis$chisq$expected ## y ## x Male Female ## Not at all interested 13.0 13.0 ## Hardly interested 31.5 31.5 ## Quite interested 41.0 41.0 ## Very interested 14.5 14.5 CT_Ergebnis$chisq$residuals ## y ## x Male Female ## Not at all interested 0.2773501 -0.2773501 ## Hardly interested -0.6236096 0.6236096 ## Quite interested -0.1561738 0.1561738 ## Very interested 0.9191450 -0.9191450 Wie können wir die Ergebnisse nun interpretieren? Zuerst deskriptiv statistisch: Wir erhalten einen Chi-Quadrat Wert gemäss Pearson von: CT_Ergebnis$chisq$statistic ## X-squared ## 2.67006 Das heisst dass ein Zusammenhang zwischen den beiden Variablen Geschlecht und Politikinteresse besteht. Allerdings ist die Grössenordnung dieses Wertes nicht nur durch eine Stärke des Zusammenhangs bestimmt, sondern auch durch das Tabellenformat (Diaz-Bone 2019, 86). Dieses Tabellenformat wird über die Freiheitsgrade (df) deutlich: (i-1) x (j-1). Oder eben: CT_Ergebnis$chisq$parameter ## df ## 3 Möchte man hingegen verschiedene Zusammenhänge und deren Stärke miteinander vergleichen, so kann man auf Cramers V zurückgreifen (etwa über die Funktion cramersV() im Paket lsr). #install.packages(&quot;lsr&quot;) library(lsr) cramersV(daten_ess$polintr, daten_ess$gndr) ## [1] 0.1155435 #oder auch manuell: sqrt(CT_Ergebnis$chisq$statistic/(200*(2-1))) ## X-squared ## 0.1155435 Cramers V verweist so auf einen schwachen bis mittleren Zusammenhang. Anschliessend können wir zur inferenzstatistischen Betrachtung übergehen: Einige von den Teilnehmer*innen haben den Chi-Quadrat Test von Hand durchgeführt. Dies ist natürlich auch möglich (vgl. Diaz-Bone 2019, 86): Hypothesen H0: In der Grundgesamtheit besteht kein Zusammenhang zwischen Geschlecht und Politikinteresse. H1: Es besteht in der Grundgesamtheit ein Zusammenhang zwischen Geschlecht und Politikinteresse. Stichprobnerverteilung Wiederum können wir die Stichprobernvertilung mittels des parameter-Unterobjektes bestimmen, nämlich als Verteilung mittels 3 Freiheitsgraden: CT_Ergebnis$chisq$parameter ## df ## 3 Annahme- und Rückweisungsbereich Der Test soll mit einem Signifikanzniveau von 5% durchgeführt werden: qchisq(0.95,3, lower.tail = T) ## [1] 7.814728 Auswertung der STichprobe und Testentscheidung Der Stichprobenkennwert ist: CT_Ergebnis$chisq$statistic ## X-squared ## 2.67006 Dieser Wert fällt nun in den Rückweisungsbereich und der Stichprobenbefund ist signifikant. Die H0 muss verworfen werden und auch in der Grundgesamtheit besteht ein Zusammenhang zwischen Geschlecht und Politikinteresse. Dieses Vorgehen entspricht allerdings nicht der Realität des statistischen Arbeitens. Wir arbeiten nämlich direkt mit dem sogenannten p-Wert. Dieser Wert liegt bei: CT_Ergebnis$chisq$p.value ## [1] 0.4453394 Unser berechnete Prüfwert Chi-Quadrat fällt auf einem 5% Signifikanzniveau in den Ablehnungsbereich beziehungsweise können wir auf einem 95% Signifikanzniveau die Alternativhypothese annehmen, dass auch in der Grundgesamtheit ein Zusammenhang zwischen den beiden Variablen besteht (vgl. auch Manderscheid 2017, 165f). Der p-Wert dreht dabei die Logik etwas um: Konkret sagt dieser Wert nämlich aus, dass lediglich eine round(CT_Ergebnis$chisq$p.value,3)*100 ## [1] 44.5 % Wahrscheinlichkeit besteht, dass unser Zusammenhang in der Stichprobe zufällig aufgetreten ist. Ist jetzt das einfach ein dummes Beispiel? Wir könnten uns jetzt nochmals anschauen, wie dieser Chi-Quadrat Wert zustandekommt: CrossTable(daten_ess$polintr, daten_ess$gndr, prop.r = F, prop.t = F, prop.c = F) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | Chi-square contribution | ## |-------------------------| ## ## ## Total Observations in Table: 200 ## ## ## | daten_ess$gndr ## daten_ess$polintr | Male | Female | Row Total | ## ----------------------|-----------|-----------|-----------| ## Not at all interested | 14 | 12 | 26 | ## | 0.077 | 0.077 | | ## ----------------------|-----------|-----------|-----------| ## Hardly interested | 28 | 35 | 63 | ## | 0.389 | 0.389 | | ## ----------------------|-----------|-----------|-----------| ## Quite interested | 40 | 42 | 82 | ## | 0.024 | 0.024 | | ## ----------------------|-----------|-----------|-----------| ## Very interested | 18 | 11 | 29 | ## | 0.845 | 0.845 | | ## ----------------------|-----------|-----------|-----------| ## Column Total | 100 | 100 | 200 | ## ----------------------|-----------|-----------|-----------| ## ## Und dort sehen wir, dass vor allem die Unterschiede in der ersten Ausprägung (polintr = 1) die Differenz ausmachen. References "],["wochenplan-09-faktoren.html", "9 Wochenplan 09: Faktoren 9.1 Lernziele 9.2 Einführung Faktoren (Lösung Aufgaben)", " 9 Wochenplan 09: Faktoren im Rahmen der 09. und 10.Einheit. 9.1 Lernziele Aufbauend auf unserer bisherigen Arbeit mit Kreuztabellen besteht das Lernziel für diese Woche darin, dass Sie sich mit einem für die tägliche Arbeit mit R wichtigen Datenformat vertraut machen: Faktoren. Faktoren sind eine geeignete Art, kategoriale  d.h. nominale und ordinale  Daten in R zu repräsentieren. Der Umgang mit dieser Art von Daten ist allerdings nicht ganz leicht und erfordert ein wenig Übung. Die folgenden konkreten Lernziele lassen sich für ein erstes Kennenlernen von Faktoren formulieren: Sie können dafür geeignete Variablen in Faktoren umwandeln. Sie verstehen den Unterschied zwischen geordneten und ungeordneten Faktoren und können beide in R erstellen. Sie verstehen die spezifische Zwei-Ebenen-Struktur von Faktoren und insbesondere die Rolle von Levels. Sie wissen, wie man die Levels eines Faktors definiert. Sie wissen, wie man Levels neu benennt, umsortiert, zusammenfasst und/oder nicht benötigte Levels entfernt. 9.2 Einführung Faktoren (Lösung Aufgaben) Einführung: Neben Vektoren (die wir bisher vor allem mit metrischen Variablen gleichgesetzt haben) sind Faktoren ein weiterer wichtiger Objekttyp der für die Organisation von sozialwissenschaftlichen Beobachtungen in Form von Variablen relevant ist (Manderscheid 2017, 35). Dieser Objekttyp enthält nun nicht mehr die Zahlen in einer Reihe, sondern Levels. Sie legen eine beschränkte Auswahl an Ausprägungen fest, welche eine Variable annehmen kann. Die Levels representieren dann benannte Elemente (die Labels) und wir können diese insbesondere für Variablen mit nominalen oder ordinalen Skalenniveau verwenden. Der Unterschied zwischen numerischen Variablen/Vektoren und kategorial Variablen/Faktoren wird in R wichtig, denn zwischen den Objekttypen wird differenziert. Das heisst das gewisse Funktionen nicht mehr mit Faktoren ausgeführt werden können. Im folgenden wird der Umgang mit Faktoren über die Möglichkeiten der Basisversion von R erläutert. Im Buch R for Data Science (Wickham and Grolemund 2016, Kap.15) wird als mögliche Vertiefung das Paket forcats eingeführt, das Teil des sogenannten Tidyverse ist. Es bietet eine Vielfalt an Werkzeuge für den Umgang mit kategorialen Variablen und Faktoren. Die Frage, welche Variablen wirklich zu Faktoren umgewandelt werden sollten, ist dabei gar nicht immer so klar. Drei Beispiele: agea, pdwrk und happy  welche davon sollten zu Faktoren werden? Die Variable zu Alter ist ohne Zweifel eine metrische Variable und wir sollten sie als Vektor im Datensatz belassen. Die zweite Variable pdwrk ist hingegen klar eine nominale Variable und daher ein Faktor. Allerdings könnten wir diese eigentlich auch als numerische Variable im Datensatz belassen, da sie 0-1 kodiert ist. Solche Dummy-Variablen können wir in der Form oftmals in Verfahren integrieren (etwa bei Korrelationen oder Regressionen). Die Variable happy wiederum ist streng genommen eine ordinale Variable, und deshalb eigentlich ein Faktor. Allerdings behandeln wir Variablen mit über 5-6 Abstufungen meistens als metrische Variablen. Happy sollte daher als Vektor im Datensatz belassen werden. Auch von Josias Bruder hat eine Frage zur Diskussion gestellt: Entsprechen edulvla und eisced geordneten oder ungeordneten Faktoren? Gewissermassen stehen die einzelnen Ausbildungsstufen in einer Hierarchie, allerdings ist die Reihenfolge aufgrund des Wertes 0 (not possible to harmonise) unzulässig. Auch wirft die Bewertung von unterschiedenen Bildungslevel Fragen auf (im Sinne: was ist mehr Wert? / Besser?). Hier bietet sich eine einfach pragmatische Antwort auf der einen und eine etwas komplexere Antwort auf der anderen Seite an. Die pragmatische Antwort würde die Variable geordnet kodieren und akzeptieren, dass wir hier gesellschaftliche Vorstellungen von besseren Bildungsniveaus übernehmen. Gegebenenfalls könnte die Variable in einer zweiten Weise als ungeordneter Faktor abgespeichert werden um auch die Ausprägung 0 abzubilden. Die komplexere Antwort auf die Frage wäre, dass auch eine solche anscheinende Hierarchie zu einem Forschungsgegenstand werden kann. Siehe auch hierfür die Ergebnisse folgender Studie, die feststellte, dass sich Löhne und Arbeitsmarkterfolg für verschiedene Bildungsabschlüsse weitgehend parallel entwickelt. Trotz all dieser Schwierigkeiten können die verschiedenen Variablen wie folgt aufgeteilt werden: cntry: Charakter-Konstante essround: numerische Konstante idno, pspwght, pweight, yrbrn, agea, eduyrs &amp; wkhtot : numerische Variablen happy &amp; hinctna: ebenfalls numerische Varablen (vgl. Anmerkungen unten) gndr, chldhm, pdwrk, edctn, uemp3m &amp; isco08: ungeordnete Faktoren polintr, edulvla &amp; eisced: geordnete Faktoren Wiederholung zu geordneten und ungeordneten Faktoren: Im folgenden wollen wir nun einige Faktoren im Datensatz definieren, etwa eine ungeordnete oder eben nominale Variable (gndr): table(daten_ess$gndr) ## ## 1 2 ## 100 100 daten_ess$gndr &lt;- factor(daten_ess$gndr) levels(daten_ess$gndr) &lt;- c(&quot;Male&quot;, &quot;Female&quot;) table(daten_ess$gndr) ## ## Male Female ## 100 100 Wir können weiter auch unnötige/nicht benötigte Levels in einem (ungeordneten) Faktor entfernen (uemp3m): table(daten_ess$uemp3m) ## ## 1 2 ## 48 152 daten_ess$uemp3m &lt;- factor(daten_ess$uemp3m) levels(daten_ess$uemp3m) &lt;- c(&quot;Has been unemployed&quot;, &quot;Has never been unemployed&quot;, &quot;Refusal&quot;, &quot;Don&#39;t know&quot;, &quot;No answer&quot;) table(daten_ess$uemp3m) ## ## Has been unemployed Has never been unemployed Refusal ## 48 152 0 ## Don&#39;t know No answer ## 0 0 #Wir sehen nun, dass einige Levels keine Ausprägungen haben. Diese können wir anschliessend wieder entfernen über &#39;drop&#39;: str(daten_ess$uemp3m) ## Factor w/ 5 levels &quot;Has been unemployed&quot;,..: 2 2 2 2 2 1 1 2 1 1 ... ?drop daten_ess$uemp3m &lt;- daten_ess$uemp3m[drop=T] table(daten_ess$uemp3m) ## ## Has been unemployed Has never been unemployed ## 48 152 str(daten_ess$uemp3m) ## Factor w/ 2 levels &quot;Has been unemployed&quot;,..: 2 2 2 2 2 1 1 2 1 1 ... Ebenfalls können wir einen geordneten Faktoren im Datensatz definieren anhand eines Beispiels (eisced): daten_ess$eisced &lt;- factor(daten_ess$eisced, ordered = T) levels(daten_ess$eisced) &lt;- c(&quot;ES-ISCED I&quot;, &quot;ES-ISCED II&quot;, &quot;ES-ISCED IIIb&quot;, &quot;ES-ISCED IIIa&quot;, &quot;ES-ISCED IV&quot;, &quot;ES-ISCED V1&quot;, &quot;ES-ISCED V2&quot;) #Eine etwas andere Überprüfung: table(as.numeric(daten_ess$eisced), daten_ess$eisced) Als Hinweis: Für solche (Kreuz-)Tabellen zur Überprüfung bietet sich die einfache table()-Funktion an. Natürlich kann auch CrossTable() des Paketes gmodels genutzt werden  die Funktion scheint aber etwas zu komplex hierfür zu sein. Die Variable polintr als weiterer geordneter Faktor stellt uns vor ein Problem, da die Ausprägung 1 (und nicht etwa 4) dem höchsten Wert entspricht: table(daten_ess$polintr) ## ## 1 2 3 4 ## 29 82 63 26 daten_ess$polintr_v1 &lt;- factor(daten_ess$polintr, ordered = T) levels(daten_ess$polintr_v1) &lt;- c(&quot;Very interested&quot;, &quot;Quite interested&quot;, &quot;Hardly interested&quot;, &quot;Not at all interested&quot;) # So scheint die Reihenfolge noch nicht zu stimmen: str(daten_ess$polintr_v1) ## Ord.factor w/ 4 levels &quot;Very interested&quot;&lt;..: 3 3 1 1 1 2 4 2 3 4 ... # Und diese Lösung scheint nur fehlende Werte zu ergeben? daten_ess$polintr_v1 &lt;- ordered(daten_ess$polintr, levels = c(&quot;Not at all interested&quot;, &quot;Hardly interested&quot;, &quot;Quite interested&quot;, &quot;Very interested&quot;)) str(daten_ess$polintr_v1) ## Ord.factor w/ 4 levels &quot;Not at all interested&quot;&lt;..: NA NA NA NA NA NA NA NA NA NA ... # Wir müssen also einen Umgang machen: daten_ess$polintr_v2 &lt;- factor(daten_ess$polintr, levels =c(4,3,2,1), ordered = T) str(daten_ess$polintr_v2) ## Ord.factor w/ 4 levels &quot;4&quot;&lt;&quot;3&quot;&lt;&quot;2&quot;&lt;&quot;1&quot;: 2 2 4 4 4 3 1 3 2 1 ... levels(daten_ess$polintr_v2) &lt;- c(&quot;Not at all interested&quot;, &quot;Hardly interested&quot;, &quot;Quite interested&quot;, &quot;Very interested&quot;) str(daten_ess$polintr_v2) ## Ord.factor w/ 4 levels &quot;Not at all interested&quot;&lt;..: 2 2 4 4 4 3 1 3 2 1 ... table(daten_ess$polintr_v2) ## ## Not at all interested Hardly interested Quite interested ## 26 63 82 ## Very interested ## 29 Die Levels scheinen also auch direkt in der factor()-Funktion integrierbar zu sein. Lässt sich dies auch bei einer anderen (ungeordneten) Faktorvariable übernehmen (chldhm)? #chldhm table(daten_ess$chldhm) ## ## 1 2 ## 70 130 daten_ess$chldhm_v1 &lt;- factor(daten_ess$chldhm, levels = c(&quot;Respondent lives with children at household grid&quot;, &quot;Does not&quot;, &quot;Not available&quot;)) table(daten_ess$chldhm_v1) ## ## Respondent lives with children at household grid ## 0 ## Does not ## 0 ## Not available ## 0 daten_ess$chldhm_v2 &lt;- factor(daten_ess$chldhm, levels = c(&quot;Respondent lives with children at household grid&quot;, &quot;Does not&quot;)) table(daten_ess$chldhm_v2) ## ## Respondent lives with children at household grid ## 0 ## Does not ## 0 Dies scheint nicht zu funktionieren. Was könnte hier nun das Problem gewesen sein? Für ein besseres Verständnis von Faktoren ist es lohnenswert, sich die Unterscheidung von Levels und Labels zu vergegenwärtigen (vgl. auch hier). Levels und Labels: Die Levels definieren, was in einem Faktor reingehen kann, also was von diesem als gültiges Datum angesehen wird. Die Labels bestimmen, was anschliessend vom Faktor ausgegeben wird. Wenn wir aber im Code die Definition des Faktors und die Definition von dessen Levels trennen, ignorieren wir diese Unterscheidung: daten_ess$chldhm &lt;- factor(daten_ess$chldhm) levels(daten_ess$chldhm) &lt;- c(&quot;Respondent lives with children at household grid&quot;, &quot;Does not&quot;, &quot;Not available&quot;) daten_ess$chldhm &lt;- daten_ess$chldhm[drop=T] table(daten_ess$chldhm) ## ## Respondent lives with children at household grid ## 70 ## Does not ## 130 Wir können uns den Unterschied vergegenwärtigen, hier am Beispiel einer weiteren, ungeordneten kategorialen Variable (pdwrk): table(daten_ess$pdwrk) ## ## 0 1 ## 65 135 #Hier kodieren wir die Variable zu einem Charakterformat um und bauen einen Schreibfehler ein: daten_ess$pdwrk[daten_ess$pdwrk==0] &lt;- &quot;not marked&quot; daten_ess$pdwrk[daten_ess$pdwrk==1] &lt;- &quot;marked&quot; daten_ess$pdwrk[18] &lt;- &quot;markwd&quot; class(daten_ess$pdwrk) ## [1] &quot;character&quot; table(daten_ess$pdwrk) ## ## marked markwd not marked ## 134 1 65 #Anschliessend definieren wir die Variable als Faktor und bestimmen ihre Levels: daten_ess$pdwrk &lt;- factor(daten_ess$pdwrk, levels = c(&quot;not marked&quot;, &quot;marked&quot;)) table(daten_ess$pdwrk) ## ## not marked marked ## 65 134 #Unser Schreibfehler wurde daher nicht akzeptiert (&quot;er geht nicht in die Varialbe rein&quot;): which(is.na(daten_ess$pdwrk)) ## [1] 18 # Jetzt definieren wir in einem zweiten Schritt die Labels: daten_ess$pdwrk &lt;- factor(factor(daten_ess$pdwrk), labels = c(&quot;no paidwork for the last 7&quot;, &quot;has been doing paid work&quot;)) table(daten_ess$pdwrk) ## ## no paidwork for the last 7 has been doing paid work ## 65 134 Diese Unterscheidung können wir nun gleichzeitig nutzen, etwa bei ungeordneten Faktoren (edctn): table(daten_ess$edctn) ## ## 0 1 ## 186 14 daten_ess$edctn &lt;- factor(daten_ess$edctn, levels = c(0,1), labels = c(&quot;Not marked&quot;, &quot;Marked&quot;)) table(daten_ess$edctn) ## ## Not marked Marked ## 186 14 Ebenfalls können wir diesen Unterschied zwischen Levels und Labels nutzen, um fehlende Werte direkt auszuschliessend, etwa bei folgenden geordneten Faktor (edulvla): table(daten_ess$edulvla) ## ## 1 2 3 4 5 ## 6 41 88 6 56 daten_ess$edulvla &lt;- factor(daten_ess$edulvla, levels = c(1,2,3,4,5), labels = c(&quot;ISCED 0-1&quot;, &quot;ISCED 2&quot;, &quot;ISCED 3&quot;, &quot;ISCED 4&quot;, &quot;ISCED 5-6&quot;), ordered = T) table(daten_ess$edulvla) ## ## ISCED 0-1 ISCED 2 ISCED 3 ISCED 4 ISCED 5-6 ## 6 41 88 6 56 str(daten_ess$edulvla) ## Ord.factor w/ 5 levels &quot;ISCED 0-1&quot;&lt;&quot;ISCED 2&quot;&lt;..: 5 4 5 3 NA 5 2 3 5 2 ... summary(daten_ess$edulvla) ## ISCED 0-1 ISCED 2 ISCED 3 ISCED 4 ISCED 5-6 NA&#39;s ## 6 41 88 6 56 3 Als Hinweis: Da die Variable edulvla als geordneter Faktor definiert wird, wurde auch die Ausprägung Other (55) ausgeschlossen. Diese könnte als gültige Ausprägung integriert werden, wenn die Variable als ungeordnet-kategorial/nominal definiert würde. Nicht zuletzt hilft uns die Unterscheidungen von Levels und Labels bei dem komplizierten Fall der Variable polintr: table(daten_ess$polintr) ## ## 1 2 3 4 ## 29 82 63 26 daten_ess$polintr &lt;- factor(daten_ess$polintr, levels = c(4,3,2,1), labels = c(&quot;Not at all interested&quot;, &quot;Hardly interested&quot;, &quot;Quite interested&quot;, &quot;Very interested&quot;), ordered = T) str(daten_ess$polintr) ## Ord.factor w/ 4 levels &quot;Not at all interested&quot;&lt;..: 2 2 4 4 4 3 1 3 2 1 ... table(daten_ess$polintr) ## ## Not at all interested Hardly interested Quite interested ## 26 63 82 ## Very interested ## 29 Eine kleine Aufgabe: Wir können uns den Unterschied zwischen Levels und Labels auch nochmals an einer kleinen Aufgabe verdeutlichen. Versuchen Sie aus den beiden Variablen edctn und pdwrk eine neue Variable beschaeft zu erstellen. table(daten_ess$edctn, daten_ess$pdwrk) ## ## no paidwork for the last 7 has been doing paid work ## Not marked 57 128 ## Marked 8 6 Erstellen Sie zuerst einen Charakter-Vektor für die neue Variable, bevor Sie diese zu einem Faktor umwandeln. Versuchen Sie dann die Trennung zwischen Levels und Labels zu beachten. Arbeiten Sie mit folgendes Levels * working * eduction * both * none und mit beliebigen Labels. daten_ess$beschaeft &lt;- 0 #da anschliessend Levels definiert werden können wir hier irgdeinen Wert einfügen. daten_ess$beschaeft[daten_ess$pdwrk==&quot;has been doing paid work&quot;] &lt;- &quot;working&quot; daten_ess$beschaeft[daten_ess$edctn==&quot;Marked&quot;] &lt;- &quot;eduction&quot; daten_ess$beschaeft[daten_ess$pdwrk==&quot;has been doing paid work&quot; &amp; daten_ess$edctn==&quot;Marked&quot;] &lt;- &quot;both&quot; daten_ess$beschaeft[daten_ess$pdwrk!=&quot;has been doing paid work&quot; &amp; daten_ess$edctn!=&quot;Marked&quot;] &lt;- &quot;none&quot; table(daten_ess$beschaeft) ## ## 0 both eduction none working ## 1 6 8 57 128 class(daten_ess$beschaeft) ## [1] &quot;character&quot; daten_ess$beschaeft &lt;- factor(daten_ess$beschaeft, levels = c(&quot;working&quot;, &quot;eduction&quot;, &quot;both&quot;, &quot;none&quot;)) table(daten_ess$beschaeft) ## ## working eduction both none ## 128 8 6 57 daten_ess$beschaeft &lt;- factor(daten_ess$beschaeft, levels = c(&quot;working&quot;, &quot;eduction&quot;, &quot;both&quot;, &quot;none&quot;), labels = c(&quot;Arbeitende Person&quot;, &quot;Person in Ausbildung&quot;, &quot;Arbeit und Ausbildung&quot;, &quot;Weder Arbeit noch Ausbildung&quot;)) table(daten_ess$beschaeft) ## ## Arbeitende Person Person in Ausbildung ## 128 8 ## Arbeit und Ausbildung Weder Arbeit noch Ausbildung ## 6 57 Integration ins Skript zur Datenaufbereitung: Mit diesem Bewusstsein für Levels und Labels können wir alle kategorialen Variablen im Datensatz jeweils über einen Befehl definieren. Dieses Vorgehen wird dann Datenaufbereitungsskript integriert. Hier ein Vorschlag: # Faktoren definieren ## ungeordnete Faktoren daten_ess$gndr &lt;- factor(daten_ess$gndr, levels = c(1,2), labels = c(&quot;Male&quot;, &quot;Female&quot;)) daten_ess$chldhm &lt;- factor(daten_ess$chldhm, levels = c(0,1), labels = c(&quot;Respondent lives with children at household grid&quot;, &quot;Does not&quot;)) daten_ess$pdwrk &lt;- factor(daten_ess$pdwrk, levels = c(1,2), labels = c(&quot;Not marked&quot;, &quot;Marked&quot;)) daten_ess$edctn &lt;- factor(daten_ess$edctn, levels = c(1,2), labels = c(&quot;Not marked&quot;, &quot;Marked&quot;)) daten_ess$uemp3m &lt;- factor(daten_ess$uemp3m, levels = c(1,2,7,8), labels = c(&quot;Yes&quot;, &quot;No&quot;, &quot;Refusal&quot;, &quot;Don&#39;t know&quot;)) ## geordnete Faktoren daten_ess$polintr &lt;- factor(daten_ess$polintr, levels = c(4,3,2,1), labels = c(&quot;Not at all interested&quot;, &quot;Hardly interested&quot;, &quot;Quite interested&quot;, &quot;Very interested&quot;), ordered = T) daten_ess$edulvla &lt;- factor(daten_ess$edulvla, levels = c(1,2,3,4,5), labels = c(&quot;ISCED 0-1&quot;, &quot;ISCED 2&quot;, &quot;ISCED 3&quot;, &quot;ISCED 4&quot;, &quot;ISCED 5-6&quot;), ordered = T) daten_ess$eisced &lt;- factor(daten_ess$eisced, levels = c(0,1,2,3,4,5,6,7), labels = c(&quot;Not possible to harmonise into ES-ISCED&quot;, &quot;ES-ISCED I&quot;, &quot;ES-ISCED II&quot;, &quot;ES-ISCED IIIb&quot;, &quot;ES-ISCED IIIa&quot;, &quot;ES-ISCED IV&quot;, &quot;ES-ISCED V1&quot;, &quot;ES-ISCED V2&quot;), ordered = T) Zur Überprüfung kann wie in der letzten Aufgabenstellung formuliert zuerst die Environment (und alle Objekte) gelöscht werden. Dies geht mit folgender Funktion: rm(list = ls()) Anschliessend kann der mittels source() der Datensatz neu geladen werden und  als Test  eine als Faktor definierte Variable aufgerufen werden: source(file = &quot;Daten/ess_import.R&quot;) plot(daten_ess$gndr) References "],["wochenplan-10-einführung-regression.html", "10 Wochenplan 10: Einführung Regression 10.1 Lernziele 10.2 Aufgaben", " 10 Wochenplan 10: Einführung Regression im Rahmen der 10. &amp; 11. Einheit. 10.1 Lernziele Nachdem wir unsere Daten aufbereitet, ersten Analysen mittels Kreuztabellen gemacht und die Faktoren definiert haben gehen wir in der kommenden Woche sukzessive zur Regressionsanalyse über. Dabei behalten wir die Arbeit mit unterschiedlichen Datenformaten (inklusive Faktoren) im Auge. Wir beginnen mit bivariaten Regressionsmodellen und machen uns Schritt für Schritt mit der Definition und Interpretation dieser Modelle in R vertraut. Konkret lassen sich für den 10. Wochenplan folgende Lernziele definieren: Sie sind mit der Funktion lm() und deren Grundstrukturen vertraut. Sie kennen die Notation, mit der in R Regressionsmodelle in Formeln dargestellt werden. Sie verstehen, wie in einem linearen Regressionsmodell sowohl metrische als auch kategoriale und binominale unabhängige Variablen einfliessen können. Sie haben einen ersten Eindruck von der Struktur des Ergebnisobjekts, das lm() erzeugt. Sie wissen, wie Sie den Modelloutput einer linearen Regression in R interpretieren müssen und die Güte eines Modells einschätzen sollten. 10.2 Aufgaben Laden Sie Ihr Datenimport-Skript. Arbeiten Sie anschliessend in einem R Markdown-Dokument weiter. setwd(&quot;C:/Users/SchweglG/R_Daten/06_HS21/R_Seminar-HS21&quot;) source(file = &quot;Daten/ess_import.R&quot;)   Formulieren Sie zwei inhaltliche Fragestellungen, die Sie anhand unserer ESS-Daten mit einem einfachen (bivariaten) Regressionsmodell beantworten können. Eine der beiden Fragestellungen soll eine kategoriale Variable als unabhängige Variable enthalten. Nutzen Sie je ein Streudiagramm, um erste Hinweis zu Ihren Fragestellungen zu erhalten. Die bivariate Regressionsanalyse berechnet die gerichtete (asymmetrische) Beziehung zwischen zwei metrischen Variablen, in der einfachsten Form einer unabhängigen Variable X auf eine abhängige Variable Y. Dabei repräsentiert X die Ursache und Y die Wirkung. Im Rahmen einer solchen Regressionsanalyse können auch kategoriale Variablen als unabhängige Variablen hinzugezogen werden. Bei kategorialen Variablen mit mehreren Ausprägungen wird allerdings ein multiples Modell generiert, das für die jeweiligen Ausprägungen einmal eine Referenzkategorie und für den Rest eine Dummy-Variable erstellt. Im weiteren Verlauf wird nochmals deutlich, was dies bedeutet. Hier sollen als vier verschiedenen Beispielen kurz erläutert werden, zuerst zwei für rein metrische Modelle: eduyrs &amp; wkhtot plot(daten_ess$eduyrs, daten_ess$wkhtot) Dazu etwa die Erläuterungen von Katrin Oesch: Arbeiten Personen mit mehr Ausbildungsjahren mehr Stunden in der Woche in ihrem Hauptberuf? (Y=wkhtot: Total hours normally worked per week in main job overtime included | X=eduyrs: Years of full-time education completed) Interpretation: - Zwischen den beiden Variablen lässt sich ein linearer Zusammenhang erkennen. Die Linearitaet gilt als Voraussetzung für die lineare Regression. - Bei 9 Ausbildungjahren gibt es eine Verdichtung. Dies entspricht der obligatorischen Schulzeit in der Schweiz. - Die 42.5h Woche bildet sich im Streudiagramm nicht klar ab -&gt; wichtiger Hinweis: Hier wurde nach den Stunden im Hauptjob gefragt. Personen mit mehreren Jobs/ vielen Nebenjobs haben daher tendenziell weniger Stunden in ihrem Hauptjob. agea &amp; happy plot(daten_ess$agea, daten_ess$happy) Hierzu die Erläuterungen von Delia Bazziger: Welchen Einfluss hat das Alter auf die Zufriedenheit? [] Die Grafik zeigt deutlich auf, dass zwischen diesen beiden Variablen kein Zusammenhang besteht. Vor allem in den Zufriedenheitsklassen 7 - 10 sind fast alle Altersgruppen vertreten. In den unteren Klassen sind zu wenige Ausprägungen rapportiert um einen Zusammenhang herzustellen oder zu verneinen. In Anbetracht der Ausprägungen in den oberen Klassen kann aber erwartet werden, dass das Alter auch in diesen Klassen kaum einen Einfluss auf die Zufriedenheit hat. Und zwei Beispiele für Modelle, die eine kategoriale Variable enhalten: gndr &amp; edyurs plot(jitter(as.numeric(daten_ess$gndr)), jitter(as.numeric(daten_ess$eduyrs))) Hierzu die Erläuterungen von Valentina Meyer: Gibt es einen Zusammenhang zwischen dem Geschlecht (gndr, uv) und der Anzahl Ausbildungsjahre (eduyrs, av)? Hinweis Diagramm: Es scheint kaum ein Zusammenhang zu bestehen. eisced &amp; Einkommen plot(daten_ess$eisced, factor(daten_ess$hinctnta)) Hier die Erläuterungen von Josias Bruderer: Hat der Bildungsabschluss (unabhängige Variable) einen Einfluss auf das Haushaltseinkommen (abhängige Variable)? Hypothese: Menschen mit niedrigerem Bildungsabschluss haben ein niedrigeres Haushaltseinkommen. Erster Hinweis: es ist ein Trend zu beobachten. Bei folgendem Beispiel, das in dieser oder vergleichbarer Weise von einigen Seminarteilnehmer*innen verwendet wurde, scheint ein Problem vorhanden. agea &amp; polintr plot(daten_ess$agea,daten_ess$polintr) Das Problem ist, dass Y ist eine kategoriale Variable mit lediglich vier Ausprägungen. Solche eine Variable eignet sich nicht als abhängige Variable in einem linearen Regressionsmodell. Eine Ausnahme (in unserem Datensatz) wäre hier etwa happy (oder auch hinctna, siehe weiter unten). Happy ist zwar streng genommen ebenfalls kategorial, sie könnte aber aufgrund der Anzahl Ausprägungen und deren Verteilung als abhängig Variable verwendet werden: hist(daten_ess$happy)   Formulieren und überprüfen Sie Ihre beiden Modelle mittels der Funktion lm(). Machen Sie sich dazu vorab mit dieser Funktion vertraut (Hilfeseite der Funktion, Online-Tutorials ). Erläutern Sie lm() kurz in eigenen Worten! Die Funktion lm() ist der Befehl in R zur Berechnung von linearen Modellen allgemein. Er übersetzt die normale Regressionsgleichung: y = b0 + b1x + e zur Formulierung response ~ term. Mit der übersetzten Formel können dann auch multiple, lineare Regressionen dargestellt werden, response ~ term + term, oder um Interaktionsterme ergänzt werden: response ~ term + term * term. Die weiteren Argumente (neben der eigentlichen Formel sowie der Angabe des Datensatzes) erlauben etwa die Daten zu unterteilen, Gewichtungen einzubauen oder den Umgang mit fehlenden Werten genauer zu bestimmen. Im Folgenden brauchen wir die Funktion und ihre Argumente aber nicht weiter zu spezifizieren, sondern die Standardeinstellungen der Argumente reichen aus. Wir wollen hier zwei Modelle ins Zentrum rücken, nämlich der Zusammenhang zwischen Bildungsjahren und Arbeitsstunden (eduyrs &amp; wkhtot, a) und Ausbildungsniveau und Einkommen (b)  bei letzterem greifen wir aber auf edulvla züruck (anstatt eisced), um das Einkommen zu erklären (hinctnta) . #Modell a) lm(formula = wkhtot ~ eduyrs, data = daten_ess) ## ## Call: ## lm(formula = wkhtot ~ eduyrs, data = daten_ess) ## ## Coefficients: ## (Intercept) eduyrs ## 38.7450 -0.1663 #Modell b) lm(formula = hinctnta ~ edulvla, data = daten_ess) ## ## Call: ## lm(formula = hinctnta ~ edulvla, data = daten_ess) ## ## Coefficients: ## (Intercept) edulvla.L edulvla.Q edulvla.C edulvla^4 ## 5.18028 1.53421 0.04600 0.70695 -0.09197 Wendet man die Funktion lm() direkt an (ohne ein Ergebnisobjekt abzuspeichern) gibt uns R die deskriptiv statistischen Ausprägungen der Regressionskoeffizienten aus. Für das Modell a bedeutet dies folgendes: Wenn eine Person über 0 Bildungsjahre verfügt, dann wird sie als Vorhersagewert 28.5h pro Woche in ihrem Hauptberuf arbeiten (Intercept / b0). Pro zusätzlichen Bildungsjahr erhöht sich denn der Vorhersagewert um 0.6h. Mit diesen beiden Wert (bzw. mit diesem Modell) können wir dann auch die Regressionsgerade in den Plot einzeichnen: #entweder plot(daten_ess$eduyrs, daten_ess$wkhtot) abline(28.4598, 0.6154, col = &quot;red&quot;) #oder plot(daten_ess$eduyrs, daten_ess$wkhtot) abline(lm(formula = wkhtot ~ eduyrs, data = daten_ess), col = &quot;red&quot;) Beim Modell b zeigt sich nun etwas anders, denn hier scheinen neben dem b0-Wert drei verschiedene Koeffizienten angezeigt zu werden. R wandelt kategoriale Variablen mit mehreren Ausprägungen automatisch in Dummy-Variablen um, die mit 0-1 kodiert sind (vgl. Manderscheid 2017, S.197f). Aus unserer Variable edulvla table(daten_ess$edulvla) ## ## ISCED 0-1 ISCED 2 ISCED 3 ISCED 4 ISCED 5-6 ## 6 41 88 6 56 werden vier neue Variable, die jeweils 0-1 kodiert sind und eine Referenzkategorie (1 + 4). Ist also jemand auf dem Bildungsniveau ISCED 3, dann hat er oder sie bei dieser Variable eine 1  und bei allen anderen eine 0. Die Bezeichnung im Modell werden wird weiter unten nochmals besprochen. Die erste Kategorie  in unserem Beispiel das Level ISCED 0-1 der Variable edulval  wird zur eigentlichen Referenzkategorie für das Modell oder eben b0. Das heisst wenn jemand das Bildungslevel ISCED 0-1, dann wird er oder sie voraussichtlich auf dem Einkommensniveau 5.2 sein. Wie können die weiteren Koeffizienten interpretiert werden? Hier ist wichtig zu beachten, dass die Wirkung immer in Bezug zur Referenzkategorie beschrieben wird: Der Wert 0.22 (edulvla.C) beschreibt die Veränderung wenn eine Person vom Bildungsniveau ISCED 0-1 (Referenzkategorie) auf das Bildungsniveau ISCED 4.   Weisen Sie Ihre Regressionsanalysen jeweils einem Ergebnisobjekt zu! Wie ist dieses Objekt aufgebaut? Welche Komponenten und Informationen beinhaltet es? #Modell a) modell_a &lt;- lm(formula = wkhtot ~ eduyrs, data = daten_ess) #Modell b) modell_b &lt;- lm(formula = hinctnta ~ edulvla, data = daten_ess) Die Funktion lm() generiert uns eine Listenobjekt mit 13 bzw. 14 weiteren Unterobjekten. Die genauen Details, was in diesen Ergebnisobjekten zu finden ist, können wir der Hilfeseite unter Value entnehmen. ?lm str(modell_a) str(modell_b) An object of class lm is a list containing at least the following components: 1. coefficients: a named vector of coefficients [GS: die Koeffizienten] 2. residuals: the residuals, that is response minus fitted values. [GS: die Residuen der einzelnen Fälle] 3. effects: the uncorrelated single-degree-of-freedom values obtained by projecting the data onto the successive orthogonal subspaces generated by the QR decomposition during the fitting process [GS: die vorhergesagten Effekte/Wirkung der unabhängigen Variable pro einzelnem Fall] 4. rank: the numeric rank of the fitted linear model. [GS: Die Anzahl Variablen im Modell] 5. fitted.values: the fitted mean values. [GS: die Vorhersagewerte der Fälle] 6. assign: [GS: Nummern der Variablen] 7. qr: [GS: Werte des Vorhersagemodells der kleinsten Quadrate in den beiden Dimensionen] weights: (only for weighted fits) the specified weights. 8. df.residual: the residual degrees of freedom. [GS: die Freiheitsgrade des Modells, also Fallzahl - J - 1] 9. na.action: (where relevant) information returned by model.frame on the special handling of NAs. [GS: die aufgrund fehlender Werte ausgeschlossenen Fälle] 10. xlevels: (only where relevant) a record of the levels of the factors used in fitting. [GS: die Namen der Levels, wenn Faktoren verwendet wurden] 11. call: the matched call. [GS: die Regressionsformel] 12. terms: the terms object used. [GS: Die Bezeichnungen der Elemente im Modell/in der Funktion selber] 13. model: if requested (the default), the model frame used. [GS: alle Elemente im Modell/in der Funktion selber] 14. contrasts: (only where relevant) the contrasts used. [GS: Die Verwendung der Referenzkategorie bei kategorialen Variablen, poly bei mehreren Elementen die über die Referenzkategorien vergliechen werden] Beim Ergebnisobjekt des Modells b) sehen wir nun eine Veränderung, nämliche dass als weiteres Element der Liste contrasts hinzugekommen ist (siehe Punkt 14). Wie immer können wir auf diese Elemente zugreifen, z.Bsp. um uns ein Streudiagramm der Residuen ausgeben zu lassen: plot(modell_a$model$wkhtot, modell_a$residuals) Der Plot verdeutlicht uns bereits, dass im Modell a nicht wirklich eine Varianzhomogenität vorhanden ist (und wir noch mindestens eine neue Variable integrieren müssten).   Interpretieren Sie die Ergebnisse Ihrer beiden Regressionsanalysen. Für die Interpretation des Modells bzw. des Objektes verwenden wir die summary()-Funktion. Diese können wir anhand unseres ersten Modells erläutern: summary(modell_a) ## ## Call: ## lm(formula = wkhtot ~ eduyrs, data = daten_ess) ## ## Residuals: ## Min 1Q Median 3Q Max ## -37.248 -10.289 4.752 8.417 33.583 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 38.7450 3.4124 11.354 &lt;2e-16 *** ## eduyrs -0.1663 0.2913 -0.571 0.569 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.68 on 178 degrees of freedom ## (20 Beobachtungen als fehlend gelöscht) ## Multiple R-squared: 0.001827, Adjusted R-squared: -0.003781 ## F-statistic: 0.3258 on 1 and 178 DF, p-value: 0.5688 Die Funktion gibt uns verschiedenste Kennzahlen aus. Die Auflistung beginnt mit der Formel (Call) und geht über zu einer kurzen Beschreibung der Verteilung der Residuen (Residuals). Hier zeigen sich bereits erste Hinweise zur Modellgüte: Streuen die Residuen besonders stark oder nicht? Das Modell unterschätzt beispielsweise einen Fall um 53.541h pro Woche. Anschliessend folgn dann die Übersicht zu den Koeffizienten: der b0-Wert (Intercept) und die Estimates der unabhängigen Variable. Ersterer gibt den Vorhersagewert an, wenn die unabhängige Variable 0 wäre (28.4598), während letzterer die Zunahme des Vorhersagewertes bei einer Zunahme der unabhängigen Variable um 1 angibt (0.6154). Dies entspricht dem inferenzstatistischen Ergebnis von oben. Die zweite Kennzahl in der Mitte der summary ist der Standardfehler (Std.Error), also die durchschnittliche Abweichung der Koeffizienten, da diese ja auf Stichprobendaten beruhen. Das heisst wir würden einen neuen Wert bekommen, wenn wir eine neue Stichprobe ziehen würden und dieser Wert würde im Durchschnitt um 0.3365 abweichen. Als Daumenregel für diesen Wert: Das Ergebnis von Regressionskoeffizient /+ (2 x SE) sollte nicht 0 überschreiten beziehungsweise im Vergleich zum Koeffizienten das Vorzeichen wechseln. Sonst wäre das ein Hinweis, dass kein Einfluss der unabhängigen Variable besteht (da jeweils +2x &amp; -2x Standardfehler rund 95% der Ausprägungen einer Normalverteilung repräsentieren). Es gilt also den Standardfehler im Zusammenhang zum Regressionskoeffizient zu betrachten. Die dritte Kennzahl ist der T-Wert (t value), was dem eigentlichen Testwerte für die t-Verteilung repräsentiert. Als Daumenregel: T Wert die grösser als 2 sind werden meistens signifikant sein. Und wir könnten uns natürlich auch eine t-Verteilung in R ausgeben lassen um nach dem genauen Annahme und Rückweisungsbereich zu fragen, etwa für ein Signifikanzniveau von 95% (zweiseitig): qt(.975, 178) ## [1] 1.973381 Der p-Wert (Pr(&gt;|t|)) ist der Signifikanzwert für einzelne Regressionskoeffizienten. Wiederum: Die Kennzahl gibt uns an, wie wahrscheinlich es ist, einen solchen (oder grösseren) Wert für den Regressionskoeffizienten zu erhalten unter der Annahme, dass die Nullhypothese in der Grundgesamtheit zutreffen würde. Zusätzlich findet sich auch jeweils noch der Hinweis zu den Sternchen als Signifikanzcodes. In unserem Fall besteht eine knapp 7% Chance, dass der Wert unseres Koeffizienten aufgetreten ist, obschon in der Grundgesamtheit kein Einfluss von den Bildungsjahren auf die Arbeitsstunden erfolgt (deshalb ist der Koeffizient auch nicht signifikant). Anschliessend folgen weitere Kennwerte zum gesamten Modell: Die Standardabweichung der Residuen wird ausgegeben (Residual standard error) sowie die Freiheitsgrade (degrees of freedom), das heisst N - J - 1, wobei J die Anzahl unabhängiger Variablen ist. Danach folgen das multiple R-Quadrart (Multiple R-squared) sowie (Adjusted R-squared). Ersteres gibt uns die Erklärungsleistung des Modells sowie zugleich die Stärke des gerichteten, statistischen Zusammenhangs zwischen X und Y an. Letzteres ist ein Gütemass, dass die Erklärungsleistung im Bezug zu den Anzahl Variablen betrachtet und den Wert für Stichprobendaten etwas anpasst. Unser Modell a erklärt also nur knapp 1% der Variation der Variable wkhtot. Schliesslich finden sich noch die Angaben zum F-Test (F-statistic). Dieser Wert weisst aus, ob das Modell auch über eine Erklärungsleistung in der Grundgesamtheit verfügt. Hier wird global getestet, ob der Regressionsansatz insgesamt etwas aussagt (Diaz-Bone 2019, 221). Dazu gibt uns das Modell sowohl den konkreten F-Test-Wert (und wiederum die Freiheitsgrade der Verteilung) als auch den p-Wert an. Dieser sagt uns nun aus, ob das Modell auch in der Grundgesamtheit eine Erklärungsleistung besitzt (wiederum wie wahrscheinlich es ist, einen solchen oder grösseren R-Quadrat Wert zu erhalten unter der Annahme, dass das die Nullhypothese zutrifft). qf(0.95, 1, 178) ## [1] 3.894232 Wir sehen also, dass das Modell knapp nicht signfikant ist und in der Grundsgesamtheit keine Erklärungsleistung zu besitzen scheint. summary(modell_b) ## ## Call: ## lm(formula = hinctnta ~ edulvla, data = daten_ess) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.3878 -1.9142 -0.0897 1.8261 4.9103 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.18028 0.37164 13.939 &lt;2e-16 *** ## edulvla.L 1.53421 0.90737 1.691 0.0929 . ## edulvla.Q 0.04600 0.78104 0.059 0.9531 ## edulvla.C 0.70695 0.92992 0.760 0.4483 ## edulvla^4 -0.09197 0.68135 -0.135 0.8928 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.447 on 153 degrees of freedom ## (42 Beobachtungen als fehlend gelöscht) ## Multiple R-squared: 0.06471, Adjusted R-squared: 0.04026 ## F-statistic: 2.647 on 4 and 153 DF, p-value: 0.03563 Im Unterschied zum ersten Modell sehen wir beim zweiten Modell wiederum, dass die einzelnen Ausprägungen der kategorialen Variable zu je eigene Koeffizienten mit zugehörige Kennwerte umgewandelt wurden. Die etwas speziellen Bezeichnungen der Kategorien resultieren daraus, da die Variable ein geordneter Faktor ist und geben eine Rangfolge an (ohne jetzt weiter auf die genauen Bezeichnungen einzugehen): L=linear trend [1 dimensional/1. Abstufung], Q=quadratic trend [2 dimensional/2. Abstufung], C=cubic trend [3 dimensional/3. Abstufung], ^4=[4 dimensional/4. Abstufung]. Wenn edulvla als ein nicht-geordneter Faktor verwendet wird dann tauchen auch auch die eigentlichen Namen der Levels im Modell auf: daten_ess$edulvla_no &lt;- factor(as.numeric(daten_ess$edulvla), levels = c(1,2,3,4,5), labels = c(&quot;ISCED 0-1&quot;, &quot;ISCED 2&quot;, &quot;ISCED 3&quot;, &quot;ISCED 4&quot;, &quot;ISCED 5-6&quot;), ordered = F) modell_b_no &lt;- lm(formula = hinctnta ~ edulvla_no, data = daten_ess) summary(modell_b_no) ## ## Call: ## lm(formula = hinctnta ~ edulvla_no, data = daten_ess) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.3878 -1.9142 -0.0897 1.8261 4.9103 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.000 1.223 3.269 0.00133 ** ## edulvla_noISCED 2 1.174 1.326 0.886 0.37723 ## edulvla_noISCED 3 1.090 1.254 0.869 0.38636 ## edulvla_noISCED 4 1.250 1.730 0.722 0.47112 ## edulvla_noISCED 5-6 2.388 1.272 1.877 0.06248 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.447 on 153 degrees of freedom ## (42 Beobachtungen als fehlend gelöscht) ## Multiple R-squared: 0.06471, Adjusted R-squared: 0.04026 ## F-statistic: 2.647 on 4 and 153 DF, p-value: 0.03563 Ebenfalls zeigen sich Veränderung bei den Koeffizienten, da R im ergänzenden Modell b_no die Reihenfolge nicht als Rangfolg interpretiert. Nun ist das letzte Level auf einem Niveau von 95% signifikant. Wiederum besitzt das Modell insgesamt eine Erklärungsleistung in der Grundgesamtheit besitzt. Hier haben sich die Kennzahlen nicht geändert. References "],["wochenplan-11.html", "11 Wochenplan 11 11.1 Lernziele 11.2 Aufgaben", " 11 Wochenplan 11 zur 11. &amp; 12. Einheit 11.1 Lernziele Im Rahmen des 11. Wochenplans wollen wir uns einem multiplen linearen Regressionsmodell widmen. Wie immer möchten wir uns dazu als erstes einen Überblick zu möglichen Variablen verschaffen und eine Fragestellung formulieren, bevor dann ein Modell berechnet und interpretiert werden kann. Anschliessend gilt es über eine grafische Residuenanalyse die Anwendungsvoraussetzungen für die multiple lineare Regression zu testen sowie über den Ausschluss von gewissen Fällen das Modell zu verbessern. Konkret lassen sich folgende beiden Ziele für diesen nächsten Wochenplan festlegen: Sie verstehen, wie Sie mittels Korrelationstabellen erste Ideen für ein multiples Modell sammeln können. Sie können multiple lineare Regressionsanalysen in R korrekt modellieren, durchführen und interpretieren. Die weiteren Aspekte zur multiplen linearen Regression werdeb dann im nächsten Abschnitt behandelt. 11.2 Aufgaben Laden Sie Ihr Datenimport-Skript. Arbeiten Sie anschliessend in einem R-Markdown-Dokument weiter. setwd(&quot;C:/Users/SchweglG/R_Daten/06_HS21/R_Seminar-HS21&quot;) source(file = &quot;Daten/ess_import.R&quot;)   Berechnen Sie mittels der Funktion rcorr(), die Teil des Paketes Hmisc ist, eine Korrelationstabelle zwischen den Variablen wkhtot, agea, edulvla und chldhm. Wieso sollten wir eine Korrelationstabelle erstellen? Eine solche Tabelle liefert uns einen schnellen Überblick zu verschiedenen Variablen und deren Zusammenhängen, wie eben zu wkhtot, agea, edulvla und chldhm. Trotzdem macht es jeweils Sinn, sich zuerst die Variablen noch einzeln anzuschauen: attach(daten_ess) #eine gute Gelegenheit für die attach-Funktion? summary(agea) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 15.00 30.75 47.00 45.49 59.00 85.00 summary(edulvla) ## ISCED 0-1 ISCED 2 ISCED 3 ISCED 4 ISCED 5-6 NA&#39;s ## 6 41 88 6 56 3 summary(chldhm) ## Respondent lives with children at household grid ## 70 ## Does not ## 130 summary(wkhtot) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00 27.00 42.00 36.76 45.00 70.00 19 detach(daten_ess) Über den summary()-Funktion werden schnell Verteilungen deutlich  etwa die Schiefe bei der Variable edulvla  und auch das Vorhandensein von fehlenden Werte  wie etwa bei wkhtot. Anschliessend können wir nochmals die Hilfeseite der Funktion rcorr() aufrufen. Dabei wird ersichtlich, dass wir eben eine Matrix als Input der Funktion benötigen. library(Hmisc) ## Lade nötiges Paket: lattice ## Lade nötiges Paket: survival ## Lade nötiges Paket: Formula ## ## Attache Paket: &#39;Hmisc&#39; ## Die folgenden Objekte sind maskiert von &#39;package:base&#39;: ## ## format.pval, units ?rcorr Die Matrix erstellen wir wiefolgt: corr_m &lt;- cbind(daten_ess$wkhtot, daten_ess$agea, daten_ess$edulvla, daten_ess$chldhm) head(corr_m) ## [,1] [,2] [,3] [,4] ## [1,] 60 22 5 2 ## [2,] 41 64 4 2 ## [3,] 35 35 5 1 ## [4,] 45 25 3 2 ## [5,] 40 59 NA 1 ## [6,] 32 37 5 1 colnames(corr_m) &lt;- c(&quot;wkhtot&quot;, &quot;agea&quot;, &quot;edulvla&quot;, &quot;chldhm&quot;) head(corr_m) ## wkhtot agea edulvla chldhm ## [1,] 60 22 5 2 ## [2,] 41 64 4 2 ## [3,] 35 35 5 1 ## [4,] 45 25 3 2 ## [5,] 40 59 NA 1 ## [6,] 32 37 5 1   Interpretieren Sie die Korrelationstabelle (bzw. deren verschiedenen Teile): Wo sehen Sie bereits Zusammenhänge, wo nicht? Formulieren Sie anschliessend eine Fragestellung sowie eine Hypothese zu diesen Variablen für ein multiples lineares Regressionsmodell. Anschliessend berechen wir die Korrelationstabelle: rcorr(corr_m, type = &quot;pearson&quot;) ## wkhtot agea edulvla chldhm ## wkhtot 1.00 -0.10 0.11 -0.01 ## agea -0.10 1.00 0.04 0.04 ## edulvla 0.11 0.04 1.00 -0.01 ## chldhm -0.01 0.04 -0.01 1.00 ## ## n ## wkhtot agea edulvla chldhm ## wkhtot 181 181 178 181 ## agea 181 200 197 200 ## edulvla 178 197 197 197 ## chldhm 181 200 197 200 ## ## P ## wkhtot agea edulvla chldhm ## wkhtot 0.1648 0.1449 0.9295 ## agea 0.1648 0.5747 0.5987 ## edulvla 0.1449 0.5747 0.8765 ## chldhm 0.9295 0.5987 0.8765 Die Funktion gibt uns nun drei verschiedene Tabellen aus: Zuerst die eigentlichen Korrelationswerte (1) dann die Fallzahlen (2), wo die fehlende Werte, die wir oben in der Summary gesehen haben wieder deutlich werden, und schliesslich die p-Werte für die einzelnen Korrelationen (3). Schnell wird auch deutlich, dass die Tabelle redundante Informationen besitzt, da sie über die diagonale Achse gespiegelt ist (d.h. der jeweilige Wert von edulvla und agea ist natürlich derselbe Werte wie derjenige von agea und edulvla). Welche Korrelationenswerte sehen wir (deskripitv)? -kleine positive Korrelationen zwischen wkhtot und edulvla, wkhtot und chldhm sowie agea und chldhm -kleine negative Korrelation zwischen edulvla und chldhm siwue agea und wkhtot -sehr kleine Korrelationswerte zwischen zwischen agea und edulvla Weiter sehen wir, wo eben fehlende Werte vorhanden sind wie etwa der eine fehlende Werte bei edulvla. Dann wird deutlich, dass lediglich ein Zusammenhang auf einem 95% Niveau signifikant ist, nämlich der Zusammenhang zwischen edulvla und chldhm. Alle anderen Korrelationswerte haben eine hohe Wahrscheinlichkeit, dass sie unserer Stichprobe aufgetreten sind, obwohl in der Grundgesamtheit kein Zusammenhang zwischen den Variablen besteht. Anschliessend können wir übergehen unser multiples lineares Modell zu formulieren. Dabei integrieren wir eine Richtung in die betrachteten Zusammenhänge und definieren so eine abhängige Variable (auf die eingewirkt wird) und verschiedenen unabhängige Variablen (die wirken). Bereits jetzt sollte deutlich sein, dass nur die Variable wkhtot zur abhängigen Variable werden kann. Alle anderen Variablen sind entweder inhaltlich nicht sinnvoll (agea) oder wären nicht geeignet für ein lineares Regressionsmodell (edulvla und chldhm). Hier einige Beispiele für die Regressionsmodelle in Form von Forschungsfragen: V1 - reduziert: Katrin Oesch: Wie stark beeinflusst die Tatsache, ob Kinder im Haushalt leben und der hoechste Ausbildungsabschluss die duchschnittlich geleisteten Arbeitsstunden pro Woche im Hauptjob? Julien Lattmann: Inwiefern können das Bildungsniveau und die Anzahl Kinder im Haushalt vohersagen, wieviel jemand pro Woche arbeitet? Warum fehlt hier die Varialbe agea? Diese könnten wir trotz einem anders gelagerten Interesse auch einfach als Kontrollvariable integrieren. V2 - vollständig: Vanesse Leutener: Wie gross ist der Anteil der Variation von wkhtot, der durch die gemeinsame Variation der Variablen agea, edulvla und chldhm erklärt werden kann? Als These von Valentina Meyer: Die Anzahl Arbeitsstunden (wkhtot, av) hängt davon ab, welches Bildungslevel (edulvl) und welches Alter (agea) die befragte Person hat.   Berechnen Sie anschliessend Ihr Regressionsmodell mittels der Funktion lm(). Speichern Sie das Modell wie immer als Objekt ab und interpretieren Sie es anschliessend. Anschliessend können wir unser multiples lineares Modell formulieren. Die Auseinandersetzungen der vergangenen Wochen haben gezeigt, dass es angebracht ist die Variable edulvla als nicht-geordneter Faktor zu integrieren: daten_ess$edulvla_no &lt;- factor(daten_ess$edulvla, ordered = F) Dann formulieren wir das Modell: modell_m &lt;- lm(wkhtot ~ agea + edulvla_no + chldhm, data = daten_ess) summary(modell_m) ## ## Call: ## lm(formula = wkhtot ~ agea + edulvla_no + chldhm, data = daten_ess) ## ## Residuals: ## Min 1Q Median 3Q Max ## -38.469 -8.622 3.456 9.170 33.778 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 51.19795 7.61602 6.722 2.56e-10 *** ## agea -0.10006 0.06643 -1.506 0.1338 ## edulvla_noISCED 2 -14.53823 7.01734 -2.072 0.0398 * ## edulvla_noISCED 3 -10.57326 6.68716 -1.581 0.1157 ## edulvla_noISCED 4 -16.45756 9.16319 -1.796 0.0743 . ## edulvla_noISCED 5-6 -7.17038 6.77776 -1.058 0.2916 ## chldhmDoes not 0.74300 2.28406 0.325 0.7454 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.4 on 171 degrees of freedom ## (22 Beobachtungen als fehlend gelöscht) ## Multiple R-squared: 0.05682, Adjusted R-squared: 0.02372 ## F-statistic: 1.717 on 6 and 171 DF, p-value: 0.1198 Die Koeffizienten unseres Modells lassen sich wie folgt interpretieren: Wenn jemand 0 Jahre alt ist, auf dem ISCED 0-1 Bildungsniveau ist und Kinder zuhause hat, dann würde diese Person 41.7h pro Woche arbeiten (Intercept). Mit jeden zusätzlichen Altersjahr arbeitet eine Person gemäss dem Modell 0.005h weniger. Wechselt man vom ersten auf das zweite Bildungsniveau so arbeitet man 11.5 weniger, beim Wechsel auf das dritte Bildungsniveau 10.1h weniger, usw. Keine Kinder im Haushalt zu haben führt dazu, dass man rund 3h mehr arbeitet pro Woche. Von den Koeffizienten ist keine einzige Variable signifikant (mit Ausnahme des bo-Wertes). Alle Werte haben hohe Wahrscheinlichkeiten zufällig aufzutreten, wenn sie keinen Effekt hätten in der Grundgesamtheit (etwa der Koeffizient von ISCED 5-6 hat eine Wahrscheinlichkeit von 80% völlig zufällig aufgetreten zu sein). Dies sieht man auch anhand der Standardfehler, die alle sehr grosse zufällige Streuung der Koeffizienten ausweisen. Als Hinweis: Hier sieht man sehr schön, wie die t-Werte entstehen, nämlich einfach über die Berechnung von Estimate(bj)/St.Error(Sigma bj). Unser Modell beziehungsweise unsere unabhängigen Variablen erklären rund 8% bzw. 5% der Varianz von der Variable wkhtot. Dieser Wert selber ist signifikant auf einem 95% Niveau. Oder umgekehrt formuliert: Es besteht lediglich eine Wahrescheinlichkeit von 2.4%, dass unser r-Quadrat Wert zufällig in der Stichprobe aufgetreten ist, obwohl das Modell in der Grundgesamtheit keine Erklärungsleistung besitzt. "],["ergänzungen-einheit-14-wp1314.html", "12 Ergänzungen Einheit 14 (WP13/14) 12.1 Dateiformat RDS 12.2 Relevanz von Stichprobengrösse 12.3 Übung zur multiplen, linearen Regression 12.4 Anwendungsvoraussetzungen 12.5 Exkurs: Residuenwolken", " 12 Ergänzungen Einheit 14 (WP13/14) Im folgenden Abschnitt werden einige Punkte erläutert, die Teil der letzten Einheit des Seminars waren beziehungsweise Teil des WP13/14 (da die 12.Sitzung und damit der WP12 ausgefallen ist). Wir möchten nochmals auf die multiple, lineare Regression und das bereits eingeführte Modell eingehen. Über einen gemeinsam geteilten Datensatz sollen die Interpretation dieser Regressionsmodelle im Detail besprochen werden. Das bietet auch die Möglichkeit, das R-eigene Dateiformat zum Speichern von Datensätzen zu verwenden. Neben der Repetition der Interpretation gilt es weiter über eine grafische Residuenanalyse die Anwendungsvoraussetzungen für die multiple lineare Regression zu testen. Das heisst wir widmen uns folgenden Lernziele: Sie können multiple lineare Regressionsanalysen in R korrekt modellieren, durchführen und interpretieren. Sie verstehen, wie die Anwendung von plot() auf unser Ergebnisobjekt bei der Analyse der Qualität und Aussagekraft eines Regressionsmodells helfen kann. Sie können Ausreisser in einem Regressionsmodell ausschliessen und die damit erreichten Veränderungen im Modell einschätzen. und ergänzen diese um folgenden Aspekt: Sie haben eine Möglichkeit kennengerlernt, wie wir in einem R-eigenen Dateiformat Datensätze abspeichern und aufrufen können. 12.1 Dateiformat RDS Das Speichern eines einzelnen Objektes in einem R-eigenen Dateiformat ist möglich mittels saveRDS().12 Das Einlesen der gespeicherten Datei erfolgt dann wieder über readRDS(). ?saveRDS() ?readRDS() saveRDS and readRDS provide the means to save a single R object to a connection (typically a file) and to restore the object, quite possibly under a different name. Damit ist die Möglichkeit gegeben, die Bearbeitung eines Datensatz (das Löschen von unötigen Fällen und Fehlern, Umkodieren, Definieren von Faktoren, usw.) nicht als Code zu sichern, sondern eben als ein Datei selber, die den Datensatz enthält. Wir können daher in unserem Import Skrit die Funktionen zur Zufallsauswahl von 200 Fällen löschen und dann eben einen saveDRS()-Befehl ergänzen. Die so erstelle Datei landet dann natürlich in unserem definierten Arbeitsverzeichnis: # Laden der Daten setwd(&quot;C:/Users/SchweglG/R_Daten/06_HS21/R_Seminar-HS21&quot;) daten_ess &lt;- read.csv(file = &quot;Daten/ESS1-8e01.csv&quot;) # nicht benoetigte Variablen loeschen daten_ess$x &lt;- NULL # Definieren von fehlenden Werten daten_ess$yrbrn[daten_ess$yrbrn==7777 | daten_ess$yrbrn==8888 | daten_ess$yrbrn==9999] &lt;- NA ## usw... # Fehler in den Faellen korrigieren ## Fall 132 daten_ess$agea[132] &lt;- 2016 - daten_ess$yrbrn[132] ## usw... # Faktoren definieren daten_ess$gndr &lt;- factor(daten_ess$gndr, levels = c(1,2), labels = c(&quot;Male&quot;, &quot;Female&quot;)) ## usw... ## geordnete Faktoren daten_ess$polintr &lt;- factor(daten_ess$polintr, levels = c(4,3,2,1), labels = c(&quot;Not at all interested&quot;, &quot;Hardly interested&quot;, &quot;Quite interested&quot;, &quot;Very interested&quot;), ## usw... saveRDS(file = &quot;daten_ess_E14.rds&quot;) Was bietet diese Variante zum Umgang mit Datensätzen für Vorteile? Dies ermöglicht eine Kontrolle und einen etwas vereinfachten Umgang mit aufbereiteten Datensätzen, der auch optimiert ist für R (im Gegensatz etwa zu SPSS oder STATA Dateien). Im Seminar erläuterte Josias Bruderer zwei weitere Aspekte: Erstens erfolgt nur eine Lese- und keine Rechenoperation, was eben zu mehr Geschwindigkeit führt. Zweitens benötigt einE jeweiligeR BenutzerIn nicht all die Pakete, die wir womögilch zur Aufbereitung eines Datensatzes verwendet haben. Was ist ein Nachteil im Umgang mit diesem Dateiformat? Mit dieser Vorgehensweise verlieren wir wiederum etwas Transparenz. Wir kreiieren uns hier eine Blackbox  und damit etwas, was wir bisher immer versucht haben zu umgehen. setwd(&quot;C:/Users/SchweglG/R_Daten/06_HS21/R_Seminar-HS21&quot;) daten_ess_f &lt;- readRDS(file = &quot;daten_ess_E14.rds&quot;) 12.2 Relevanz von Stichprobengrösse Bereits im Rahmen des WP11 haben wir uns der Korrelationstabelle gewidmet. Hier soll zuerst ergänzend noch eine etwas andere Variante präsentiert werden, wie wir die Tabelle mittels der Funktion attach() erstellen können und so direkt eine Beschriftungen erhalten. Anschliessend können wir die Werte wiederum kurz interpretieren. library(Hmisc) attach(daten_ess_f) rcorr(cbind(wkhtot, agea, edulvla, chldhm), type = &quot;pearson&quot;) ## wkhtot agea edulvla chldhm ## wkhtot 1.00 0.00 0.13 0.11 ## agea 0.00 1.00 0.02 0.08 ## edulvla 0.13 0.02 1.00 -0.09 ## chldhm 0.11 0.08 -0.09 1.00 ## ## n ## wkhtot agea edulvla chldhm ## wkhtot 1394 1390 1386 1394 ## agea 1390 1518 1509 1518 ## edulvla 1386 1509 1516 1516 ## chldhm 1394 1518 1516 1525 ## ## P ## wkhtot agea edulvla chldhm ## wkhtot 0.8788 0.0000 0.0000 ## agea 0.8788 0.3715 0.0015 ## edulvla 0.0000 0.3715 0.0003 ## chldhm 0.0000 0.0015 0.0003 detach(daten_ess_f) Welche Korrelationenswerte sehen wir (deskripitv)? -kleine positive Korrelationen zwischen wkhtot und edulvla sowie zwischen wkhtot und chldhm -sehr kleine negative Korrelation zwischen edulvla und chldhm -sehr kleiner Korrelationswert zwischen zwischenchldhm und agea -kaum vorhandene Korrlation zwischen edulvla und agea sowie wkhtot und agea Trotz der geringen Korrelationswerte sehen wir aber, dass ausser zwei alle Zusammenhänge auch auf die Grundgesamtheit übertragbar sind (lediglich edulvla und agea sowie wkhtot und agea sind nicht signfikant). Zum Vergleich: Als wir mit einem 200er Datensatz gearbeitet haben waren diese Werte noch nicht signifikant. source(file = &quot;Daten/ess_import.R&quot;) attach(daten_ess) rcorr(cbind(wkhtot, agea, edulvla, chldhm), type = &quot;pearson&quot;) ## wkhtot agea edulvla chldhm ## wkhtot 1.00 -0.10 0.11 -0.01 ## agea -0.10 1.00 0.04 0.04 ## edulvla 0.11 0.04 1.00 -0.01 ## chldhm -0.01 0.04 -0.01 1.00 ## ## n ## wkhtot agea edulvla chldhm ## wkhtot 181 181 178 181 ## agea 181 200 197 200 ## edulvla 178 197 197 197 ## chldhm 181 200 197 200 ## ## P ## wkhtot agea edulvla chldhm ## wkhtot 0.1648 0.1449 0.9295 ## agea 0.1648 0.5747 0.5987 ## edulvla 0.1449 0.5747 0.8765 ## chldhm 0.9295 0.5987 0.8765 detach(daten_ess) Da statistische Signifkanz stark von der Stichprobengrösse abhängig ist, sollte sie nicht überbewertet werden. Wichtig ist die verschiedenen Kennzahlen miteinander zu betrachte: die deskriptiven Werte, die Standardfehler, die p-Werte (Signifikanz) und die Stichprobengrösse. 12.3 Übung zur multiplen, linearen Regression Anschliessend können wir das multiple, lineare Regressionsmodell nochmals erstellen. Dieses soll uns die Wirkung des Bildungsniveau sowie der Tatsache, ob jemand mit Kindern zuhause lebt, auf die wöchentlichen Arbeitsstunden aufzeigen. Die Variable zum Alter der Personen verwenden wir als Kontrollvariable. Weiter integrieren wir wiederum die Variable edulvla als nicht-geordneter Faktor: daten_ess_f$edulvla_no &lt;- factor(daten_ess_f$edulvla, ordered = F) modell_m &lt;- lm(wkhtot ~ agea + edulvla_no + chldhm, data = daten_ess_f) summary(modell_m) ## ## Call: ## lm(formula = wkhtot ~ agea + edulvla_no + chldhm, data = daten_ess_f) ## ## Residuals: ## Min 1Q Median 3Q Max ## -41.607 -10.028 4.595 9.716 62.096 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 35.54966 3.19870 11.114 &lt; 2e-16 *** ## agea -0.01167 0.02489 -0.469 0.639 ## edulvla_noISCED 2 -4.58629 3.08366 -1.487 0.137 ## edulvla_noISCED 3 -2.27678 2.94290 -0.774 0.439 ## edulvla_noISCED 4 -1.56067 3.79260 -0.412 0.681 ## edulvla_noISCED 5-6 2.41428 2.96493 0.814 0.416 ## chldhmDoes not 4.41304 0.93014 4.744 2.31e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 16.43 on 1375 degrees of freedom ## (143 Beobachtungen als fehlend gelöscht) ## Multiple R-squared: 0.03677, Adjusted R-squared: 0.03257 ## F-statistic: 8.748 on 6 and 1375 DF, p-value: 2.258e-09 Folgende Sätze wurden im Seminar präsentiert (kursiv) und es galt Sie mittels Text zu ergänzen. Insgesamt erklärt das berechnete Regressionsmodell lediglich lediglich 3% der Variation der Variable wkhtot. Trotzdem weisst das gesamte Modell auch in der Grundgesamtheit eine Erklärungsleistung auf. Wenn wir übergehen zu den einzelnen Koeffizienten so wird deutlich, dass abgesehen vom bO-Wert lediglich die Variable chldhm auch in der Grundgesamtheit eine Wirkung auf die geleisteten Arbeiststunden aufweist. Dies zeigt sich auch an den jeweiligen Standardfehlern, die alle sehr hoch sind im Vergleich zu den Koeffizienten (ausser bei chldhm). Deskriptiv statistisch betrachtet wird wiederum der starke Effekt der Variable chldhm deutlich: Wenn jemand nicht mit Kindern lebt erhöht dies den Vorhersagewert für wkhtot um fast 4.5h. Eine ähnlich hoher Effekt  allerdings negativ und nicht signifikant  zeigt sich auch bei einem Wechsel vom ersten Bildungsniveau zu ISCED 2. Ein positiver Effekt einer als Dummy-Variable kodierter Ausprägung von knapp 2.5 zeigt sich beim Wechsel vom ersten zum letzten Bildungsniveau. Die Kontrollvariable Alter hat einen sehr geringen Einfluss auf den Vorhersagtwerte, nämlich eine Abnahme um 0.01 Arbeitsstunden pro Altersjahr. Für den (empirisch nicht sinnvollen) Fall dass jemand Kinder hätte, 0 Jahre alt wäre und sich auf dem ersten Bildungsniveau befinden würde, würde das Modell 35.5h Arbeitsstunden pro Woche ausweisen. 12.4 Anwendungsvoraussetzungen Nachdem wir uns der Interpretation des Modells gewidmet haben können wir noch die Anwendungsvoraussetzungen testen. Diese sind folgende: Linearität Keine hohe Multikollinearität Varianzhomogenität der Residuen Normalverteilung der Residuen Unkorreliertheit der Residuen (lediglich bei Zeitreihendaten relevant) Die Frage nach der Multikollinearität (4) haben wir bereits in der Korrelationstabelle angeschnitten, können dies aber auch noch genauer prüfen. Dies erfolgt mittels der Funktion vif() wie dies etwa Vanessa Leutner und Julien Lattmann gemacht haben: library(car) ## Lade nötiges Paket: carData ?vif Der ausgegebene variance inflation factor entspricht dem Umkehrwert der Toleranz (vgl. Diaz-Bone 2019, 207f): 1/vif(modell_m) ## GVIF Df GVIF^(1/(2*Df)) ## agea 0.9790688 1.00 0.9894791 ## edulvla_no 0.9795957 0.25 0.9974264 ## chldhm 0.9807544 1.00 0.9903305 Der Toleranzwert variiert zwischen 0 und 1. Dabei gibt er uns an, wie stark eine jeweilige unabhängige Variable durch eine andere unabhängige Variable erklärt werden kann. Wirklich zu einem Problem werden die Werte nur dann, wenn sie sehr klein sind (z.Bsp. unter 0.2). Für die weiteren Anwendungsvoraussetzungen bietet R eine detaillierte Residuenanalyse an. Dies funktioniert so, dass wir mittels der Funktion plot() auf das Ergebnisobjekt des Regressionsmodell zugreifen. Damit werden uns vier verschiedenen Plots zur grafisch gestützten Residuenanalyse ausgegeben: plot(modell_m) Die erste Grafik ist ein Streudiagramm der Residuen und der Vorhersagwerte. Da hier der Vorhersagwerte (und nicht die abhängige Variable) auf der X-Achse abgebildet ist lassen sich hier alle unabhängigen Variablen überprüfen, und zwar auf die Frage der Linearität (1) hin.13 Ein spezifisches Muster im Plot würde auf fehlende Linearität hinweisen oder auch darauf, dass es weitere erklärende Variablen gibt, die noch nicht im Modell eingeschlossen sind (Manderscheid 2017, 194).14 In unserem Fall scheint die Linearitätsannahme nicht verletzt zu sein und es zeigt sich auch kein (starkes) spezifisches Muster zu zeigen. Die zweite Grafik, der Normal-Q-Q Plot, gibt uns Hinweise zu der Normalverteilung der Residuen (4). Dazu werden die empirischen Verteilungen der Residuen (z-standardisiert) gegen die Quantile geplottet, die bei einer Normalverteilung zu erwarten wären. Quantile sind hier die Wahrscheinlichkeiten: In der theoretischen Normalverteilung liegen links und rechts von 0 je 50%. Je weiter die Werte von Null abweichen, desto weniger Fälle liegen dort (links von -1 wären noch rund 13% der Fälle). Die theoretische Normalverteilung (die gerade, gestrichelte Linie) wird dann mit der empirischen Verteilung verglichen (die über Kreise gebildete, wackelige Linie). Ziel sollte es sei, dass eben im mindestens im Bereich + 1 x Standardabweichung und - 1 x Standardabweichung (68% der Fälle) die beiden Linien ziemlich deckungsgleich wären. Bei grösseren Stichproben werden die Abweichungen immer weniger problematisch (vgl. auch Diaz-Bone 2019:231). Hier zeigen sich allerdings bereits grössere Abweichungen. Die dritte Grafik der Scale-Location prüft die Varianzhomogenität der Residuen (3) (Homoskedaszität) in dem die Vorhersagwerte gegen die standardisierten Residuen geplottet werden. Die Punkte sollten hier horizontal in einheitlicher Breite variieren (Manderscheid 2017, 194). Wie bereits bei der ersten Grafik findet sich hier eine rote Linie, ein sogenannter Smoother. Dies ist eine immer aufs neue geschätzte Line, die im optimalfall gerade verlaufen sollte. Hier sehen wir nun, dass das wir vor allem bei grösseren Vorhersagewerte auch etwas grössere Residuen haben (unabhängig davon ob diese positiv oder negativ sind). Hier scheint also eine gewisse Heteroskedaszität vorzuliegen. Die vierte und letzte Grafik stellt die Residuen dem sogenannten Leverage Wert gegenüber. Dieser Wert gibt an, wie stark ein jeweiliger Punkt auf das Modell wirkt (dessen Hebelwirkung). Je stärker ein solcher Leverage-Wert eines Punktes ist  je weiter rechts er ist  desto geringer sollte die Residue sein. Insbesondere die Werte ausserhalb der Cooks Distance sollten wir genauer betrachten und uns überlegen, was dabei los ist (dies ist hier nicht ersichtlich, findet sich aber etwa hier). In allen vier Grafiken, welche die Funktion plot() von unserem Modell erzeugt hat, wurden immer wieder bestimmte Fälle ausgewiesen. Dies sind Fälle, die das Modell womöglich stark beeinflussen (bzw. dessen Modellgüte reduzieren). Diese Fälle  insbesondere diejenige im letzten Plot  können wir versuchsweise ausschliessen und ein neues Modell rechnen (natürlich müssten wir diese Fälle auch noch genauer betrachten). Es zeigt sich dann allerdings, dass kaum eine Verbesserung erreicht wurde. Unser Modell bleibt auch nachher diesem Schritt ziemlich schlecht modell_m2 &lt;- lm(wkhtot ~ agea + edulvla_no + chldhm, data = daten_ess[-c(130,159,228,768,216, 277),]) summary(modell_m2) 12.5 Exkurs: Residuenwolken Im folgenden sollen fünf verschiedenen Residuenwolken und deren jeweiligen Diagnose kurz beschrieben werden. Die erste Residuenwolke verweist auf ein schlechtes Modell  aber es gleichzeitig auch ein Modell, das sonst kein Problem aufweist/keine Anwendungsvoraussetzungen verletzt. Figure 12.1: Residuenwolke 1 Die zweite Residuenwolke zeigt auf, dass die Residuen nicht normalverteilt sind: Die Extremwerte kommen häufiger als andere. Figure 12.2: Residuenwolke 2 Die dritte Residuenwolke verweist auf einen fehlende Linearität, denn hier scheint ein quadratischer Zusammenhang vorhanden zu sein. Figure 12.3: Residuenwolke 3 Bei der vierten Residuenwolke ist zwar Linearität gegebe, aber hier scheint eine wichtige unabhängige Variable zu fehlen. Deshalb können wir hier Varianinhomogenität oder eben Heteroskedastizität feststellen. Figure 12.4: Residuenwolke 4 Bei der fünten Residuenwolke sehen wir ebenfalls ein schlechtes Modell und ein klarer Fall von Heteroskedastizität. In diesem Beispiel ist die verletzte Anwendungsvoraussetzung allerdings nicht so einfach auf eine fehlende unabhängige Variable zurückzuführen. Figure 12.5: Residuenwolke 5 References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
